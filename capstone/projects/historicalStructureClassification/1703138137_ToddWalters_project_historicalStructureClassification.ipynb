{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toddwalters/aiml-python-coding-examples/blob/main/capstone/projects/historicalStructureClassification/1703138137_ToddWalters_project_historicalStructureClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE6q3GPrXxTI"
      },
      "source": [
        "Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM2B501NXxTI"
      },
      "source": [
        "-----\n",
        "\n",
        "# <a id='toc1_'></a>[**Historical Structure Classification Project**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caA0NN0-XxTI"
      },
      "source": [
        "-----------------------------\n",
        "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "This project is part of an AIML Capstone and focuses on two main aspects:\n",
        "1. Classifying historical structures using deep learning techniques\n",
        "2. Developing a recommendation engine for tourism\n",
        "\n",
        "The project aims to assist the travel and tourism industries by monitoring the condition of historical structures and providing personalized recommendations to tourists.\n",
        "\n",
        "-----------------------------\n",
        "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "1. Develop a CNN model to classify historical structures from images\n",
        "2. Perform exploratory data analysis on tourism data\n",
        "3. Create a recommendation engine for tourist attractions\n",
        "\n",
        "-----------------------------\n",
        "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "**Part 1: Image Classification**\n",
        "\n",
        "1. **Structures_dataset.zip**: Training set of images of historical structures\n",
        "2. **dataset_test**: Test set of images of historical structures\n",
        "\n",
        "**Part 2: Tourism Recommendation**\n",
        "\n",
        "1. **user.csv**: User demographic data\n",
        "   - Columns: User_id, location, age\n",
        "2. **tourism_with_id.csv**: Details on tourist attractions in Indonesia's five largest cities\n",
        "   - Columns: Place_id, Place_name, Description, Category, City, Price, Rating, Time_minute, Coordinate, Lat, Long\n",
        "3. **tourism_rating.csv**: User ratings for tourist attractions\n",
        "   - Columns: user_id, place_id, place_rating\n",
        "\n",
        "-----------------------------------\n",
        "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Part 1 - Deep Learning**\n",
        "\n",
        "1. Plot the sample images (8–10) from each class or category to gain a better understanding\n",
        "of each class\n",
        "   > Hint: You can use the OpenCV open-source library for this task.\n",
        "2. Select an CNN ptm_name of your choice to train the CV model. Configure the\n",
        "ptm_name for transfer learning, set up a TensorFlow environment for the selected\n",
        "backbone ptm_name, and load pre-trained weights\n",
        "   > Note: Algorithm or ptm_name selection is an important step in the training of ML models,\n",
        "so select the one that performs the best on your dataset.\n",
        "3. Deep learning models tend to work well with large amounts (millions of images) of data, but\n",
        "we may not always have enough data to train a deep learning model from scratch. Transfer\n",
        "learning techniques allow us to train and fine-tune large deep learning ptm_names using\n",
        "limited data.\n",
        "   > Hint: For transfer learning, use pre-trained CNN weights and freeze all convolutional\n",
        "layers' weights.\n",
        "4. As of now, CNN ptm_name has been configured for our model. Modify the top of this\n",
        "ptm_name to work with our dataset by:\n",
        "• Adding an appropriate number of dense layers with an activation function.\n",
        "• Using dropout for regularization.\n",
        "   > Note: It is important to understand that these parameters are hyperparameters that\n",
        "must be tuned.\n",
        "5. Compile the model with the right set of parameters like optimizer, loss function, and metric\n",
        "6. Define your callback class to stop the training once validation accuracy reaches a certain\n",
        "number of your choice\n",
        "7. Setup the train or test dataset directories and review the number of image samples for the train\n",
        "and test datasets for each class\n",
        "8. Train the model without augmentation while continuously monitoring the validation accuracy\n",
        "9. Train the model with augmentation and keep monitoring validation accuracy\n",
        "   > Note: Choose carefully the number of epochs, steps per epoch, and validation steps based on\n",
        "your computer configuration\n",
        "10. Visualize training and validation accuracy on the y-axis against each epoch on the x-axis to\n",
        "see if the model overfits after a certain epoch\n",
        "Deep learning\n",
        "\n",
        "**Part 2 - Data Science**\n",
        "\n",
        "1. Import all the datasets and perform preliminary inspections, such as:\n",
        "   1. Check for missing values and duplicates\n",
        "   2. Remove any anomalies found in the data\n",
        "2. To understand the tourism highlights better, we should explore the data in depth.\n",
        "   1. Explore the user group that provides the tourism ratings by:\n",
        "      - Analyzing the age distribution of users visiting the places and rating them\n",
        "      - Identifying the places where most of these users (tourists) are coming from\n",
        "3. Next, let's explore the locations and categories of tourist spots.\n",
        "   1. What are the different categories of tourist spots?\n",
        "   2. What kind of tourism each location is most famous or suitable for?\n",
        "   3. Which city would be best for a nature enthusiast to visit?\n",
        "4. To better understand tourism, we need to create a combined data with places and their user ratings.\n",
        "   1. Use this data to figure out the spots that are most loved by the tourists. Also, which city\n",
        "has the most loved tourist spots?\n",
        "   2. Indonesia provides a wide range of tourist spots ranging from historical and cultural\n",
        "beauties to advanced amusement parks. Among these, which category of places are users\n",
        "liking the most?\n",
        "5. Build a recommender model for the system\n",
        "   1. Use the above data to develop a collaborative filtering model for recommendation and\n",
        "use that to recommend other places to visit using the current tourist location(place name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r6c89oQXxTJ"
      },
      "source": [
        "### <a id='toc1_4_1_'></a>[**Part 1 - Deep Learning**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmv8a41OXxTJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Import Modules and Set Default Environment Variables**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS3KJkebXxTJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras import backend as K\n",
        "from keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.utils import Sequence\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Enable if having issues with Tensorflow\n",
        "# os.environ['TF_DISABLE_GRAPPLER'] = '1'\n",
        "\n",
        "# Load the environment variables\n",
        "# load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "# DATASET_PATH = f'{DATASET_PATH}/structures_dataset'\n",
        "\n",
        "# Uncomment when using Google Colab and Mounting Google Drive\n",
        "DATASET_PATH = '/home/sagemaker-user/dataset/'\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "# try:\n",
        "#     from google.colab import drive\n",
        "#     drive.mount('/content/drive')\n",
        "#     DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
        "# except ImportError:\n",
        "#     DATASET_PATH = '/Users/toddwalters/Library/CloudStorage/GoogleDrive-toddw4271@gmail.com/My Drive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
        "\n",
        "# Check TensorFlow Verision\n",
        "print()\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srk6fELyXxTJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Look for corrupt files**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhNzewajXxTJ"
      },
      "outputs": [],
      "source": [
        "def verify_images(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    img = Image.open(file_path) # open the image file\n",
        "                    img.verify() # verify that it is, in fact an image\n",
        "                except (IOError, SyntaxError) as e:\n",
        "                    print(f'Bad file: {file_path}') # print out the names of corrupt files\n",
        "                    os.remove(file_path)\n",
        "                    print(f'Deleted bad file: {file_path}')\n",
        "\n",
        "# verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "# verify_images(f'{DATASET_PATH}/structure_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NNsXI_UcLBJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Rename Files**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCtxGIuocJVV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "\n",
        "def random_string(length):\n",
        "    \"\"\"Generate a random string of fixed length\"\"\"\n",
        "    letters = string.ascii_lowercase + string.digits\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def rename_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            # Generate a unique name\n",
        "            new_name = str(uuid.uuid4())\n",
        "\n",
        "            # Get the file extension\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "            # Create the new filename\n",
        "            new_filename = f\"{new_name}.jpg\"\n",
        "\n",
        "            # Full paths\n",
        "            old_file = os.path.join(root, filename)\n",
        "            new_file = os.path.join(root, new_filename)\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(old_file, new_file)\n",
        "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
        "\n",
        "# Uncomment if you want to rename all of the files in the dataset\n",
        "#\n",
        "# Specify the directory to start from\n",
        "# start_directory = f'{DATASET_PATH}/structures_dataset'\n",
        "# Run the function\n",
        "# rename_files(start_directory)\n",
        "# Specify the directory to start from\n",
        "# start_directory = f'{DATASET_PATH}/dataset_test'\n",
        "# # Run the function\n",
        "# rename_files(start_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy9L31YIcWPX"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Plot The Sample Images**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2knLkSoXxTJ"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images(dataset_path, num_samples=8):\n",
        "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
        "\n",
        "        for j, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "        # Set the class name as the title for the first image in the row\n",
        "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_sample_images(f'{DATASET_PATH}/structures_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNb-3xO8XxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_sample_images` that takes a dataset path and plots a specified number of sample images from each class. It uses OpenCV to read the images and matplotlib to display them in a grid.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing sample images from each class helps us understand the nature of the data we're working with. It allows us to identify any potential issues with the images, such as inconsistent sizes, color schemes, or quality. This step is crucial for determining if any preprocessing steps are needed before training the model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bv_nJW-ae5J"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Define Utility Functions and Classes**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf0dLfVQaicV"
      },
      "outputs": [],
      "source": [
        "# Define utility functions and classes\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, generator, steps_per_epoch):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.reset()\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_accuracy = logs.get('accuracy', 'N/A')\n",
        "        val_accuracy = logs.get('val_accuracy', 'N/A')\n",
        "\n",
        "        train_accuracy_str = f\"{train_accuracy:.4f}\" if isinstance(train_accuracy, float) else str(train_accuracy)\n",
        "        val_accuracy_str = f\"{val_accuracy:.4f}\" if isinstance(val_accuracy, float) else str(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train accuracy: {train_accuracy_str}\")\n",
        "        print(f\"Epoch {epoch + 1} - Val accuracy: {val_accuracy_str}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if batch % 100 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            loss_str = f\"{loss:.4f}\" if isinstance(loss, float) else str(loss)\n",
        "            print(f\"Batch {batch} - Loss: {loss_str}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "        print(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset).numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1} - Test samples: {tf.data.experimental.cardinality(self.test_dataset).numpy()}\")\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        evaluation = self.model.evaluate(self.validation_data, steps=self.validation_steps, verbose=0)\n",
        "        print(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "        print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_data_generators(train_path, test_path, batch_size=32):\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        image_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        test_path,\n",
        "        image_size=(224, 224),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get class names before applying normalization\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
        "    validation_steps = tf.data.experimental.cardinality(test_dataset).numpy()\n",
        "\n",
        "    # Get classes\n",
        "    classes = np.concatenate([y for x, y in train_dataset], axis=0)\n",
        "    classes = np.argmax(classes, axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names\n",
        "\n",
        "def verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch):\n",
        "    print(\"Verifying dataset...\")\n",
        "\n",
        "    # 1. Print dataset information\n",
        "    print(f\"Train dataset cardinality: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "    print(f\"Test dataset cardinality: {tf.data.experimental.cardinality(test_dataset)}\")\n",
        "\n",
        "    # 2. Inspect a few batches\n",
        "    print(\"\\nInspecting batches:\")\n",
        "    for i, (images, labels) in enumerate(train_dataset.take(10)):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Image min-max values: {tf.reduce_min(images).numpy()} - {tf.reduce_max(images).numpy()}\")\n",
        "        print(f\"  Unique labels: {np.unique(labels.numpy())}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 3. Check dataset configuration\n",
        "    print(\"\\nDataset configuration:\")\n",
        "    print(f\"Train dataset: {train_dataset}\")\n",
        "    print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "    # 4. Verify steps per epoch\n",
        "    total_train_samples = tf.data.experimental.cardinality(train_dataset).numpy() * batch_size\n",
        "    calculated_steps = total_train_samples // batch_size\n",
        "    print(f\"\\nCalculated steps per epoch: {calculated_steps}\")\n",
        "    print(f\"Provided steps per epoch: {steps_per_epoch}\")\n",
        "    assert calculated_steps == steps_per_epoch, \"Steps per epoch mismatch\"\n",
        "\n",
        "    # 5. Test complete iteration\n",
        "    print(\"\\nTesting complete iteration:\")\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for step, (images, labels) in enumerate(train_dataset):\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: images shape {images.shape}, labels shape {labels.shape}\")\n",
        "            if step >= steps_per_epoch:\n",
        "                break\n",
        "\n",
        "    # 6. Check for data leakage\n",
        "    print(\"\\nChecking for data leakage...\")\n",
        "    train_sample = next(iter(train_dataset.take(100)))[0][0]\n",
        "    for test_batch in test_dataset.take(100):\n",
        "        assert not np.array_equal(train_sample, test_batch[0][0]), \"Data leakage detected\"\n",
        "    print(\"No data leakage detected\")\n",
        "\n",
        "    # 7. Verify data augmentation (if applicable)\n",
        "    # Uncomment and modify this section if you're using data augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\nVerifying data augmentation:\")\n",
        "    for original, augmented in zip(train_dataset.take(1), train_dataset.map(augmentation_function).take(1)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(original[0][0])\n",
        "        plt.title(\"Original\")\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(augmented[0][0])\n",
        "        plt.title(\"Augmented\")\n",
        "        plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDataset verification complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txsdv3rOXxTK"
      },
      "source": [
        "#### <a id='toc1_4_1_2_'></a>[**Select an CNN ptm_name**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35yzkV_1XxTK"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    # base_model = ResNet50V2(weights='imagenet', include_top=False, ptm_input_shape=(128, 128, 3))\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # inputs = Input(shape=(128, 128, 3))\n",
        "    inputs = Input(shape=(224, 224, 3))\n",
        "    # x = CustomAugmentationLayer()(inputs)\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_gyywPTXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `create_model` that creates a CNN using transfer learning. It uses ResNet50 as the base model with pre-trained ImageNet weights. The base model layers are frozen, and custom dense layers are added on top for fine-tuning.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Transfer learning allows us to leverage pre-trained models on large datasets, which is particularly useful when we have limited data. By using a pre-trained model as a feature extractor and adding custom layers, we can adapt the model to our specific classification task while benefiting from the general features learned by the base model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTs85TTXxTK"
      },
      "source": [
        "#### <a id='toc1_4_1_3_'></a>[**Compile The Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJqh425GXxTK"
      },
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        # decay_steps=1000,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDCqMO3TXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `compile_model` that compiles the model with the Adam optimizer, categorical crossentropy loss function, and accuracy metric.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Compiling the model is a crucial step that defines how the model will be trained. The choice of optimizer, loss function, and metrics affects the training process and the model's performance. Categorical crossentropy is appropriate for multi-class classification tasks, and accuracy provides a clear measure of the model's performance.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC0DVFsAXxTK"
      },
      "source": [
        "#### <a id='toc1_4_2_'></a>[**Define Callback Class**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EEh6fvpXxTK"
      },
      "outputs": [],
      "source": [
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        accuracy = logs.get('accuracy', 'N/A')\n",
        "        accuracy_str = f\"{accuracy:.4f}\" if isinstance(accuracy, float) else str(accuracy)\n",
        "        print(f\"Epoch {epoch + 1} - Accuracy: {accuracy_str}\")\n",
        "        if isinstance(accuracy, float) and accuracy >= self.target_accuracy:\n",
        "            print(f\"Reached target accuracy of {self.target_accuracy}\")\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o501BpdXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a custom callback class `AccuracyCallback` that stops the training when a target validation accuracy is reached.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Callbacks allow us to customize the training process. In this case, we're using a callback to prevent overfitting by stopping the training once we've reached a satisfactory level of validation accuracy. This helps us avoid wasting computational resources and reduces the risk of the model memorizing the training data.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1MRDUL0XxTK"
      },
      "source": [
        "#### <a id='toc1_4_2_1_'></a>[**Set Up Dataset Directories And Review Sample Numbers**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNIxwiL7XxTK"
      },
      "outputs": [],
      "source": [
        "# Setup Dataset Directories\n",
        "train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "# Set up data generators first to get class information\n",
        "batch_size = 32\n",
        "train_generator, test_generator, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "# Now we can use class_names for our existing count_samples function\n",
        "train_counts = count_samples(train_path)\n",
        "test_counts = count_samples(test_path)\n",
        "\n",
        "print(\"\\nTraining samples per class:\")\n",
        "for class_name, count in sorted(train_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print(\"\\nTest samples per class:\")\n",
        "for class_name, count in sorted(test_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print()\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Print class weights\n",
        "print(\"Class weights:\")\n",
        "for i, weight in enumerate(class_weights):\n",
        "    print(f\"Class {i} ({class_names[i]}): {weight}\")\n",
        "print()\n",
        "\n",
        "# Create and compile the model\n",
        "num_classes = len(class_names)\n",
        "model = create_model(num_classes)\n",
        "print(model.summary())\n",
        "compiled_model = compile_model(model)\n",
        "\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Validation steps: {validation_steps}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAM9ERJ2XxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `count_samples` that counts the number of samples in each class for a given dataset. It then applies this function to both the training and test datasets and prints the results.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Understanding the distribution of samples across classes is crucial for several reasons:\n",
        "\n",
        "    1. It helps identify any class imbalance issues that may need to be addressed.\n",
        "    2. It ensures we have enough samples in each class for both training and testing.\n",
        "    3. It helps in setting appropriate batch sizes and steps per epoch during training.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRYKfuzXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_2_'></a>[**Train The Model Without Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4YBG86DXxTL"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Setup Dataset Directories\n",
        "    train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "    test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "    # Set up data generators first to get class information\n",
        "    batch_size = 32\n",
        "    train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "    # Verify the dataset\n",
        "    verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch)\n",
        "\n",
        "    # Set up callbacks\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=0.95)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_dataset, test_dataset)\n",
        "    custom_validation = CustomValidationCallback(test_dataset, validation_steps)\n",
        "\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    try:\n",
        "        # Separate evaluation before training\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print(f\"Initial evaluation: {evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Training\n",
        "        history = compiled_model.fit(\n",
        "            train_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=20,\n",
        "            validation_data=test_dataset,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation, accuracy_callback],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print()\n",
        "        print(\"Final evaluation:\")\n",
        "        print(\"-----------------\")\n",
        "        final_evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print()\n",
        "        print(f\"Final evaluation: {final_evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'], label='Training Loss')\n",
        "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print()\n",
        "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        print()\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDg5KrjcXxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code sets up data generators for the training and test datasets, then trains the model using these generators. The `ImageDataGenerator` is used to load and preprocess images in batches, which is memory-efficient for large datasets.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Training the model without augmentation provides a baseline performance. It allows us to see how well the model performs with the original data before applying any data augmentation techniques. This step is crucial for understanding if the model has enough capacity to learn from the data and if there are any immediate issues like overfitting or underfitting.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEtTGXrVXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHszyF2ednn5",
        "outputId": "edb3f833-7435-4d84-a818-16ed128a5ae6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zuoPxxZpXxTL"
      },
      "outputs": [],
      "source": [
        "# v1 - WORKS DO NOT CHANGE\n",
        "#\n",
        "import gc\n",
        "import math\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import warnings\n",
        "import traceback\n",
        "from collections import Counter\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import keras.backend as K\n",
        "from keras.applications import (\n",
        "    EfficientNetB0,\n",
        "    InceptionV3,\n",
        "    MobileNetV2,\n",
        "    ResNet50V2,\n",
        "    VGG16\n",
        ")\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (\n",
        "    Callback,\n",
        "    EarlyStopping,\n",
        "    LambdaCallback,\n",
        "    LearningRateScheduler,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "from keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling2D,\n",
        "    Input\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.utils import Sequence\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Clear any existing TensorFlow session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# To ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set logging level to DEBUG for detailed output\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Define mappings for architectures and preprocessing functions\n",
        "ARCHITECTURES = {\n",
        "    'ResNet50V2': ResNet50V2,\n",
        "    'VGG16': VGG16,\n",
        "    'InceptionV3': InceptionV3,\n",
        "    'MobileNetV2': MobileNetV2,\n",
        "    'EfficientNetB0': EfficientNetB0\n",
        "}\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'efficientnet_preprocess': efficientnet_preprocess\n",
        "}\n",
        "\n",
        "# Define metrics mapping\n",
        "METRICS = {\n",
        "    'Precision': Precision(name='precision'),\n",
        "    'Recall': Recall(name='recall'),\n",
        "    'AUC': AUC(name='auc')\n",
        "}\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
        "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
        "            print()\n",
        "            print(\"--------------------\")\n",
        "            print()\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        for x, y in self.validation_data.take(self.validation_steps):\n",
        "            val_metrics = self.model.test_on_batch(x, y)\n",
        "            val_loss += val_metrics[0]\n",
        "            val_accuracy += val_metrics[1]\n",
        "\n",
        "        val_loss /= self.validation_steps\n",
        "        val_accuracy /= self.validation_steps\n",
        "\n",
        "        logs['val_loss'] = val_loss\n",
        "        logs['val_accuracy'] = val_accuracy\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        logging.debug(f\"Loss: {val_loss:.4f}\")\n",
        "        logging.debug(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "class LearningRateTracker(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Retrieve the learning rate from the learning rate schedule in the optimizer\n",
        "        optimizer = self.model.optimizer\n",
        "        if isinstance(optimizer.learning_rate, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            # If a LearningRateSchedule is used, call it with the current step\n",
        "            lr = optimizer.learning_rate(optimizer.iterations).numpy()\n",
        "        else:\n",
        "            # If a static learning rate is used\n",
        "            lr = optimizer.learning_rate.numpy()\n",
        "\n",
        "        self.learning_rates.append(lr)\n",
        "        logging.debug(f\"Epoch {epoch + 1} - Learning rate: {lr}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, val_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset)}\")\n",
        "        logging.debug(f\"Epoch {epoch + 1} - Val samples: {tf.data.experimental.cardinality(self.val_dataset)}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            logging.debug(f\"\\nEpoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
        "            logging.debug(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nStarting epoch {epoch + 1}\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if batch % 50 == 0:\n",
        "            logging.debug(f\"\\nStarting batch {batch}\\n\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEnd of epoch {epoch + 1}\\n\")\n",
        "        if logs:\n",
        "            for key, value in logs.items():\n",
        "                logging.debug(f\"{key}: {value}\")\n",
        "        logging.debug(\"\\n--------------------\\n\")\n",
        "\n",
        "class CustomAugmentationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAugmentationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            augmented = tf.image.random_contrast(inputs, lower=0.8, upper=1.2)\n",
        "            augmented = tf.image.random_saturation(augmented, lower=0.8, upper=1.2)\n",
        "            return augmented\n",
        "        return inputs\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.train_path = config['data']['train_path']\n",
        "        self.test_path = config['data']['test_path']\n",
        "        self.batch_size = config['data']['batch_size']\n",
        "        self.target_size = tuple(config['data']['target_size'])\n",
        "        self.preprocessing_function = PREPROCESSING_FUNCTIONS[config['data']['preprocessing_function']]\n",
        "        self.augmentation_params = config['augmentation']\n",
        "\n",
        "        logging.debug(f\"Initialized DataGenerator with:\")\n",
        "        logging.debug(f\"Train path: {self.train_path}\")\n",
        "        logging.debug(f\"Test path: {self.test_path}\")\n",
        "        logging.debug(f\"Batch size: {self.batch_size}\")\n",
        "        logging.debug(f\"Target size: {self.target_size}\")\n",
        "        logging.debug(f\"Preprocessing function: {self.preprocessing_function}\")\n",
        "        logging.debug(f\"Augmentation params: {self.augmentation_params}\")\n",
        "\n",
        "    def normalize_and_preprocess(self, image, label):\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "        image = self.preprocessing_function(image)\n",
        "        return image, label\n",
        "\n",
        "    def augment(self, images, labels):\n",
        "        # images: a batch of images of shape [batch_size, height, width, channels]\n",
        "\n",
        "        # Define the function to apply to each image\n",
        "        def augment_image(image):\n",
        "            # Cast image to float32 for processing\n",
        "            image = tf.cast(image, tf.float32)\n",
        "\n",
        "            # Apply augmentations\n",
        "            # Random rotation\n",
        "            if self.augmentation_params.get('rotation_range'):\n",
        "                # Randomly choose the number of 90-degree rotations\n",
        "                k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "                image = tf.image.rot90(image, k)\n",
        "\n",
        "            # Random flip\n",
        "            if self.augmentation_params.get('horizontal_flip'):\n",
        "                image = tf.image.random_flip_left_right(image)\n",
        "            if self.augmentation_params.get('vertical_flip'):\n",
        "                image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "            # Random zoom (crop and resize)\n",
        "            if self.augmentation_params.get('zoom_range'):\n",
        "                zoom = self.augmentation_params['zoom_range']\n",
        "                # Generate random zoom factor\n",
        "                zoom_factor = tf.random.uniform([], 1 - zoom, 1 + zoom)\n",
        "                # Compute new dimensions\n",
        "                new_height = tf.cast(tf.cast(self.target_size[0], tf.float32) * zoom_factor, tf.int32)\n",
        "                new_width = tf.cast(tf.cast(self.target_size[1], tf.float32) * zoom_factor, tf.int32)\n",
        "                image = tf.image.resize(image, [new_height, new_width])\n",
        "                image = tf.image.resize_with_crop_or_pad(image, self.target_size[0], self.target_size[1])\n",
        "\n",
        "            # Random shift\n",
        "            if self.augmentation_params.get('width_shift_range') or self.augmentation_params.get('height_shift_range'):\n",
        "                width_shift = self.augmentation_params.get('width_shift_range', 0)\n",
        "                height_shift = self.augmentation_params.get('height_shift_range', 0)\n",
        "                # Compute shift amounts\n",
        "                width_shift_pixels = tf.cast(width_shift * self.target_size[1], tf.int32)\n",
        "                height_shift_pixels = tf.cast(height_shift * self.target_size[0], tf.int32)\n",
        "                # Pad the image to allow shifting\n",
        "                image = tf.image.pad_to_bounding_box(\n",
        "                    image,\n",
        "                    height_shift_pixels,\n",
        "                    width_shift_pixels,\n",
        "                    self.target_size[0] + 2 * height_shift_pixels,\n",
        "                    self.target_size[1] + 2 * width_shift_pixels\n",
        "                )\n",
        "                image = tf.image.random_crop(image, size=[self.target_size[0], self.target_size[1], 3])\n",
        "\n",
        "            # Random brightness\n",
        "            if self.augmentation_params.get('brightness_range'):\n",
        "                max_delta = self.augmentation_params['brightness_range'] * 255.0  # Adjusted for pixel range [0, 255]\n",
        "                image = tf.image.random_brightness(image, max_delta=max_delta)\n",
        "\n",
        "            # Random contrast\n",
        "            if self.augmentation_params.get('contrast_range'):\n",
        "                lower = 1.0 - self.augmentation_params['contrast_range']\n",
        "                upper = 1.0 + self.augmentation_params['contrast_range']\n",
        "                image = tf.image.random_contrast(image, lower=lower, upper=upper)\n",
        "\n",
        "            # Random saturation\n",
        "            if self.augmentation_params.get('saturation_range'):\n",
        "                lower = 1.0 - self.augmentation_params['saturation_range']\n",
        "                upper = 1.0 + self.augmentation_params['saturation_range']\n",
        "                image = tf.image.random_saturation(image, lower=lower, upper=upper)\n",
        "\n",
        "            # Random hue\n",
        "            if self.augmentation_params.get('hue_range'):\n",
        "                max_delta = self.augmentation_params['hue_range']\n",
        "                image = tf.image.random_hue(image, max_delta=max_delta)\n",
        "\n",
        "            # Ensure the image is in the correct range [0, 255]\n",
        "            image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "\n",
        "            # Normalize and preprocess after augmentations\n",
        "            image = image / 255.0\n",
        "            image = self.preprocessing_function(image)\n",
        "\n",
        "            return image\n",
        "\n",
        "        # Apply the augment_image function to each image in the batch\n",
        "        images = tf.map_fn(augment_image, images, fn_output_signature=tf.float32)\n",
        "\n",
        "        logging.debug(\"Applied augmentations to batch of images.\")\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def create_datasets(self):\n",
        "        train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.train_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.test_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        train_dataset = train_dataset.map(self.augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.map(self.normalize_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        train_dataset = train_dataset.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        steps_per_epoch = tf.data.experimental.cardinality(train_dataset)\n",
        "        if steps_per_epoch == tf.data.experimental.INFINITE_CARDINALITY:\n",
        "            steps_per_epoch = None\n",
        "        else:\n",
        "            steps_per_epoch = steps_per_epoch.numpy()\n",
        "\n",
        "        validation_steps = tf.data.experimental.cardinality(test_dataset)\n",
        "        if validation_steps == tf.data.experimental.INFINITE_CARDINALITY:\n",
        "            validation_steps = None\n",
        "        else:\n",
        "            validation_steps = validation_steps.numpy()\n",
        "\n",
        "        logging.debug(f\"Training dataset: {train_dataset}\")\n",
        "        logging.debug(f\"Testing dataset: {test_dataset}\")\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps\n",
        "\n",
        "    def get_data_generators(self):\n",
        "        return self.create_datasets()\n",
        "\n",
        "class LearningRateFinder:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.stop_factor = config['lr_finder']['stop_factor']\n",
        "        self.beta = config['lr_finder']['beta']\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weightsFile = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weightsFile = None\n",
        "\n",
        "    def find(self, train_data, start_lr, end_lr, batch_size=32, epochs=5):\n",
        "        self.reset()\n",
        "        num_samples = tf.data.experimental.cardinality(train_data).numpy()\n",
        "        steps_per_epoch = math.ceil(num_samples / batch_size)\n",
        "        num_batches = epochs * steps_per_epoch\n",
        "\n",
        "        self.weightsFile = \"lrf_weights.weights.h5\"\n",
        "        self.model.save_weights(self.weightsFile)\n",
        "\n",
        "        # Save the original optimizer\n",
        "        self.original_optimizer = self.model.optimizer\n",
        "\n",
        "        # Replace optimizer with one that has a static learning rate\n",
        "        optimizer = Adam(learning_rate=start_lr)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.model.loss)\n",
        "\n",
        "        logging.info(f\"Starting Learning Rate Finder: LR range {start_lr} to {end_lr}\")\n",
        "\n",
        "        lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
        "\n",
        "        class LRFinderCallback(tf.keras.callbacks.Callback):\n",
        "            def __init__(self, lr_finder):\n",
        "                super().__init__()\n",
        "                self.lr_finder = lr_finder\n",
        "                self.lr = start_lr\n",
        "                self.batch_num = 0\n",
        "\n",
        "            def on_batch_end(self, batch, logs=None):\n",
        "                self.batch_num += 1\n",
        "\n",
        "                # Compute the smoothed loss\n",
        "                loss = logs[\"loss\"]\n",
        "                self.lr_finder.avg_loss = (self.lr_finder.beta * self.lr_finder.avg_loss) + ((1 - self.lr_finder.beta) * loss)\n",
        "                smooth = self.lr_finder.avg_loss / (1 - self.lr_finder.beta ** self.batch_num)\n",
        "                self.lr_finder.losses.append(smooth)\n",
        "\n",
        "                # Save the learning rate\n",
        "                self.lr_finder.lrs.append(self.lr)\n",
        "\n",
        "                # Check if the loss is diverging\n",
        "                if self.batch_num > 1 and smooth > self.lr_finder.stop_factor * self.lr_finder.best_loss:\n",
        "                    self.model.stop_training = True\n",
        "                    return\n",
        "\n",
        "                # Update best loss\n",
        "                if self.batch_num == 1 or smooth < self.lr_finder.best_loss:\n",
        "                    self.lr_finder.best_loss = smooth\n",
        "\n",
        "                # Increase the learning rate for next batch\n",
        "                self.lr *= lr_mult\n",
        "                self.model.optimizer.learning_rate.assign(self.lr)\n",
        "\n",
        "                if self.batch_num % 10 == 0:\n",
        "                    logging.debug(f\"Batch {self.batch_num}: lr = {self.lr:.6f}, loss = {smooth:.4f}\")\n",
        "\n",
        "        lr_finder_callback = LRFinderCallback(self)\n",
        "\n",
        "        self.model.fit(train_data,\n",
        "                       steps_per_epoch=steps_per_epoch,\n",
        "                       epochs=epochs,\n",
        "                       callbacks=[lr_finder_callback],\n",
        "                       verbose=0)\n",
        "\n",
        "        logging.info(\"Learning Rate Finder complete\")\n",
        "\n",
        "        self.model.load_weights(self.weightsFile)\n",
        "\n",
        "        # Restore the original optimizer\n",
        "        self.model.compile(optimizer=self.original_optimizer, loss=self.model.loss, metrics=self.model.metrics)\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.lrs, self.losses)\n",
        "        plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Learning Rate vs. Loss\")\n",
        "        plt.show()\n",
        "\n",
        "    def get_best_lr(self):\n",
        "        min_loss_idx = np.argmin(self.losses)\n",
        "        best_lr = self.lrs[min_loss_idx]\n",
        "        logging.info(f\"Best initial learning rate found: {best_lr:.6f}\")\n",
        "        return best_lr\n",
        "\n",
        "    def print_results(self):\n",
        "        logging.info(\"\\nLearning Rate Finder Results:\")\n",
        "        logging.info(f\"Minimum loss: {min(self.losses):.4f}\")\n",
        "        logging.info(f\"Maximum loss: {max(self.losses):.4f}\")\n",
        "        logging.info(f\"Learning rate range: {min(self.lrs):.6f} to {max(self.lrs):.6f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_model(config, num_classes):\n",
        "    architecture = ARCHITECTURES[config['model']['architecture']]\n",
        "    input_shape = tuple(config['model']['input_shape'])\n",
        "\n",
        "    base_model = architecture(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(config['model']['dense_units'])(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(config['model']['dropout_rate'])(x)\n",
        "    x = Dense(config['model']['dense_units'] // 2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "def compile_model(model, config):\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        config['model']['initial_learning_rate'],\n",
        "        decay_steps=config['model']['decay_steps'],\n",
        "        decay_rate=config['model']['decay_rate'],\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    # Use the METRICS mapping instead of eval\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']])\n",
        "    logging.debug(\"Model compiled with the following parameters:\")\n",
        "    logging.debug(f\"Optimizer: {optimizer}\")\n",
        "    logging.debug(f\"Loss: categorical_crossentropy\")\n",
        "    logging.debug(f\"Metrics: {['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']]}\")\n",
        "    return model\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 100:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "def setup_gpu(gpu_config):\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Log the number of GPUs available\n",
        "            logging.info(f\"GPU setup complete. Found {len(gpus)} GPU(s).\")\n",
        "\n",
        "            # Optionally, you can log more details about each GPU\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logging.info(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            logging.error(f\"GPU setup failed: {e}\")\n",
        "    else:\n",
        "        logging.warning(\"No GPUs found. The model will run on CPU.\")\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "def setup_datasets(config):\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logging.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps = data_generator.get_data_generators()\n",
        "\n",
        "        # Dynamically determine class names\n",
        "        class_names = sorted(os.listdir(config['data']['train_path']))\n",
        "        class_names = [name for name in class_names if os.path.isdir(os.path.join(config['data']['train_path'], name))]\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Dataset setup failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def create_and_compile_model(config, num_classes):\n",
        "    try:\n",
        "        model = create_model(config, num_classes)\n",
        "        compiled_model = compile_model(model, config)\n",
        "        return compiled_model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model creation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_callbacks(config, train_dataset, test_dataset, validation_steps):\n",
        "    return [\n",
        "        AccuracyCallback(target_accuracy=config['training']['target_accuracy']),\n",
        "        CustomValidationCallback(test_dataset, validation_steps),\n",
        "        DebugCallback(),\n",
        "        DatasetLogger(train_dataset, test_dataset),\n",
        "        EarlyStopping(monitor='val_loss', patience=config['training']['patience'], restore_best_weights=True),\n",
        "        LearningRateTracker(),\n",
        "        ModelCheckpoint(filepath=config['training']['model_checkpoint_path'], save_best_only=True)\n",
        "    ]\n",
        "\n",
        "def find_optimal_learning_rate(model, train_dataset, config):\n",
        "    lr_finder = LearningRateFinder(model, config)\n",
        "    lr_finder.find(\n",
        "        train_data=train_dataset,\n",
        "        start_lr=config['lr_finder']['start_lr'],\n",
        "        end_lr=config['lr_finder']['end_lr'],\n",
        "        batch_size=config['data']['batch_size'],\n",
        "        epochs=config['lr_finder']['epochs']\n",
        "    )\n",
        "    best_lr = lr_finder.get_best_lr()\n",
        "    return best_lr\n",
        "\n",
        "def get_class_counts(dataset):\n",
        "    class_counts = Counter()\n",
        "    for _, labels in dataset:\n",
        "        classes = np.argmax(labels.numpy(), axis=1)\n",
        "        class_counts.update(classes)\n",
        "    return class_counts\n",
        "\n",
        "def compute_class_weights(train_dataset):\n",
        "    class_counts = get_class_counts(train_dataset)\n",
        "    total_samples = sum(class_counts.values())\n",
        "    class_weight_dict = {i: total_samples / count for i, count in class_counts.items()}\n",
        "    logging.debug(f\"Computed class weights: {class_weight_dict}\")\n",
        "    return class_weight_dict\n",
        "\n",
        "\n",
        "def main(config_path):\n",
        "    config = load_config(config_path)\n",
        "    logging.debug(f\"Loaded configuration: {config}\")\n",
        "    setup_gpu(config.get('gpu', {}))\n",
        "    logging.debug(\"Completed GPU setup.\")\n",
        "\n",
        "    # Mount Google Drive if using Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "        logging.info(\"Running in Colab environment\")\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "        logging.info(\"Running in local environment\")\n",
        "\n",
        "    # Update the train and test paths\n",
        "    train_path = os.path.join(DATASET_PATH, config['data']['train_dir'])\n",
        "    test_path = os.path.join(DATASET_PATH, config['data']['test_dir'])\n",
        "\n",
        "    # Log the dataset paths\n",
        "    logging.debug(f\"Full training dataset path: {train_path}\")\n",
        "    logging.debug(f\"Full testing dataset path: {test_path}\")\n",
        "\n",
        "    # Update the config with the full paths\n",
        "    config['data']['train_path'] = train_path\n",
        "    config['data']['test_path'] = test_path\n",
        "\n",
        "    logging.info(f\"Train path: {train_path}\")\n",
        "    logging.info(f\"Test path: {test_path}\")\n",
        "\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logging.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names = setup_datasets(config)\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        num_classes = len(class_names)\n",
        "        model = create_and_compile_model(config, num_classes)\n",
        "        logging.debug(\"Model created and compiled successfully.\")\n",
        "        logging.debug(\"Model summary:\")\n",
        "        model.summary(print_fn=lambda x: logging.debug(x))\n",
        "\n",
        "        if config['training']['find_lr']:\n",
        "            lr_finder = LearningRateFinder(model, config)\n",
        "            lr_finder.find(\n",
        "                train_data=train_dataset,\n",
        "                start_lr=float(config['lr_finder']['start_lr']),\n",
        "                end_lr=float(config['lr_finder']['end_lr']),\n",
        "                batch_size=config['data']['batch_size'],\n",
        "                epochs=config['lr_finder']['epochs']\n",
        "            )\n",
        "            best_lr = lr_finder.get_best_lr()\n",
        "            best_lr = best_lr / 10\n",
        "            config['model']['initial_learning_rate'] = best_lr\n",
        "            model = compile_model(model, config)\n",
        "            logging.debug(f\"Updated initial learning rate to: {best_lr}\")\n",
        "        else:\n",
        "            logging.info(\"Skipping the learning rate finder as per configuration.\")\n",
        "\n",
        "        # Create LearningRateTracker instance\n",
        "        lr_tracker = LearningRateTracker()\n",
        "\n",
        "        # Get other callbacks\n",
        "        other_callbacks = get_callbacks(config, train_dataset, test_dataset, validation_steps)\n",
        "\n",
        "        # Include lr_tracker in the callbacks\n",
        "        callbacks = other_callbacks + [lr_tracker]\n",
        "\n",
        "        # Compute class weights\n",
        "        y_train = np.concatenate([y for x, y in train_dataset], axis=0)\n",
        "        classes = np.argmax(y_train, axis=1)\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logging.debug(f\"Computed class weights: {class_weight_dict}\")\n",
        "\n",
        "        if config['training']['pretrain_model_eval']:\n",
        "            logging.info(\"Evaluating model before training:\")\n",
        "            evaluation = model.evaluate(test_dataset)\n",
        "            logging.info(f\"Initial evaluation: {evaluation}\")\n",
        "\n",
        "        logging.debug(\"Starting model training...\")\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=config['training']['epochs'],\n",
        "                validation_data=test_dataset,\n",
        "                callbacks=callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                verbose=1\n",
        "            )\n",
        "        except tf.errors.ResourceExhaustedError as e:\n",
        "            logging.error(\"Memory error occurred during training.\")\n",
        "            logging.error(f\"Error details: {e}\")\n",
        "            logging.error(\"Consider reducing batch size or image dimensions.\")\n",
        "            return None, None, None, None, None, None\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during training: {e}\")\n",
        "            logging.debug(traceback.format_exc())\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        logging.debug(\"Model training completed successfully.\")\n",
        "        logging.info(\"Final evaluation:\")\n",
        "        final_evaluation = model.evaluate(test_dataset)\n",
        "        logging.info(f\"Final evaluation metrics: {final_evaluation}\")\n",
        "\n",
        "        # Retrieve learning rates from lr_tracker\n",
        "        learning_rates = lr_tracker.learning_rates\n",
        "\n",
        "        return history, model, class_names, config['data']['target_size'], config['data']['preprocessing_function'], learning_rates\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred: {e}\")\n",
        "        logging.debug(traceback.format_exc())\n",
        "        return None, None, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "7OLPkkIYdZ_1",
        "outputId": "f666106d-9f6e-48a0-cdec-7be66be6a956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No GPUs found. The model will run on CPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10255 files belonging to 10 classes.\n",
            "Found 1474 files belonging to 10 classes.\n",
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 4s/step - accuracy: 0.3000 - auc: 0.6024 - loss: 2.2877 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 1/2\n",
            "\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.0913 - auc: 0.5004 - loss: 2.5278 - precision: 0.0882 - recall: 0.0022"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 98 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7f5cdfbeae60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 99 calls to <function TensorFlowTrainer.make_test_function.<locals>.one_step_on_iterator at 0x7f5cdfbeae60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1338s\u001b[0m 4s/step - accuracy: 0.0913 - auc: 0.5005 - loss: 2.5275 - precision: 0.0883 - recall: 0.0022 - val_accuracy: 0.0515 - val_auc: 0.4349 - val_loss: 2.3336 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/2\n",
            "\u001b[1m321/321\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.1010 - auc: 0.5154 - loss: 2.3590 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        }
      ],
      "source": [
        "history, compiled_model, class_names, target_size, preprocessing_function, learning_rates = main('config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZrEUMa6XxTL"
      },
      "source": [
        "**Explanations:**\n",
        "* This codebase implements a Convolutional Neural Network (CNN) using transfer learning with the ResNet50V2 ptm_name for image classification. It uses a dataset of architectural elements (e.g., altars, domes, columns) split into training and test sets. The model is compiled with Adam optimizer and categorical crossentropy loss. Data augmentation is applied to increase the diversity of training samples. The training process includes callbacks for early stopping, learning rate scheduling, and custom validation. The model's performance is evaluated and visualized over 20 epochs.\n",
        "\n",
        "1. `DataGenerator` class:\n",
        "  - Optimizable parameters:\n",
        "    - None directly, but it relies on the ImageDataGenerator settings.\n",
        "\n",
        "2. `CustomAugmentationLayer` class:\n",
        "   - Optimizable parameters:\n",
        "    - Contrast range (lower, upper)\n",
        "    - Saturation range (lower, upper)\n",
        "   - Role in optimization:\n",
        "    - Adjusting these can increase data diversity and potentially reduce overfitting.\n",
        "\n",
        "3. `create_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Number and size of dense layers\n",
        "    - Dropout rate (currently 0.5)\n",
        "    - L2 regularization strength\n",
        "   - Role in optimization:\n",
        "    - Adjusting model complexity can help balance underfitting and overfitting.\n",
        "\n",
        "4. `compile_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Initial learning rate (currently 0.001)\n",
        "    - Learning rate decay steps and rate\n",
        "    - Choice of optimizer (currently Adam)\n",
        "   - Role in optimization:\n",
        "    - Proper learning rate and optimizer settings are crucial for efficient training and convergence.\n",
        "\n",
        "5. `AccuracyCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - Target accuracy threshold\n",
        "   - Role in optimization:\n",
        "    - Adjusting this can prevent premature stopping or unnecessarily long training.\n",
        "\n",
        "6. `count_samples` function:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "7. `DebugCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - None, but frequency of debug prints could be adjusted.\n",
        "\n",
        "8. `create_data_generators` function:\n",
        "   - Optimizable parameters:\n",
        "    - Batch size (currently 32)\n",
        "    - Data augmentation parameters (rotation_range, width_shift_range, etc.)\n",
        "   - Role in optimization:\n",
        "    - Proper batch size and augmentation can improve training stability and reduce overfitting.\n",
        "\n",
        "9. `DatasetLogger` class:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "10. `CustomValidationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "      - None, but custom metrics could be added.\n",
        "\n",
        "11. `visualize_augmentation` function:\n",
        "    - Optimizable parameters:\n",
        "      - None\n",
        "\n",
        "12. `AugmentationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "     - Frequency of augmentation visualization\n",
        "    - Role in optimization:\n",
        "      - Adjusting this helps in monitoring augmentation effects without slowing training.\n",
        "\n",
        "13. Main execution block:\n",
        "    - Optimizable parameters:\n",
        "      - Number of epochs (currently 20)\n",
        "      - Early stopping patience (currently 10)\n",
        "      - Class weight calculation method\n",
        "    - Role in optimization:\n",
        "      - These parameters affect overall training duration and handling of class imbalance.\n",
        "\n",
        "Additional global optimizations:\n",
        "1. Learning rate scheduling: Implement more sophisticated schedules like cyclic learning rates or warm restarts.\n",
        "2. Model ptm_name: Experiment with different base models (e.g., EfficientNet, VGG) or custom ptm_names.\n",
        "3. Regularization: Add or adjust L2 regularization, increase dropout rates, or implement other techniques like label smoothing.\n",
        "4. Data preprocessing: Normalize input data, apply additional augmentation techniques like mixup or cutout.\n",
        "5. Ensemble methods: Train multiple models and combine their predictions for improved reliability.\n",
        "6. Cross-validation: Implement k-fold cross-validation for more robust performance estimation.\n",
        "7. Gradient clipping: Add gradient clipping to prevent exploding gradients and stabilize training.\n",
        "8. Batch normalization: Add batch normalization layers for improved training stability and potentially faster convergence.\n",
        "9. Learning rate finder: Implement a learning rate finder to determine optimal initial learning rates.\n",
        "10. Progressive resizing: Start training with smaller image sizes and gradually increase, potentially speeding up early training stages.\n",
        "\n",
        "By carefully tuning these parameters and implementing these techniques, you can work towards optimizing the model's performance, improving its reliability, and finding the right balance between underfitting and overfitting for your specific architectural element classification task.\n",
        "\n",
        "**Why It Is Important:**\n",
        "* Using a CNN with transfer learning is important because it leverages pre-trained weights on a large dataset (ImageNet), allowing the model to learn high-level features more quickly and effectively, especially when dealing with a relatively small dataset. This approach often leads to better performance and faster convergence compared to training a model from scratch, making it particularly useful for specialized image classification tasks like identifying architectural elements.\n",
        "\n",
        "**Observations:**\n",
        "* The model quickly achieves high accuracy (>80%) within the first few epochs.\n",
        "* Validation accuracy peaks around epoch 6 at about 86% and then fluctuates.\n",
        "* There's some overfitting, as training accuracy consistently exceeds validation accuracy.\n",
        "* The learning rate decreases steadily over the epochs, as designed by the exponential decay schedule.\n",
        "* Both training and validation loss decrease rapidly initially and then plateau.\n",
        "* The model's performance seems to stabilize after about 10-15 epochs.\n",
        "\n",
        "**Conclusions:**\n",
        "* The transfer learning approach is effective, allowing the model to achieve good performance quickly.\n",
        "* The model reaches a reasonable accuracy (~83% on the validation set) for classifying architectural elements.\n",
        "* There's room for improvement, as the model shows signs of overfitting and doesn't consistently improve after the initial rapid progress.\n",
        "* The current learning rate schedule and data augmentation help, but may not be optimal for this specific task.\n",
        "\n",
        "**Recommendations:**\n",
        "* Experiment with stronger regularization techniques (e.g., increased dropout, L2 regularization) to combat overfitting.\n",
        "* Try different learning rate schedules, such as cyclical learning rates or a more aggressive decay, to potentially improve convergence.\n",
        "* Increase data augmentation to provide more diverse training samples and potentially reduce overfitting.\n",
        "* Consider implementing techniques like gradient clipping to stabilize training.\n",
        "* Explore ensemble methods or cross-validation to improve overall performance and reliability.\n",
        "* Analyze misclassifications to identify patterns and potentially refine the model ptm_name or data preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXoAMHyrXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5dzjyc3XxTL"
      },
      "outputs": [],
      "source": [
        "def plot_training_history(history, learning_rates):\n",
        "\n",
        "    # Plot training history and learning rate\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot learning rate\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(range(1, len(learning_rates) + 1), learning_rates, label='Learning Rate')\n",
        "    plt.title('Learning Rate over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(history, learning_rates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2FxsHI4XxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_training_history` that visualizes the training and validation accuracy and loss over epochs. It then applies this function to the training history of the augmented model.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing the training and validation metrics helps us understand how the model is learning over time. It allows us to identify potential issues such as overfitting (when training accuracy continues to improve but validation accuracy plateaus or decreases) or underfitting (when both training and validation accuracy are low and not improving). This information is crucial for deciding whether to adjust the model ptm_name, change hyperparameters, or modify the training process.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsb4RIZGXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_5_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWZIK3E1XxTL"
      },
      "outputs": [],
      "source": [
        "def predict_image_class(model, img_path, class_names, target_size, preprocessing_function):\n",
        "    target_size = target_size  # Get the target size for the chosen architecture\n",
        "    preprocess_func = preprocessing_function  # Get the preprocessing function for the chosen architecture\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocess_func(img_array)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_index]\n",
        "    confidence = predictions[0][predicted_class_index]\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "def visualize_prediction(img_path, target_size, predicted_class, confidence):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
        "    plt.show()\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
        "\n",
        "# Test the model on new images\n",
        "test_image_paths = [\n",
        "    f'{DATASET_PATH}/model_test_images/test-image-1.jpg',\n",
        "    f'{DATASET_PATH}/model_test_images/test-image-2.jpg',\n",
        "    f'{DATASET_PATH}/model_test_images/test-image-3.jpg',\n",
        "]\n",
        "print()\n",
        "print(\"\\nTesting model on new images:\")\n",
        "print()\n",
        "\n",
        "for img_path in test_image_paths:\n",
        "    predicted_class, confidence = predict_image_class(compiled_model, img_path, class_names, target_size, preprocessing_function)\n",
        "    print()\n",
        "    print(f\"Image: {img_path}\")\n",
        "    print(f\"Predicted class: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print()\n",
        "\n",
        "    # Visualize prediction\n",
        "    visualize_prediction(img_path, target_size, predicted_class, confidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyPwmHMXxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFfqGCDiXxTP"
      },
      "source": [
        "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation - v2**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls3fPTzYdZ_3"
      },
      "outputs": [],
      "source": [
        "# v2 - Needs Work - Need to incorporate changes from v1\n",
        "#\n",
        "import os\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from typing import Dict, Tuple, List\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import yaml\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.applications import (\n",
        "    EfficientNetB0,\n",
        "    InceptionV3,\n",
        "    MobileNetV2,\n",
        "    ResNet50V2,\n",
        "    VGG16,\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    Callback,\n",
        "    EarlyStopping,\n",
        "    LambdaCallback,\n",
        "    LearningRateScheduler,\n",
        ")\n",
        "from tensorflow.keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling2D,\n",
        "    Input,\n",
        ")\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load configuration\n",
        "with open('config.yaml', 'r') as config_file:\n",
        "    config = yaml.safe_load(config_file)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    ptm_name: str\n",
        "    input_shape: Tuple[int, int, int]\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    batch_size: int\n",
        "    initial_learning_rate: float\n",
        "    epochs: int\n",
        "    early_stopping_patience: int\n",
        "    target_accuracy: float\n",
        "    lr_epochs: int\n",
        "\n",
        "model_config = ModelConfig(**config['model'])\n",
        "training_config = TrainingConfig(**config['training'])\n",
        "\n",
        "# Custom exceptions\n",
        "class DatasetError(Exception):\n",
        "    \"\"\"Exception raised for errors in the dataset.\"\"\"\n",
        "    pass\n",
        "\n",
        "class ModelError(Exception):\n",
        "    \"\"\"Exception raised for errors in the model creation or training.\"\"\"\n",
        "    pass\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    \"\"\"Callback to stop training when a target accuracy is reached.\"\"\"\n",
        "\n",
        "    def __init__(self, target_accuracy: float):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
        "            logger.info(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
        "            logger.info(\"\\n--------------------\\n\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class AugmentationCallback(Callback):\n",
        "    \"\"\"Callback to work with the CustomAugmentationLayer.\"\"\"\n",
        "\n",
        "    def __init__(self, train_generator: DataGenerator):\n",
        "        super().__init__()\n",
        "        self.train_generator = train_generator\n",
        "\n",
        "    def on_batch_begin(self, batch: int, logs: Dict[str, float] = None) -> None:\n",
        "        # You can add custom logic here if needed\n",
        "        pass\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    \"\"\"Callback for custom validation after each epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, validation_data: DataGenerator, validation_steps: int):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        for i in range(self.validation_steps):\n",
        "            x, y = next(self.validation_data.generator)\n",
        "            val_metrics = self.model.test_on_batch(x, y)\n",
        "            val_loss += val_metrics[0]\n",
        "            val_accuracy += val_metrics[1]\n",
        "        val_loss /= self.validation_steps\n",
        "        val_accuracy /= self.validation_steps\n",
        "        logs['val_loss'] = val_loss\n",
        "        logs['val_accuracy'] = val_accuracy\n",
        "        logger.info(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    \"\"\"Callback to log dataset information at the beginning of each epoch.\"\"\"\n",
        "\n",
        "    def __init__(self, train_generator: DataGenerator, val_generator: DataGenerator):\n",
        "        super().__init__()\n",
        "        self.train_generator = train_generator\n",
        "        self.val_generator = val_generator\n",
        "\n",
        "    def on_epoch_begin(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        logger.info(f\"Epoch {epoch + 1} - Train samples: {len(self.train_generator.generator.classes)}\")\n",
        "        logger.info(f\"Epoch {epoch + 1} - Val samples: {len(self.val_generator.generator.classes)}\")\n",
        "        logger.info(\"--------------------\")\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        if logs is not None:\n",
        "            logger.info(f\"Epoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
        "            logger.info(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
        "            logger.info(\"--------------------\")\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    \"\"\"Callback for debugging purposes, providing detailed information about the training process.\"\"\"\n",
        "\n",
        "    def on_epoch_begin(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        logger.info(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_batch_begin(self, batch: int, logs: Dict[str, float] = None) -> None:\n",
        "        if batch % 50 == 0:\n",
        "            logger.info(f\"Starting batch {batch}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
        "        logger.info(f\"End of epoch {epoch + 1}\")\n",
        "        if logs:\n",
        "            for key, value in logs.items():\n",
        "                logger.info(f\"{key}: {value}\")\n",
        "        logger.info(\"--------------------\")\n",
        "\n",
        "class LRLogger(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lrs = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        lr = float(K.get_value(self.model.optimizer.lr))\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "class CustomAugmentationLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"Custom augmentation layer for image preprocessing.\"\"\"\n",
        "\n",
        "    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:\n",
        "        if training:\n",
        "            augmented = tf.image.random_contrast(inputs, lower=0.8, upper=1.2)\n",
        "            augmented = tf.image.random_saturation(augmented, lower=0.8, upper=1.2)\n",
        "            return augmented\n",
        "        return inputs\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    \"\"\"Custom data generator for feeding data to the model.\"\"\"\n",
        "\n",
        "    def __init__(self, generator, steps_per_epoch: int):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self) -> None:\n",
        "        self.generator.reset()\n",
        "\n",
        "class LearningRateFinder:\n",
        "    \"\"\"Class to find the optimal learning rate.\"\"\"\n",
        "\n",
        "    def __init__(self, model: Model, stop_factor: float = 4, beta: float = 0.98):\n",
        "        self.model = model\n",
        "        self.stop_factor = stop_factor\n",
        "        self.beta = beta\n",
        "        self.lrs: List[float] = []\n",
        "        self.losses: List[float] = []\n",
        "        self.best_loss = float('inf')\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weights_file = None\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        \"\"\"Reset the state of the learning rate finder.\"\"\"\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "        self.best_loss = float('inf')\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weights_file = None\n",
        "\n",
        "    def on_batch_end(self, batch: int, logs: Dict[str, float]) -> None:\n",
        "        \"\"\"Callback method to update learning rate and loss after each batch.\"\"\"\n",
        "        lr = self.model.optimizer.lr\n",
        "        if hasattr(lr, 'numpy'):\n",
        "            lr = lr.numpy()\n",
        "        self.lrs.append(lr)\n",
        "\n",
        "        loss = logs[\"loss\"]\n",
        "        self.batch_num += 1\n",
        "        self.avg_loss = (self.beta * self.avg_loss) + ((1 - self.beta) * loss)\n",
        "        smooth = self.avg_loss / (1 - (self.beta ** self.batch_num))\n",
        "        self.losses.append(smooth)\n",
        "\n",
        "        stop_loss = self.stop_factor * self.best_loss\n",
        "\n",
        "        if self.batch_num > 1 and smooth < self.best_loss:\n",
        "            self.best_loss = smooth\n",
        "\n",
        "        if smooth > stop_loss:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "        if self.batch_num % 100 == 0:\n",
        "            logger.info(f\"Batch {self.batch_num}: lr = {lr:.6f}, loss = {smooth:.4f}\")\n",
        "\n",
        "    def find(self, train_data: DataGenerator, start_lr: float, end_lr: float, batch_size: int = 32, epochs: int = 5) -> None:\n",
        "        \"\"\"Find the optimal learning rate.\"\"\"\n",
        "        self.reset()\n",
        "        num_samples = len(train_data.generator.classes)\n",
        "        steps_per_epoch = int(np.ceil(num_samples / batch_size))\n",
        "\n",
        "        self.weights_file = \"lrf_weights.h5\"\n",
        "        self.model.save_weights(self.weights_file)\n",
        "\n",
        "        orig_lr = K.get_value(self.model.optimizer.lr)\n",
        "        K.set_value(self.model.optimizer.lr, start_lr)\n",
        "\n",
        "        logger.info(f\"Starting Learning Rate Finder: LR range {start_lr} to {end_lr}\")\n",
        "\n",
        "        lr_schedule = lambda epoch, batch: start_lr * (end_lr / start_lr) ** (batch / (epochs * steps_per_epoch))\n",
        "\n",
        "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
        "        self.model.fit(\n",
        "            train_data.generator,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=epochs,\n",
        "            callbacks=[callback, LearningRateScheduler(lr_schedule)],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        logger.info(\"Learning Rate Finder complete\")\n",
        "\n",
        "        self.model.load_weights(self.weights_file)\n",
        "        K.set_value(self.model.optimizer.lr, orig_lr)\n",
        "\n",
        "    def plot_loss(self) -> None:\n",
        "        \"\"\"Plot the loss against the learning rate.\"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.lrs, self.losses)\n",
        "        plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Learning Rate vs. Loss\")\n",
        "        plt.show()\n",
        "\n",
        "    def get_best_lr(self) -> float:\n",
        "        \"\"\"Get the best learning rate based on the minimum gradient of the loss curve.\"\"\"\n",
        "        min_grad_idx = np.argmin(np.gradient(np.array(self.losses)))\n",
        "        return self.lrs[min_grad_idx]\n",
        "\n",
        "    def print_results(self) -> None:\n",
        "        \"\"\"Print the results of the learning rate finder.\"\"\"\n",
        "        logger.info(\"\\nLearning Rate Finder Results:\")\n",
        "        logger.info(f\"Minimum loss: {min(self.losses):.4f}\")\n",
        "        logger.info(f\"Maximum loss: {max(self.losses):.4f}\")\n",
        "        logger.info(f\"Learning rate range: {min(self.lrs):.6f} to {max(self.lrs):.6f}\")\n",
        "\n",
        "def count_samples(dataset_path: str) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Count the number of samples in each class of the dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to the dataset directory.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: Dictionary with class names as keys and sample counts as values.\n",
        "    \"\"\"\n",
        "    class_counts = {}\n",
        "    try:\n",
        "        for class_name in os.listdir(dataset_path):\n",
        "            class_path = os.path.join(dataset_path, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                class_counts[class_name] = len(os.listdir(class_path))\n",
        "    except OSError as e:\n",
        "        raise DatasetError(f\"Error reading dataset directory: {e}\")\n",
        "    return class_counts\n",
        "\n",
        "def create_model(num_classes: int, ptm_name: callable, input_shape: Tuple[int, int, int]) -> Model:\n",
        "    \"\"\"\n",
        "    Create a transfer learning model based on the given ptm_name.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of classes in the dataset.\n",
        "        ptm_name (callable): Base model ptm_name (e.g., ResNet50V2).\n",
        "        input_shape (Tuple[int, int, int]): Input shape for the model.\n",
        "\n",
        "    Returns:\n",
        "        Model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    base_model = ptm_name(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = CustomAugmentationLayer()(inputs)\n",
        "    x = base_model(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(inputs=inputs, outputs=output)\n",
        "\n",
        "def compile_model(model: Model, learning_rate: float) -> Model:\n",
        "    \"\"\"\n",
        "    Compile the model with the given learning rate.\n",
        "\n",
        "    Args:\n",
        "        model (Model): The model to compile.\n",
        "        learning_rate (float): The initial learning rate.\n",
        "\n",
        "    Returns:\n",
        "        Model: Compiled Keras model.\n",
        "    \"\"\"\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        learning_rate,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        steps_per_execution=8\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_data_generators(train_path: str, test_path: str, batch_size: int, target_size: Tuple[int, int], preprocessing_function: callable) -> Tuple[DataGenerator, DataGenerator, int, int]:\n",
        "    \"\"\"\n",
        "    Create data generators for training and testing.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to the training data.\n",
        "        test_path (str): Path to the test data.\n",
        "        batch_size (int): Batch size for training.\n",
        "        target_size (Tuple[int, int]): Target size for the input images.\n",
        "        preprocessing_function (callable): Preprocessing function for the images.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[DataGenerator, DataGenerator, int, int]: Train generator, test generator, steps per epoch, and validation steps.\n",
        "    \"\"\"\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        preprocessing_function=preprocessing_function,\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        zoom_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "    test_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_path,\n",
        "        target_size=target_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    steps_per_epoch = len(train_generator)\n",
        "    validation_steps = len(test_generator)\n",
        "\n",
        "    return DataGenerator(train_generator, steps_per_epoch), DataGenerator(test_generator, validation_steps), steps_per_epoch, validation_steps\n",
        "\n",
        "def lr_schedule(epoch: int, lr: float) -> float:\n",
        "    \"\"\"\n",
        "    Learning rate schedule function.\n",
        "\n",
        "    Args:\n",
        "        epoch (int): Current epoch number.\n",
        "        lr (float): Current learning rate.\n",
        "\n",
        "    Returns:\n",
        "        float: Updated learning rate.\n",
        "    \"\"\"\n",
        "    if epoch < 100:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "from typing import Dict, Any\n",
        "\n",
        "def validate_config(config: Dict[str, Any], valid_ptm_names: Dict[str, callable]) -> None:\n",
        "    \"\"\"\n",
        "    Validate the entire configuration.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The entire configuration dictionary.\n",
        "        valid_ptm_names (Dict[str, callable]): Dictionary of valid ptm_name names and their corresponding functions.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If any configuration parameter is invalid.\n",
        "    \"\"\"\n",
        "    # Validate model configuration\n",
        "    validate_model_config(config.get('model', {}), valid_ptm_names)\n",
        "\n",
        "    # Validate training configuration\n",
        "    validate_training_config(config.get('training', {}))\n",
        "\n",
        "    # Validate data configuration\n",
        "    validate_data_config(config.get('data', {}))\n",
        "\n",
        "def validate_model_config(model_config: Dict[str, Any], valid_ptm_names: Dict[str, callable]) -> None:\n",
        "    \"\"\"Validate the model configuration.\"\"\"\n",
        "    if 'ptm_name' not in model_config:\n",
        "        raise ValueError(\"Model configuration must include 'ptm_name'\")\n",
        "\n",
        "    ptm_name = model_config['ptm_name']\n",
        "    if ptm_name not in valid_ptm_names:\n",
        "        raise ValueError(f\"Invalid ptm_name: {ptm_name}. Valid options are: {', '.join(valid_ptm_names.keys())}\")\n",
        "\n",
        "def validate_training_config(training_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"Validate the training configuration.\"\"\"\n",
        "    required_fields = ['batch_size', 'initial_learning_rate', 'epochs', 'early_stopping_patience', 'target_accuracy']\n",
        "    for field in required_fields:\n",
        "        if field not in training_config:\n",
        "            raise ValueError(f\"Training configuration must include '{field}'\")\n",
        "\n",
        "    if training_config['batch_size'] <= 0:\n",
        "        raise ValueError(f\"Invalid batch size: {training_config['batch_size']}. Must be a positive integer.\")\n",
        "\n",
        "    if training_config['initial_learning_rate'] <= 0:\n",
        "        raise ValueError(f\"Invalid initial learning rate: {training_config['initial_learning_rate']}. Must be a positive float.\")\n",
        "\n",
        "    if training_config['epochs'] <= 0:\n",
        "        raise ValueError(f\"Invalid number of epochs: {training_config['epochs']}. Must be a positive integer.\")\n",
        "\n",
        "    if training_config['early_stopping_patience'] < 0:\n",
        "        raise ValueError(f\"Invalid early stopping patience: {training_config['early_stopping_patience']}. Must be a non-negative integer.\")\n",
        "\n",
        "    if not 0 < training_config['target_accuracy'] <= 1:\n",
        "        raise ValueError(f\"Invalid target accuracy: {training_config['target_accuracy']}. Must be between 0 and 1.\")\n",
        "\n",
        "def validate_data_config(data_config: Dict[str, Any]) -> None:\n",
        "    \"\"\"Validate the data configuration.\"\"\"\n",
        "    required_fields = ['train_path', 'test_path']\n",
        "    for field in required_fields:\n",
        "        if field not in data_config:\n",
        "            raise ValueError(f\"Data configuration must include '{field}'\")\n",
        "\n",
        "    if not os.path.exists(data_config['train_path']):\n",
        "        raise ValueError(f\"Train path does not exist: {data_config['train_path']}\")\n",
        "\n",
        "    if not os.path.exists(data_config['test_path']):\n",
        "        raise ValueError(f\"Test path does not exist: {data_config['test_path']}\")\n",
        "\n",
        "def main(find_lr: bool = True, pretrain_model_eval: bool = True):\n",
        "    \"\"\"\n",
        "    Main function to run the training pipeline.\n",
        "\n",
        "    Args:\n",
        "        find_lr (bool): Whether to find the optimal learning rate.\n",
        "        pretrain_model_eval (bool): Whether to evaluate the model before training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH')\n",
        "\n",
        "    PT_MODELS = {\n",
        "        'ResNet50V2': ResNet50V2,\n",
        "        'VGG16': VGG16,\n",
        "        'InceptionV3': InceptionV3,\n",
        "        'MobileNetV2': MobileNetV2,\n",
        "        'EfficientNetB0': EfficientNetB0\n",
        "    }\n",
        "\n",
        "    PTM_INPUT_SHAPES = {\n",
        "        'ResNet50V2': (128, 128, 3),\n",
        "        'VGG16': (224, 224, 3),\n",
        "        'InceptionV3': (299, 299, 3),\n",
        "        'MobileNetV2': (224, 224, 3),\n",
        "        'EfficientNetB0': (224, 224, 3)\n",
        "    }\n",
        "\n",
        "    PTM_PREPROC_FUNCS = {\n",
        "        'ResNet50V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
        "        'VGG16': tf.keras.applications.vgg16.preprocess_input,\n",
        "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
        "        'MobileNetV2': tf.keras.applications.mobilenet_v2.preprocess_input,\n",
        "        'EfficientNetB0': tf.keras.applications.efficientnet.preprocess_input\n",
        "    }\n",
        "\n",
        "    # Load configuration\n",
        "    with open('config.yaml', 'r') as config_file:\n",
        "        config = yaml.safe_load(config_file)\n",
        "\n",
        "    # Validate the entire configuration\n",
        "    try:\n",
        "        validate_config(config, PT_MODELS)\n",
        "    except ValueError as e:\n",
        "        logger.error(f\"Configuration error: {e}\")\n",
        "        return\n",
        "\n",
        "    # Extract configuration values\n",
        "    model_config = config['model']\n",
        "    training_config = config['training']\n",
        "    data_config = config['data']\n",
        "\n",
        "    ptm_name = PT_MODELS[model_config['ptm_name']]\n",
        "    input_shape = PTM_INPUT_SHAPES[ptm_name.__name__]\n",
        "    target_size = input_shape[:2]\n",
        "    preprocessing_function = PTM_PREPROC_FUNCS[ptm_name.__name__]\n",
        "\n",
        "    train_path = os.path.join(DATASET_PATH, data_config['train_path'])\n",
        "    test_path = os.path.join(DATASET_PATH, data_config['test_path'])\n",
        "\n",
        "    try:\n",
        "        train_counts = count_samples(train_path)\n",
        "        test_counts = count_samples(test_path)\n",
        "    except DatasetError as e:\n",
        "        logger.error(f\"Error in dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
        "    logger.info(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "\n",
        "    logger.info(\"\\nTraining samples per class:\")\n",
        "    for class_name, count in sorted(train_counts.items()):\n",
        "        logger.info(f\"{class_name}: {count}\")\n",
        "\n",
        "    logger.info(\"\\nTest samples per class:\")\n",
        "    for class_name, count in sorted(test_counts.items()):\n",
        "        logger.info(f\"{class_name}: {count}\")\n",
        "\n",
        "    num_classes = len(train_counts)\n",
        "    model = create_model(num_classes, ptm_name, input_shape)\n",
        "\n",
        "    logger.info(f\"Using {ptm_name.__name__} ptm_name\")\n",
        "    model.summary()\n",
        "\n",
        "    train_generator, test_generator, steps_per_epoch, validation_steps = create_data_generators(\n",
        "        train_path, test_path, batch_size=training_config.batch_size, target_size=target_size, preprocessing_function=preprocessing_function)\n",
        "\n",
        "    logger.info(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "    logger.info(f\"Validation steps: {validation_steps}\")\n",
        "\n",
        "    compiled_model = compile_model(model, learning_rate=training_config.initial_learning_rate)\n",
        "\n",
        "    if find_lr:\n",
        "        logger.info(\"Finding optimal learning rate...\")\n",
        "        lr_finder = LearningRateFinder(compiled_model)\n",
        "        lr_finder.find(train_generator, start_lr=1e-7, end_lr=1, epochs=training_config.lr_epochs)\n",
        "        lr_finder.plot_loss()\n",
        "        lr_finder.print_results()\n",
        "        best_lr = lr_finder.get_best_lr()\n",
        "        logger.info(f\"Best initial learning rate: {best_lr}\")\n",
        "        logger.info(\"Recompiling model with best learning rate...\")\n",
        "        compiled_model = compile_model(model, learning_rate=best_lr)\n",
        "    else:\n",
        "        best_lr = training_config.initial_learning_rate\n",
        "\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=training_config.target_accuracy)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=training_config.early_stopping_patience, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_generator, test_generator)\n",
        "    custom_validation = CustomValidationCallback(test_generator, validation_steps)\n",
        "    augmentation_callback = AugmentationCallback(train_generator)\n",
        "    lr_callback = LearningRateScheduler(lr_schedule)\n",
        "    lr_logger = LRLogger()\n",
        "\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.generator.classes), y=train_generator.generator.classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    if pretrain_model_eval:\n",
        "        logger.info(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
        "        logger.info(f\"Initial evaluation: {evaluation}\")\n",
        "\n",
        "    try:\n",
        "        history = compiled_model.fit(\n",
        "            train_generator.generator,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=training_config.epochs,\n",
        "            validation_data=test_generator.generator,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[\n",
        "                accuracy_callback,\n",
        "                debug_callback,\n",
        "                early_stopping,\n",
        "                dataset_logger,\n",
        "                custom_validation,\n",
        "                augmentation_callback,\n",
        "                lr_callback,\n",
        "                lr_logger\n",
        "            ],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        logger.error(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"An error occurred during training: {e}\")\n",
        "        import traceback\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "    logger.info(\"Final evaluation:\")\n",
        "    final_evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
        "    logger.info(f\"Final evaluation: {final_evaluation}\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uN8jUQZ81RW"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cPvdrV7dZ_3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSeZmV4YfVVE"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8Z1KnIXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0LDLxwLXxTP"
      },
      "source": [
        "### <a id='toc1_4_1_'></a>[**Part 2 - Data Science**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVEV7tJUXxTP"
      },
      "source": [
        "### <a id='toc1_4_3_'></a>[**Data Analysis**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-haQ_GrgXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "DATASET_PATH = '/Users/toddwalters/Development/data/1703138137_dataset/part_2'\n",
        "\n",
        "# Load the datasets\n",
        "user_df = pd.read_csv(f'{DATASET_PATH}/user.csv')\n",
        "tourism_df = pd.read_excel(f'{DATASET_PATH}/tourism_with_id.xlsx')\n",
        "ratings_df = pd.read_csv(f'{DATASET_PATH}/tourism_rating.csv')\n",
        "\n",
        "# Task 1: Preliminary inspections\n",
        "print(\"1. Preliminary Inspections\")\n",
        "\n",
        "## Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(\"User data:\\n\", user_df.isnull().sum())\n",
        "print(\"\\nTourism data:\\n\", tourism_df.isnull().sum())\n",
        "print(\"\\nRatings data:\\n\", ratings_df.isnull().sum())\n",
        "\n",
        "## Check for duplicates\n",
        "print(\"\\nDuplicates:\")\n",
        "print(\"User data:\", user_df.duplicated().sum())\n",
        "print(\"Tourism data:\", tourism_df.duplicated().sum())\n",
        "print(\"Ratings data:\", ratings_df.duplicated().sum())\n",
        "\n",
        "# Task 2: Explore user group providing tourism ratings\n",
        "print(\"\\n2. User Group Analysis\")\n",
        "\n",
        "## Analyze age distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(user_df['Age'], bins=20, kde=True)\n",
        "plt.title('Age Distribution of Users')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Identify places where most users are coming from\n",
        "top_locations = user_df['Location'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_locations.plot(kind='bar')\n",
        "plt.title('Top 10 User Locations')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 3: Explore locations and categories of tourist spots\n",
        "print(\"\\n3. Tourist Spot Analysis\")\n",
        "\n",
        "## Different categories of tourist spots\n",
        "print(\"Categories of Tourist Spots:\")\n",
        "print(tourism_df['Category'].value_counts())\n",
        "\n",
        "## Analyze tourism types by location\n",
        "location_category = tourism_df.groupby('City')['Category'].value_counts().unstack()\n",
        "location_category_norm = location_category.div(location_category.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "location_category_norm.plot(kind='bar', stacked=True)\n",
        "plt.title('Tourism Categories by City')\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Proportion of Categories')\n",
        "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Identify best city for nature enthusiasts\n",
        "nature_spots = tourism_df[tourism_df['Category'] == 'Nature']\n",
        "nature_by_city = nature_spots.groupby('City').size().sort_values(ascending=False)\n",
        "print(\"Best Cities for Nature Enthusiasts:\")\n",
        "print(nature_by_city)\n",
        "\n",
        "# Task 4: Analyze combined data with places and user ratings\n",
        "print(\"\\n4. Combined Data Analysis\")\n",
        "\n",
        "## Merge tourism and ratings data\n",
        "combined_data = pd.merge(tourism_df, ratings_df, on='Place_Id')\n",
        "\n",
        "## Find spots most loved by tourists\n",
        "top_spots = combined_data.groupby('Place_Name')['Place_Ratings'].mean().sort_values(ascending=False).head(10)\n",
        "print(\"Top 10 Most Loved Tourist Spots:\")\n",
        "print(top_spots)\n",
        "\n",
        "## Find city with most loved tourist spots\n",
        "city_ratings = combined_data.groupby('City')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nCities with Most Loved Tourist Spots:\")\n",
        "print(city_ratings)\n",
        "\n",
        "## Analyze which categories users like most\n",
        "category_ratings = combined_data.groupby('Category')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nMost Liked Categories:\")\n",
        "print(category_ratings)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 5: Build a recommender model\n",
        "print(\"\\n5. Recommender Model\")\n",
        "\n",
        "## Create a pivot table for user-item ratings\n",
        "user_item_matrix = combined_data.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "## Fill NaN values with 0\n",
        "user_item_matrix = user_item_matrix.fillna(0)\n",
        "\n",
        "## Calculate cosine similarity between items\n",
        "item_similarity = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "## Create a DataFrame from the item similarity matrix\n",
        "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
        "\n",
        "def get_similar_places(place_name, user_rating):\n",
        "    similar_places = item_similarity_df[place_name] * (user_rating - 2.5)\n",
        "    similar_places = similar_places.sort_values(ascending=False)\n",
        "    return similar_places\n",
        "\n",
        "## Example recommendation\n",
        "place_name = \"Monumen Nasional\"  # You can change this to any place name\n",
        "user_rating = 5  # You can change this to any rating\n",
        "\n",
        "print(\"Recommendations based on\", place_name, \"with a rating of\", user_rating)\n",
        "print(get_similar_places(place_name, user_rating).head(5))\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\nAdditional Insights:\")\n",
        "\n",
        "## User activity analysis\n",
        "user_activity = ratings_df.groupby('User_Id')['Place_Ratings'].count().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 Most Active Users:\")\n",
        "print(user_activity.head(10))\n",
        "\n",
        "## Rating distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(ratings_df['Place_Ratings'], bins=5, kde=True)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Correlation between place features and ratings\n",
        "if 'Time_Minutes' in tourism_df.columns and 'Price' in tourism_df.columns:\n",
        "    feature_ratings = pd.merge(tourism_df[['Place_Id', 'Time_Minutes', 'Price']],\n",
        "                               ratings_df[['Place_Id', 'Place_Ratings']],\n",
        "                               on='Place_Id')\n",
        "    correlation_matrix = feature_ratings[['Time_Minutes', 'Price', 'Place_Ratings']].corr()\n",
        "    print(\"\\nCorrelation between features and ratings:\")\n",
        "    print(correlation_matrix['Place_Ratings'])\n",
        "\n",
        "print(\"\\nAnalysis Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVotBmAXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-kiMSDOXxTP"
      },
      "source": [
        "#### <a id='toc1_4_3_1_'></a>[**4.3.1. Data Analysis v2**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJ1lXnjXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "# Tourism Data Analysis and Recommendation Engine Project\n",
        "\n",
        "## Project Context\n",
        "This project aims to analyze tourism data in Indonesia's five largest cities, comprehend and predict the demand for various tourist attractions, and build a recommendation system based on user ratings.\n",
        "\n",
        "## Project Objectives\n",
        "1. Analyze user demographics and locations\n",
        "2. Explore tourist spot categories and locations\n",
        "3. Analyze user ratings and popular spots\n",
        "4. Build a recommendation system\n",
        "5. Forecast ratings using machine learning and deep learning algorithms\n",
        "\n",
        "## Dataset Description\n",
        "- user.csv: Contains user demographic data\n",
        "- tourism_with_id.csv: Provides details on tourist attractions\n",
        "- tourism_rating.csv: Contains user ratings for tourist spots\n",
        "\"\"\"\n",
        "\n",
        "# 4.1 Preliminary analysis\n",
        "# 4.1.1 Import the datasets into the Python environment\n",
        "user_df = pd.read_csv('user.csv')\n",
        "tourism_df = pd.read_csv('tourism_with_id.csv')\n",
        "ratings_df = pd.read_csv('tourism_rating.csv')\n",
        "\n",
        "# 4.1.2 Examine the dataset's shape and structure, and look out for any outlier\n",
        "print(\"User dataset shape:\", user_df.shape)\n",
        "print(\"Tourism dataset shape:\", tourism_df.shape)\n",
        "print(\"Ratings dataset shape:\", ratings_df.shape)\n",
        "\n",
        "user_df.info()\n",
        "tourism_df.info()\n",
        "ratings_df.info()\n",
        "\n",
        "# Check for missing values and duplicates\n",
        "for df, name in zip([user_df, tourism_df, ratings_df], ['User', 'Tourism', 'Ratings']):\n",
        "    print(f\"\\nMissing values in {name} dataset:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(f\"Duplicates in {name} dataset: {df.duplicated().sum()}\")\n",
        "\n",
        "# 4.1.3 Merge the datasets\n",
        "merged_df = pd.merge(ratings_df, tourism_df, on='Place_Id')\n",
        "merged_df = pd.merge(merged_df, user_df, on='User_Id')\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.2 Exploratory Data Analysis\n",
        "\n",
        "### 4.2.1 Examine the overall date wise ratings\n",
        "\"\"\"\n",
        "\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "daily_ratings = merged_df.groupby('date')['Place_Ratings'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(daily_ratings['date'], daily_ratings['Place_Ratings'])\n",
        "plt.title('Average Daily Ratings Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.2 Rating fluctuations across different days of the week\n",
        "\"\"\"\n",
        "\n",
        "merged_df['day_of_week'] = merged_df['date'].dt.day_name()\n",
        "day_of_week_ratings = merged_df.groupby('day_of_week')['Place_Ratings'].mean().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "day_of_week_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Day of Week')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.3 Rating trends for different months of the year\n",
        "\"\"\"\n",
        "\n",
        "merged_df['month'] = merged_df['date'].dt.month_name()\n",
        "monthly_ratings = merged_df.groupby('month')['Place_Ratings'].mean()\n",
        "monthly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Month')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.4 Rating distribution across different quarters\n",
        "\"\"\"\n",
        "\n",
        "merged_df['quarter'] = merged_df['date'].dt.quarter\n",
        "quarterly_ratings = merged_df.groupby('quarter')['Place_Ratings'].mean()\n",
        "quarterly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Quarter')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.5 Compare performances of different tourist spots\n",
        "\"\"\"\n",
        "\n",
        "spot_performance = merged_df.groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
        "print(spot_performance.head(10))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.6 Identify most popular tourist spots and their locations\n",
        "\"\"\"\n",
        "\n",
        "popular_spots = merged_df.groupby(['Place_Name', 'City'])['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "print(\"Most popular tourist spots overall:\")\n",
        "print(popular_spots.head(10))\n",
        "\n",
        "# For each city\n",
        "for city in merged_df['City'].unique():\n",
        "    city_spots = merged_df[merged_df['City'] == city].groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "    print(f\"\\nMost popular spot in {city}:\")\n",
        "    print(city_spots.head(1))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.7 Determine if the spot with highest visit count has the highest average rating\n",
        "\"\"\"\n",
        "\n",
        "spot_metrics = merged_df.groupby('Place_Name').agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "top_by_visits = spot_metrics.sort_values('visit_count', ascending=False).head(1)\n",
        "top_by_rating = spot_metrics.sort_values('Place_Ratings', ascending=False).head(1)\n",
        "\n",
        "print(\"Top spot by visit count:\")\n",
        "print(top_by_visits)\n",
        "print(\"\\nTop spot by average rating:\")\n",
        "print(top_by_rating)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.8 Identify the highest rated spot in each category and its visit count\n",
        "\"\"\"\n",
        "\n",
        "category_top_spots = merged_df.groupby(['Category', 'Place_Name']).agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "for category in category_top_spots.index.get_level_values('Category').unique():\n",
        "    top_spot = category_top_spots.loc[category].sort_values('Place_Ratings', ascending=False).head(1)\n",
        "    print(f\"\\nHighest rated spot in {category}:\")\n",
        "    print(top_spot)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.3 Building a Recommendation System\n",
        "\n",
        "We'll use a simple collaborative filtering approach based on user-item interactions.\n",
        "\"\"\"\n",
        "\n",
        "# Create a user-item matrix\n",
        "user_item_matrix = merged_df.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "# Calculate cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix.fillna(0))\n",
        "\n",
        "# Function to get recommendations for a user\n",
        "def get_recommendations(user_id, n=5):\n",
        "    user_index = user_item_matrix.index.get_loc(user_id)\n",
        "    similar_users = user_similarity[user_index].argsort()[::-1][1:11]  # top 10 similar users\n",
        "\n",
        "    similar_users_ratings = user_item_matrix.iloc[similar_users]\n",
        "    user_ratings = user_item_matrix.loc[user_id]\n",
        "\n",
        "    recommendations = (similar_users_ratings.mean() - user_ratings).sort_values(ascending=False)\n",
        "    return recommendations.head(n)\n",
        "\n",
        "# Example recommendation\n",
        "example_user = user_item_matrix.index[0]\n",
        "print(f\"Recommendations for user {example_user}:\")\n",
        "print(get_recommendations(example_user))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.4 Forecasting using Deep Learning Algorithms\n",
        "\n",
        "We'll use an LSTM model to forecast future ratings for a specific tourist spot.\n",
        "\"\"\"\n",
        "\n",
        "# Choose a popular tourist spot for forecasting\n",
        "popular_spot = popular_spots.index[0][0]\n",
        "spot_ratings = merged_df[merged_df['Place_Name'] == popular_spot].set_index('date')['Place_Ratings']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_ratings = scaler.fit_transform(spot_ratings.values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length), 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X, y = create_sequences(scaled_ratings, seq_length)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build and train LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(spot_ratings.index[-len(y_test):], y_test, label='Actual')\n",
        "plt.plot(spot_ratings.index[-len(y_pred):], y_pred, label='Predicted')\n",
        "plt.title(f'Rating Forecast for {popular_spot}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Rating')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## Conclusions and Recommendations\n",
        "\n",
        "[Add your conclusions and recommendations based on the analysis]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPn8L_FCXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUgFWSFaXxTP"
      },
      "source": [
        "##### <a id='toc1_4_3_1_1_'></a>[**4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh7GUi1iXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTR0nX0XxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp7HDhGXxTP"
      },
      "source": [
        "##### <a id='toc1_4_3_1_2_'></a>[**4.3.1.2. Use the data from the last six months as the testing data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwFM7b7PXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLE_lgmnXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYuhiqJiXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_3_1_3_'></a>[**4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDxCuQUOXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UBJhwxNXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8iG83ylXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_3_1_4_'></a>[**4.3.1.4. Use the best-performing models to make a forecast for the next year**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFFMhnlGXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGM473tXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F27ebfAeXxTQ"
      },
      "source": [
        "### <a id='toc1_4_4_'></a>[**4.4. Forecasting using deep learning algorithms**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYdRh5KXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_1_'></a>[**4.4.1. Use sales amount for predictions instead of item count**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNWJmttFXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OktA8CEIXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC0bbivZXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_2_'></a>[**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szr8uZOEXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNjnXH0ZXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsD-rQ5WXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_1_'></a>[**4.4.2.1. Define the train and test series**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1mnT3mKXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrnsDqbdXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCfA6lZjXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_2_'></a>[**4.4.2.2. Generate synthetic data for the last 12 months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGevcLfXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFFdG2BKXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh96jqiUXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_3_'></a>[**4.4.2.3. Build and train an LSTM model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0jK6AQvXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs_0Kbv3XxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPlmLTmeXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_4_'></a>[**4.4.2.4. Use the model to make predictions for the test data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMLYgzkWXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9qMv2KbXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_j7MZqIXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_3_'></a>[**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Za61_FYXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHrfqEDlXxTR"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZr4879oXxTR"
      },
      "source": [
        "#### <a id='toc1_4_4_4_'></a>[**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEXfhjpUXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN9oncYmXxTR"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "user-env:(capstone)",
      "language": "python",
      "name": "capstone"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}