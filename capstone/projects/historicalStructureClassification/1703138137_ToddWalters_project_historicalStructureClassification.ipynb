{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toddwalters/aiml-python-coding-examples/blob/main/capstone/projects/historicalStructureClassification/1703138137_ToddWalters_project_historicalStructureClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE6q3GPrXxTI"
      },
      "source": [
        "Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM2B501NXxTI"
      },
      "source": [
        "-----\n",
        "\n",
        "# <a id='toc1_'></a>[**Historical Structure Classification Project**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caA0NN0-XxTI"
      },
      "source": [
        "-----------------------------\n",
        "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "This project is part of an AIML Capstone and focuses on two main aspects:\n",
        "1. Classifying historical structures using deep learning techniques\n",
        "2. Developing a recommendation engine for tourism\n",
        "\n",
        "The project aims to assist the travel and tourism industries by monitoring the condition of historical structures and providing personalized recommendations to tourists.\n",
        "\n",
        "-----------------------------\n",
        "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "1. Develop a CNN model to classify historical structures from images\n",
        "2. Perform exploratory data analysis on tourism data\n",
        "3. Create a recommendation engine for tourist attractions\n",
        "\n",
        "-----------------------------\n",
        "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "**Part 1: Image Classification**\n",
        "\n",
        "1. **Structures_dataset.zip**: Training set of images of historical structures\n",
        "2. **dataset_test**: Test set of images of historical structures\n",
        "\n",
        "**Part 2: Tourism Recommendation**\n",
        "\n",
        "1. **user.csv**: User demographic data\n",
        "   - Columns: User_id, location, age\n",
        "2. **tourism_with_id.csv**: Details on tourist attractions in Indonesia's five largest cities\n",
        "   - Columns: Place_id, Place_name, Description, Category, City, Price, Rating, Time_minute, Coordinate, Lat, Long\n",
        "3. **tourism_rating.csv**: User ratings for tourist attractions\n",
        "   - Columns: user_id, place_id, place_rating\n",
        "\n",
        "-----------------------------------\n",
        "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Part 1 - Deep Learning**\n",
        "\n",
        "1. Plot the sample images (8–10) from each class or category to gain a better understanding\n",
        "of each class\n",
        "   > Hint: You can use the OpenCV open-source library for this task.\n",
        "2. Select an CNN ptm_name of your choice to train the CV model. Configure the\n",
        "ptm_name for transfer learning, set up a TensorFlow environment for the selected\n",
        "backbone ptm_name, and load pre-trained weights\n",
        "   > Note: Algorithm or ptm_name selection is an important step in the training of ML models,\n",
        "so select the one that performs the best on your dataset.\n",
        "3. Deep learning models tend to work well with large amounts (millions of images) of data, but\n",
        "we may not always have enough data to train a deep learning model from scratch. Transfer\n",
        "learning techniques allow us to train and fine-tune large deep learning ptm_names using\n",
        "limited data.\n",
        "   > Hint: For transfer learning, use pre-trained CNN weights and freeze all convolutional\n",
        "layers' weights.\n",
        "4. As of now, CNN ptm_name has been configured for our model. Modify the top of this\n",
        "ptm_name to work with our dataset by:\n",
        "• Adding an appropriate number of dense layers with an activation function.\n",
        "• Using dropout for regularization.\n",
        "   > Note: It is important to understand that these parameters are hyperparameters that\n",
        "must be tuned.\n",
        "5. Compile the model with the right set of parameters like optimizer, loss function, and metric\n",
        "6. Define your callback class to stop the training once validation accuracy reaches a certain\n",
        "number of your choice\n",
        "7. Setup the train or test dataset directories and review the number of image samples for the train\n",
        "and test datasets for each class\n",
        "8. Train the model without augmentation while continuously monitoring the validation accuracy\n",
        "9. Train the model with augmentation and keep monitoring validation accuracy\n",
        "   > Note: Choose carefully the number of epochs, steps per epoch, and validation steps based on\n",
        "your computer configuration\n",
        "10. Visualize training and validation accuracy on the y-axis against each epoch on the x-axis to\n",
        "see if the model overfits after a certain epoch\n",
        "Deep learning\n",
        "\n",
        "**Part 2 - Data Science**\n",
        "\n",
        "1. Import all the datasets and perform preliminary inspections, such as:\n",
        "   1. Check for missing values and duplicates\n",
        "   2. Remove any anomalies found in the data\n",
        "2. To understand the tourism highlights better, we should explore the data in depth.\n",
        "   1. Explore the user group that provides the tourism ratings by:\n",
        "      - Analyzing the age distribution of users visiting the places and rating them\n",
        "      - Identifying the places where most of these users (tourists) are coming from\n",
        "3. Next, let's explore the locations and categories of tourist spots.\n",
        "   1. What are the different categories of tourist spots?\n",
        "   2. What kind of tourism each location is most famous or suitable for?\n",
        "   3. Which city would be best for a nature enthusiast to visit?\n",
        "4. To better understand tourism, we need to create a combined data with places and their user ratings.\n",
        "   1. Use this data to figure out the spots that are most loved by the tourists. Also, which city\n",
        "has the most loved tourist spots?\n",
        "   2. Indonesia provides a wide range of tourist spots ranging from historical and cultural\n",
        "beauties to advanced amusement parks. Among these, which category of places are users\n",
        "liking the most?\n",
        "5. Build a recommender model for the system\n",
        "   1. Use the above data to develop a collaborative filtering model for recommendation and\n",
        "use that to recommend other places to visit using the current tourist location(place name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r6c89oQXxTJ"
      },
      "source": [
        "### <a id='toc1_4_1_'></a>[**Part 1 - Deep Learning**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmv8a41OXxTJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Import Modules and Set Default Environment Variables**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS3KJkebXxTJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras import backend as K\n",
        "from keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srk6fELyXxTJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Look for corrupt files**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LhNzewajXxTJ"
      },
      "outputs": [],
      "source": [
        "def verify_images(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    img = Image.open(file_path)\n",
        "                    img.verify()\n",
        "                except (IOError, SyntaxError) as e:\n",
        "                    print(f'Bad file: {file_path}')\n",
        "                    os.remove(file_path)\n",
        "                    print(f'Deleted bad file: {file_path}')\n",
        "\n",
        "# verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "# verify_images(f'{DATASET_PATH}/structure_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NNsXI_UcLBJ"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Rename Files**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PCtxGIuocJVV"
      },
      "outputs": [],
      "source": [
        "def random_string(length):\n",
        "    \"\"\"Generate a random string of fixed length\"\"\"\n",
        "    letters = string.ascii_lowercase + string.digits\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def rename_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            # Generate a unique name\n",
        "            new_name = str(uuid.uuid4())\n",
        "\n",
        "            # Get the file extension\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "            # Create the new filename\n",
        "            new_filename = f\"{new_name}.jpg\"\n",
        "\n",
        "            # Full paths\n",
        "            old_file = os.path.join(root, filename)\n",
        "            new_file = os.path.join(root, new_filename)\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(old_file, new_file)\n",
        "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
        "\n",
        "# Uncomment if you want to rename all of the files in the dataset\n",
        "# directories = [f'{DATASET_PATH}/structures_dataset', f'{DATASET_PATH}/dataset_test']\n",
        "# for start_directory in directories:\n",
        "#     rename_files(start_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy9L31YIcWPX"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Plot The Sample Images**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2knLkSoXxTJ"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images(dataset_path, num_samples=8):\n",
        "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
        "\n",
        "        for j, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "        # Set the class name as the title for the first image in the row\n",
        "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_sample_images(f'{DATASET_PATH}/structures_dataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNb-3xO8XxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_sample_images` that takes a dataset path and plots a specified number of sample images from each class. It uses OpenCV to read the images and matplotlib to display them in a grid.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing sample images from each class helps us understand the nature of the data we're working with. It allows us to identify any potential issues with the images, such as inconsistent sizes, color schemes, or quality. This step is crucial for determining if any preprocessing steps are needed before training the model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bv_nJW-ae5J"
      },
      "source": [
        "#### <a id='toc1_4_1_1_'></a>[**Define Utility Functions and Classes**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Wf0dLfVQaicV"
      },
      "outputs": [],
      "source": [
        "# Define utility functions and classes\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, generator, steps_per_epoch):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.reset()\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_accuracy = logs.get('accuracy', 'N/A')\n",
        "        val_accuracy = logs.get('val_accuracy', 'N/A')\n",
        "\n",
        "        train_accuracy_str = f\"{train_accuracy:.4f}\" if isinstance(train_accuracy, float) else str(train_accuracy)\n",
        "        val_accuracy_str = f\"{val_accuracy:.4f}\" if isinstance(val_accuracy, float) else str(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train accuracy: {train_accuracy_str}\")\n",
        "        print(f\"Epoch {epoch + 1} - Val accuracy: {val_accuracy_str}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if batch % 100 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            loss_str = f\"{loss:.4f}\" if isinstance(loss, float) else str(loss)\n",
        "            print(f\" Batch {batch} - Loss: {loss_str}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "        print(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset).numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1} - Test samples: {tf.data.experimental.cardinality(self.test_dataset).numpy()}\")\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        evaluation = self.model.evaluate(self.validation_data, steps=self.validation_steps, verbose=0)\n",
        "        print(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "        print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_data_generators(train_path, test_path, batch_size=32):\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        test_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get class names before applying normalization\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Repeat datasets\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    test_dataset = test_dataset.repeat()\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    total_train_samples = sum([len(files) for r, d, files in os.walk(train_path) if files])\n",
        "    total_test_samples = sum([len(files) for r, d, files in os.walk(test_path) if files])\n",
        "    steps_per_epoch = total_train_samples // batch_size\n",
        "    validation_steps = total_test_samples // batch_size\n",
        "\n",
        "    # Get classes\n",
        "    classes = np.concatenate([y for x, y in train_dataset.take(steps_per_epoch)], axis=0)\n",
        "    classes = np.argmax(classes, axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names\n",
        "\n",
        "def verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch):\n",
        "    print(\"Verifying dataset...\")\n",
        "\n",
        "    # 1. Print dataset information\n",
        "    print(f\"Train dataset cardinality: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "    print(f\"Test dataset cardinality: {tf.data.experimental.cardinality(test_dataset)}\")\n",
        "\n",
        "    # 2. Inspect a few batches\n",
        "    print(\"\\nInspecting batches:\")\n",
        "    for i, (images, labels) in enumerate(train_dataset.take(10)):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Image min-max values: {tf.reduce_min(images).numpy()} - {tf.reduce_max(images).numpy()}\")\n",
        "        print(f\"  Unique labels: {np.unique(labels.numpy())}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 3. Check dataset configuration\n",
        "    print(\"\\nDataset configuration:\")\n",
        "    print(f\"Train dataset: {train_dataset}\")\n",
        "    print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "    # 4. Verify steps per epoch\n",
        "    total_train_samples = tf.data.experimental.cardinality(train_dataset).numpy() * batch_size\n",
        "    calculated_steps = total_train_samples // batch_size\n",
        "    print(f\"\\nCalculated steps per epoch: {calculated_steps}\")\n",
        "    print(f\"Provided steps per epoch: {steps_per_epoch}\")\n",
        "    assert calculated_steps == steps_per_epoch, \"Steps per epoch mismatch\"\n",
        "\n",
        "    # 5. Test complete iteration\n",
        "    print(\"\\nTesting complete iteration:\")\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for step, (images, labels) in enumerate(train_dataset):\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: images shape {images.shape}, labels shape {labels.shape}\")\n",
        "            if step >= steps_per_epoch:\n",
        "                break\n",
        "\n",
        "    # 6. Check for data leakage\n",
        "    print(\"\\nChecking for data leakage...\")\n",
        "    train_sample = next(iter(train_dataset.take(100)))[0][0]\n",
        "    for test_batch in test_dataset.take(100):\n",
        "        assert not np.array_equal(train_sample, test_batch[0][0]), \"Data leakage detected\"\n",
        "    print(\"No data leakage detected\")\n",
        "\n",
        "    # 7. Verify data augmentation (if applicable)\n",
        "    # Uncomment and modify this section if you're using data augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\nVerifying data augmentation:\")\n",
        "    for original, augmented in zip(train_dataset.take(1), train_dataset.map(augmentation_function).take(1)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(original[0][0])\n",
        "        plt.title(\"Original\")\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(augmented[0][0])\n",
        "        plt.title(\"Augmented\")\n",
        "        plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDataset verification complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txsdv3rOXxTK"
      },
      "source": [
        "#### <a id='toc1_4_1_2_'></a>[**Select an CNN Pre-Trained Model for Transfer Learning**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "35yzkV_1XxTK"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_gyywPTXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `create_model` that creates a CNN using transfer learning. It uses ResNet50 as the base model with pre-trained ImageNet weights. The base model layers are frozen, and custom dense layers are added on top for fine-tuning.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Transfer learning allows us to leverage pre-trained models on large datasets, which is particularly useful when we have limited data. By using a pre-trained model as a feature extractor and adding custom layers, we can adapt the model to our specific classification task while benefiting from the general features learned by the base model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swTs85TTXxTK"
      },
      "source": [
        "#### <a id='toc1_4_1_3_'></a>[**Compile The Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "hJqh425GXxTK"
      },
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        # decay_steps=1000,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDCqMO3TXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `compile_model` that compiles the model with the Adam optimizer, categorical crossentropy loss function, and accuracy metric.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Compiling the model is a crucial step that defines how the model will be trained. The choice of optimizer, loss function, and metrics affects the training process and the model's performance. Categorical crossentropy is appropriate for multi-class classification tasks, and accuracy provides a clear measure of the model's performance.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC0DVFsAXxTK"
      },
      "source": [
        "#### <a id='toc1_4_2_'></a>[**Define Callback Class**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "6EEh6fvpXxTK"
      },
      "outputs": [],
      "source": [
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        accuracy = logs.get('accuracy', 'N/A')\n",
        "        accuracy_str = f\"{accuracy:.4f}\" if isinstance(accuracy, float) else str(accuracy)\n",
        "        print(f\" Epoch {epoch + 1} - Accuracy: {accuracy_str}\")\n",
        "        if isinstance(accuracy, float) and accuracy >= self.target_accuracy:\n",
        "            print(f\"Reached target accuracy of {self.target_accuracy}\")\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o501BpdXxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a custom callback class `AccuracyCallback` that stops the training when a target validation accuracy is reached.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Callbacks allow us to customize the training process. In this case, we're using a callback to prevent overfitting by stopping the training once we've reached a satisfactory level of validation accuracy. This helps us avoid wasting computational resources and reduces the risk of the model memorizing the training data.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1MRDUL0XxTK"
      },
      "source": [
        "#### <a id='toc1_4_2_1_'></a>[**Set Up Dataset Directories And Review Sample Numbers**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNIxwiL7XxTK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('COLAB_DATASET_PATH', default='/default/dataset/path')\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "# Check TensorFlow Verision\n",
        "print()\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "print()\n",
        "\n",
        "# Setup Dataset Directories\n",
        "train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "# Set up data generators first to get class information\n",
        "batch_size = 32\n",
        "train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "# Now we can use class_names for our existing count_samples function\n",
        "train_counts = count_samples(train_path)\n",
        "test_counts = count_samples(test_path)\n",
        "\n",
        "print(\"\\nTraining samples per class:\")\n",
        "for class_name, count in sorted(train_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print(\"\\nTest samples per class:\")\n",
        "for class_name, count in sorted(test_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print()\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Print class weights\n",
        "print(\"Class weights:\")\n",
        "for i, weight in enumerate(class_weights):\n",
        "    print(f\"Class {i} ({class_names[i]}): {weight}\")\n",
        "print()\n",
        "\n",
        "# Create and compile the model\n",
        "num_classes = len(class_names)\n",
        "model = create_model(num_classes)\n",
        "# print(model.summary())\n",
        "compiled_model = compile_model(model)\n",
        "\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Validation steps: {validation_steps}\")\n",
        "print()\n",
        "\n",
        "# Setup Dataset Directories\n",
        "train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "test_path = f'{DATASET_PATH}/dataset_test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAM9ERJ2XxTK"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `count_samples` that counts the number of samples in each class for a given dataset. It then applies this function to both the training and test datasets and prints the results.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Understanding the distribution of samples across classes is crucial for several reasons:\n",
        "\n",
        "    1. It helps identify any class imbalance issues that may need to be addressed.\n",
        "    2. It ensures we have enough samples in each class for both training and testing.\n",
        "    3. It helps in setting appropriate batch sizes and steps per epoch during training.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsRYKfuzXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_2_'></a>[**Train The Model Without Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4YBG86DXxTL"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "def main(class_names, verify_images, rename_dataset, plot_samples, verify_data, plot_history):\n",
        "    if verify_images:\n",
        "        verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "        verify_images(f'{DATASET_PATH}/structure_dataset')\n",
        "\n",
        "    if rename_dataset:\n",
        "        start_directory = f'{DATASET_PATH}/structures_dataset'\n",
        "        rename_files(start_directory)\n",
        "        start_directory = f'{DATASET_PATH}/dataset_test'\n",
        "        rename_files(start_directory)\n",
        "\n",
        "    if plot_samples:\n",
        "        plot_sample_images(f'{DATASET_PATH}/structures_dataset/')\n",
        "    \n",
        "    if verify_data:\n",
        "        verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch)\n",
        "    \n",
        "    # Set up callbacks\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=0.98)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_dataset, test_dataset)\n",
        "    custom_validation = CustomValidationCallback(test_dataset, validation_steps)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    try:\n",
        "        # Separate evaluation before training\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print(f\"Initial evaluation: {evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Training\n",
        "        history = compiled_model.fit(\n",
        "            train_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=20,\n",
        "            validation_data=test_dataset,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation, accuracy_callback],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print()\n",
        "        print(\"Final evaluation:\")\n",
        "        print(\"-----------------\")\n",
        "        final_evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print()\n",
        "        print(f\"Final evaluation: {final_evaluation}\")\n",
        "        print()\n",
        "\n",
        "        if plot_history:\n",
        "            # Plot training history\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['loss'], label='Training Loss')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print()\n",
        "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        print()\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    return compiled_model, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_model, class_names = main(class_names, verify_images=False, rename_dataset=False, plot_samples=False, verify_data=False, plot_history=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc1_4_2_3_'></a>[**Train The Model Without Augmentation All Together**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras import backend as K\n",
        "from keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.utils import Sequence\n",
        "\n",
        "def verify_images(directory): # Look for corrupt files and delete them\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    img = Image.open(file_path) # open the image file\n",
        "                    img.verify() # verify that it is, in fact an image\n",
        "                except (IOError, SyntaxError) as e:\n",
        "                    print(f'Bad file: {file_path}') # print out the names of corrupt files\n",
        "                    os.remove(file_path)\n",
        "                    print(f'Deleted bad file: {file_path}')\n",
        "\n",
        "def random_string(length):\n",
        "    \"\"\"Generate a random string of fixed length\"\"\"\n",
        "    letters = string.ascii_lowercase + string.digits\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def rename_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            # Generate a unique name\n",
        "            new_name = str(uuid.uuid4())\n",
        "\n",
        "            # Get the file extension\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "            # Create the new filename\n",
        "            new_filename = f\"{new_name}.jpg\"\n",
        "\n",
        "            # Full paths\n",
        "            old_file = os.path.join(root, filename)\n",
        "            new_file = os.path.join(root, new_filename)\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(old_file, new_file)\n",
        "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
        "\n",
        "def plot_sample_images(dataset_path, num_samples=8):\n",
        "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
        "\n",
        "        for j, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "        # Set the class name as the title for the first image in the row\n",
        "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Define utility functions and classes\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, generator, steps_per_epoch):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.reset()\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_accuracy = logs.get('accuracy', 'N/A')\n",
        "        val_accuracy = logs.get('val_accuracy', 'N/A')\n",
        "\n",
        "        train_accuracy_str = f\"{train_accuracy:.4f}\" if isinstance(train_accuracy, float) else str(train_accuracy)\n",
        "        val_accuracy_str = f\"{val_accuracy:.4f}\" if isinstance(val_accuracy, float) else str(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train accuracy: {train_accuracy_str}\")\n",
        "        print(f\"Epoch {epoch + 1} - Val accuracy: {val_accuracy_str}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if batch % 100 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            loss_str = f\"{loss:.4f}\" if isinstance(loss, float) else str(loss)\n",
        "            print(f\" Batch {batch} - Loss: {loss_str}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "        print(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset).numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1} - Test samples: {tf.data.experimental.cardinality(self.test_dataset).numpy()}\")\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        evaluation = self.model.evaluate(self.validation_data, steps=self.validation_steps, verbose=0)\n",
        "        print(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "        print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_data_generators(train_path, test_path, batch_size=32):\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        test_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get class names before applying normalization\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Repeat datasets\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    test_dataset = test_dataset.repeat()\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    total_train_samples = sum([len(files) for r, d, files in os.walk(train_path) if files])\n",
        "    total_test_samples = sum([len(files) for r, d, files in os.walk(test_path) if files])\n",
        "    steps_per_epoch = total_train_samples // batch_size\n",
        "    validation_steps = total_test_samples // batch_size\n",
        "\n",
        "    # Get classes\n",
        "    classes = np.concatenate([y for x, y in train_dataset.take(steps_per_epoch)], axis=0)\n",
        "    classes = np.argmax(classes, axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names\n",
        "\n",
        "def verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch):\n",
        "    print(\"Verifying dataset...\")\n",
        "\n",
        "    # 1. Print dataset information\n",
        "    print(f\"Train dataset cardinality: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "    print(f\"Test dataset cardinality: {tf.data.experimental.cardinality(test_dataset)}\")\n",
        "\n",
        "    # 2. Inspect a few batches\n",
        "    print(\"\\nInspecting batches:\")\n",
        "    for i, (images, labels) in enumerate(train_dataset.take(10)):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Image min-max values: {tf.reduce_min(images).numpy()} - {tf.reduce_max(images).numpy()}\")\n",
        "        print(f\"  Unique labels: {np.unique(labels.numpy())}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 3. Check dataset configuration\n",
        "    print(\"\\nDataset configuration:\")\n",
        "    print(f\"Train dataset: {train_dataset}\")\n",
        "    print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "    # 4. Verify steps per epoch\n",
        "    total_train_samples = tf.data.experimental.cardinality(train_dataset).numpy() * batch_size\n",
        "    calculated_steps = total_train_samples // batch_size\n",
        "    print(f\"\\nCalculated steps per epoch: {calculated_steps}\")\n",
        "    print(f\"Provided steps per epoch: {steps_per_epoch}\")\n",
        "    assert calculated_steps == steps_per_epoch, \"Steps per epoch mismatch\"\n",
        "\n",
        "    # 5. Test complete iteration\n",
        "    print(\"\\nTesting complete iteration:\")\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for step, (images, labels) in enumerate(train_dataset):\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: images shape {images.shape}, labels shape {labels.shape}\")\n",
        "            if step >= steps_per_epoch:\n",
        "                break\n",
        "\n",
        "    # 6. Check for data leakage\n",
        "    print(\"\\nChecking for data leakage...\")\n",
        "    train_sample = next(iter(train_dataset.take(100)))[0][0]\n",
        "    for test_batch in test_dataset.take(100):\n",
        "        assert not np.array_equal(train_sample, test_batch[0][0]), \"Data leakage detected\"\n",
        "    print(\"No data leakage detected\")\n",
        "\n",
        "    # 7. Verify data augmentation (if applicable)\n",
        "    # Uncomment and modify this section if you're using data augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\nVerifying data augmentation:\")\n",
        "    for original, augmented in zip(train_dataset.take(1), train_dataset.map(augmentation_function).take(1)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(original[0][0])\n",
        "        plt.title(\"Original\")\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(augmented[0][0])\n",
        "        plt.title(\"Augmented\")\n",
        "        plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDataset verification complete.\")\n",
        "\n",
        "def create_model(num_classes):\n",
        "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        # decay_steps=1000,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        accuracy = logs.get('accuracy', 'N/A')\n",
        "        accuracy_str = f\"{accuracy:.4f}\" if isinstance(accuracy, float) else str(accuracy)\n",
        "        print(f\" Epoch {epoch + 1} - Accuracy: {accuracy_str}\")\n",
        "        if isinstance(accuracy, float) and accuracy >= self.target_accuracy:\n",
        "            print(f\"Reached target accuracy of {self.target_accuracy}\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# Main execution\n",
        "def main(verify_images, rename_dataset, plot_samples, verify_data, plot_history):\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # Mount Google Drive if using Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH', default='/default/dataset/path')\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "\n",
        "    # Check TensorFlow Verision\n",
        "    print()\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "    print()\n",
        "    \n",
        "    if verify_images:\n",
        "        verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "        verify_images(f'{DATASET_PATH}/structure_dataset')\n",
        "    \n",
        "    if rename_dataset:\n",
        "        start_directory = f'{DATASET_PATH}/structures_dataset'\n",
        "        rename_files(start_directory)\n",
        "        start_directory = f'{DATASET_PATH}/dataset_test'\n",
        "        rename_files(start_directory)\n",
        "    \n",
        "    if plot_samples:\n",
        "        plot_sample_images(f'{DATASET_PATH}/structures_dataset/')\n",
        "    \n",
        "    # Setup Dataset Directories\n",
        "    train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "    test_path = f'{DATASET_PATH}/dataset_test'\n",
        "    \n",
        "    # Set up data generators first to get class information\n",
        "    batch_size = 32\n",
        "    train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "    if verify_data:\n",
        "        verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch)\n",
        "\n",
        "    # Now we can use class_names for our existing count_samples function\n",
        "    train_counts = count_samples(train_path)\n",
        "    test_counts = count_samples(test_path)\n",
        "    \n",
        "    print(\"\\nTraining samples per class:\")\n",
        "    for class_name, count in sorted(train_counts.items()):\n",
        "        print(f\"{class_name}: {count}\")\n",
        "    print(\"\\nTest samples per class:\")\n",
        "    for class_name, count in sorted(test_counts.items()):\n",
        "        print(f\"{class_name}: {count}\")\n",
        "    print()\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    \n",
        "    # Print class weights\n",
        "    print(\"Class weights:\")\n",
        "    for i, weight in enumerate(class_weights):\n",
        "        print(f\"Class {i} ({class_names[i]}): {weight}\")\n",
        "    print()\n",
        "    \n",
        "    # Create and compile the model\n",
        "    num_classes = len(class_names)\n",
        "    model = create_model(num_classes)\n",
        "    # print(model.summary())\n",
        "    compiled_model = compile_model(model)\n",
        "    \n",
        "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"Validation steps: {validation_steps}\")\n",
        "    print()\n",
        "    \n",
        "    # Setup Dataset Directories\n",
        "    train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "    test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "    # Set up callbacks\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=0.98)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_dataset, test_dataset)\n",
        "    custom_validation = CustomValidationCallback(test_dataset, validation_steps)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    try:\n",
        "        # Separate evaluation before training\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print(f\"Initial evaluation: {evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Training\n",
        "        history = compiled_model.fit(\n",
        "            train_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=25,\n",
        "            validation_data=test_dataset,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation, accuracy_callback],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print()\n",
        "        print(\"Final evaluation:\")\n",
        "        print(\"-----------------\")\n",
        "        final_evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print()\n",
        "        print(f\"Final evaluation: {final_evaluation}\")\n",
        "        print()\n",
        "\n",
        "        if plot_history:\n",
        "            # Plot training history\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['loss'], label='Training Loss')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print()\n",
        "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        print()\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    return compiled_model, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_model, class_names = main(verify_images=False, rename_dataset=False, plot_samples=False, verify_data=False, plot_history=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def test_model_on_sample_images(model, class_names):\n",
        "\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    sample_images_dir = os.getenv('SAMPLE_IMAGES_DIR', default='/default/dataset/path')\n",
        "\n",
        "    # Collect image file paths\n",
        "    image_paths = [\n",
        "        os.path.join(sample_images_dir, fname)\n",
        "        for fname in os.listdir(sample_images_dir)\n",
        "        if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "    ]\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"No images found in the directory.\")\n",
        "        return\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        # Load and preprocess the image\n",
        "        img = load_img(img_path, target_size=(128, 128))\n",
        "        img_array = img_to_array(img)\n",
        "\n",
        "        # Rescale pixel values (assuming the model expects inputs in [0, 1])\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Expand dimensions to match the model's input shape\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Predict the class\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class = np.argmax(predictions[0])\n",
        "        predicted_class_name = class_names[predicted_class]\n",
        "        confidence = np.max(predictions[0]) * 100\n",
        "\n",
        "        # Display the image with its predicted class name and confidence\n",
        "        plt.figure()\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Predicted: {predicted_class_name} ({confidence:.2f}% confidence)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model_on_sample_images(compiled_model, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDg5KrjcXxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code sets up data generators for the training and test datasets, then trains the model using these generators. The `ImageDataGenerator` is used to load and preprocess images in batches, which is memory-efficient for large datasets.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Training the model without augmentation provides a baseline performance. It allows us to see how well the model performs with the original data before applying any data augmentation techniques. This step is crucial for understanding if the model has enough capacity to learn from the data and if there are any immediate issues like overfitting or underfitting.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEtTGXrVXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHszyF2ednn5",
        "outputId": "edb3f833-7435-4d84-a818-16ed128a5ae6"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "zuoPxxZpXxTL"
      },
      "outputs": [],
      "source": [
        "# v1 - WORKS DO NOT CHANGE\n",
        "#\n",
        "import gc\n",
        "import math\n",
        "import os\n",
        "import yaml\n",
        "import logging\n",
        "import warnings\n",
        "import traceback\n",
        "from collections import Counter\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import keras.backend as K\n",
        "from keras.applications import (\n",
        "    EfficientNetB0,\n",
        "    InceptionV3,\n",
        "    MobileNetV2,\n",
        "    ResNet50V2,\n",
        "    VGG16\n",
        ")\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (\n",
        "    Callback,\n",
        "    EarlyStopping,\n",
        "    LambdaCallback,\n",
        "    LearningRateScheduler,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "from keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling2D,\n",
        "    Input\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.utils import Sequence\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Clear any existing TensorFlow session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# To ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set logging level to DEBUG for detailed output\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Define mappings for architectures and preprocessing functions\n",
        "ARCHITECTURES = {\n",
        "    'ResNet50V2': ResNet50V2,\n",
        "    'VGG16': VGG16,\n",
        "    'InceptionV3': InceptionV3,\n",
        "    'MobileNetV2': MobileNetV2,\n",
        "    'EfficientNetB0': EfficientNetB0\n",
        "}\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'efficientnet_preprocess': efficientnet_preprocess\n",
        "}\n",
        "\n",
        "# Define metrics mapping\n",
        "METRICS = {\n",
        "    'Precision': Precision(name='precision'),\n",
        "    'Recall': Recall(name='recall'),\n",
        "    'AUC': AUC(name='auc')\n",
        "}\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
        "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
        "            print()\n",
        "            print(\"--------------------\")\n",
        "            print()\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        for x, y in self.validation_data.take(self.validation_steps):\n",
        "            val_metrics = self.model.test_on_batch(x, y)\n",
        "            val_loss += val_metrics[0]\n",
        "            val_accuracy += val_metrics[1]\n",
        "\n",
        "        val_loss /= self.validation_steps\n",
        "        val_accuracy /= self.validation_steps\n",
        "\n",
        "        logs['val_loss'] = val_loss\n",
        "        logs['val_accuracy'] = val_accuracy\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        logging.debug(f\"Loss: {val_loss:.4f}\")\n",
        "        logging.debug(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "class LearningRateTracker(Callback):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.learning_rates = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Retrieve the learning rate from the learning rate schedule in the optimizer\n",
        "        optimizer = self.model.optimizer\n",
        "        if isinstance(optimizer.learning_rate, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            # If a LearningRateSchedule is used, call it with the current step\n",
        "            lr = optimizer.learning_rate(optimizer.iterations).numpy()\n",
        "        else:\n",
        "            # If a static learning rate is used\n",
        "            lr = optimizer.learning_rate.numpy()\n",
        "\n",
        "        self.learning_rates.append(lr)\n",
        "        logging.debug(f\"Epoch {epoch + 1} - Learning rate: {lr}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, val_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset)}\")\n",
        "        logging.debug(f\"Epoch {epoch + 1} - Val samples: {tf.data.experimental.cardinality(self.val_dataset)}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            logging.debug(f\"\\nEpoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
        "            logging.debug(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nStarting epoch {epoch + 1}\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if batch % 100 == 0:\n",
        "            logging.debug(f\"\\nStarting batch {batch}\\n\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEnd of epoch {epoch + 1}\\n\")\n",
        "        if logs:\n",
        "            for key, value in logs.items():\n",
        "                logging.debug(f\"{key}: {value}\")\n",
        "        logging.debug(\"\\n--------------------\\n\")\n",
        "\n",
        "class CustomAugmentationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAugmentationLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        if training:\n",
        "            augmented = tf.image.random_contrast(inputs, lower=0.8, upper=1.2)\n",
        "            augmented = tf.image.random_saturation(augmented, lower=0.8, upper=1.2)\n",
        "            return augmented\n",
        "        return inputs\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.train_path = config['data']['train_path']\n",
        "        self.test_path = config['data']['test_path']\n",
        "        self.batch_size = config['data']['batch_size']\n",
        "        self.target_size = tuple(config['data']['target_size'])\n",
        "        self.preprocessing_function = PREPROCESSING_FUNCTIONS[config['data']['preprocessing_function']]\n",
        "        self.augmentation_params = config['augmentation']\n",
        "\n",
        "        logging.debug(f\"Initialized DataGenerator with:\")\n",
        "        logging.debug(f\"Train path: {self.train_path}\")\n",
        "        logging.debug(f\"Test path: {self.test_path}\")\n",
        "        logging.debug(f\"Batch size: {self.batch_size}\")\n",
        "        logging.debug(f\"Target size: {self.target_size}\")\n",
        "        logging.debug(f\"Preprocessing function: {self.preprocessing_function}\")\n",
        "        logging.debug(f\"Augmentation params: {self.augmentation_params}\")\n",
        "\n",
        "    def normalize_and_preprocess(self, image, label):\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "        image = self.preprocessing_function(image)\n",
        "        return image, label\n",
        "\n",
        "    def augment(self, images, labels):\n",
        "        # images: a batch of images of shape [batch_size, height, width, channels]\n",
        "\n",
        "        # Define the function to apply to each image\n",
        "        def augment_image(image):\n",
        "            # Cast image to float32 for processing\n",
        "            image = tf.cast(image, tf.float32)\n",
        "\n",
        "            # Apply augmentations\n",
        "            # Random rotation\n",
        "            if self.augmentation_params.get('rotation_range'):\n",
        "                # Randomly choose the number of 90-degree rotations\n",
        "                k = tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)\n",
        "                image = tf.image.rot90(image, k)\n",
        "\n",
        "            # Random flip\n",
        "            if self.augmentation_params.get('horizontal_flip'):\n",
        "                image = tf.image.random_flip_left_right(image)\n",
        "            if self.augmentation_params.get('vertical_flip'):\n",
        "                image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "            # Random zoom (crop and resize)\n",
        "            if self.augmentation_params.get('zoom_range'):\n",
        "                zoom = self.augmentation_params['zoom_range']\n",
        "                # Generate random zoom factor\n",
        "                zoom_factor = tf.random.uniform([], 1 - zoom, 1 + zoom)\n",
        "                # Compute new dimensions\n",
        "                new_height = tf.cast(tf.cast(self.target_size[0], tf.float32) * zoom_factor, tf.int32)\n",
        "                new_width = tf.cast(tf.cast(self.target_size[1], tf.float32) * zoom_factor, tf.int32)\n",
        "                image = tf.image.resize(image, [new_height, new_width])\n",
        "                image = tf.image.resize_with_crop_or_pad(image, self.target_size[0], self.target_size[1])\n",
        "\n",
        "            # Random shift\n",
        "            if self.augmentation_params.get('width_shift_range') or self.augmentation_params.get('height_shift_range'):\n",
        "                width_shift = self.augmentation_params.get('width_shift_range', 0)\n",
        "                height_shift = self.augmentation_params.get('height_shift_range', 0)\n",
        "                # Compute shift amounts\n",
        "                width_shift_pixels = tf.cast(width_shift * self.target_size[1], tf.int32)\n",
        "                height_shift_pixels = tf.cast(height_shift * self.target_size[0], tf.int32)\n",
        "                # Pad the image to allow shifting\n",
        "                image = tf.image.pad_to_bounding_box(\n",
        "                    image,\n",
        "                    height_shift_pixels,\n",
        "                    width_shift_pixels,\n",
        "                    self.target_size[0] + 2 * height_shift_pixels,\n",
        "                    self.target_size[1] + 2 * width_shift_pixels\n",
        "                )\n",
        "                image = tf.image.random_crop(image, size=[self.target_size[0], self.target_size[1], 3])\n",
        "\n",
        "            # Random brightness\n",
        "            if self.augmentation_params.get('brightness_range'):\n",
        "                max_delta = self.augmentation_params['brightness_range'] * 255.0  # Adjusted for pixel range [0, 255]\n",
        "                image = tf.image.random_brightness(image, max_delta=max_delta)\n",
        "\n",
        "            # Random contrast\n",
        "            if self.augmentation_params.get('contrast_range'):\n",
        "                lower = 1.0 - self.augmentation_params['contrast_range']\n",
        "                upper = 1.0 + self.augmentation_params['contrast_range']\n",
        "                image = tf.image.random_contrast(image, lower=lower, upper=upper)\n",
        "\n",
        "            # Random saturation\n",
        "            if self.augmentation_params.get('saturation_range'):\n",
        "                lower = 1.0 - self.augmentation_params['saturation_range']\n",
        "                upper = 1.0 + self.augmentation_params['saturation_range']\n",
        "                image = tf.image.random_saturation(image, lower=lower, upper=upper)\n",
        "\n",
        "            # Random hue\n",
        "            if self.augmentation_params.get('hue_range'):\n",
        "                max_delta = self.augmentation_params['hue_range']\n",
        "                image = tf.image.random_hue(image, max_delta=max_delta)\n",
        "\n",
        "            # Ensure the image is in the correct range [0, 255]\n",
        "            image = tf.clip_by_value(image, 0.0, 255.0)\n",
        "\n",
        "            # Normalize and preprocess after augmentations\n",
        "            image = image / 255.0\n",
        "            image = self.preprocessing_function(image)\n",
        "\n",
        "            return image\n",
        "\n",
        "        # Apply the augment_image function to each image in the batch\n",
        "        images = tf.map_fn(augment_image, images, fn_output_signature=tf.float32)\n",
        "\n",
        "        logging.debug(\"Applied augmentations to batch of images.\")\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def create_datasets(self):\n",
        "        train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.train_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.test_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        train_dataset = train_dataset.map(self.augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.map(self.normalize_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        train_dataset = train_dataset.cache().shuffle(1000).prefetch(tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        steps_per_epoch = tf.data.experimental.cardinality(train_dataset)\n",
        "        if steps_per_epoch == tf.data.experimental.INFINITE_CARDINALITY:\n",
        "            steps_per_epoch = None\n",
        "        else:\n",
        "            steps_per_epoch = steps_per_epoch.numpy()\n",
        "\n",
        "        validation_steps = tf.data.experimental.cardinality(test_dataset)\n",
        "        if validation_steps == tf.data.experimental.INFINITE_CARDINALITY:\n",
        "            validation_steps = None\n",
        "        else:\n",
        "            validation_steps = validation_steps.numpy()\n",
        "\n",
        "        logging.debug(f\"Training dataset: {train_dataset}\")\n",
        "        logging.debug(f\"Testing dataset: {test_dataset}\")\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps\n",
        "\n",
        "    def get_data_generators(self):\n",
        "        return self.create_datasets()\n",
        "\n",
        "class LearningRateFinder:\n",
        "    def __init__(self, model, config):\n",
        "        self.model = model\n",
        "        self.stop_factor = config['lr_finder']['stop_factor']\n",
        "        self.beta = config['lr_finder']['beta']\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weightsFile = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.lrs = []\n",
        "        self.losses = []\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.batch_num = 0\n",
        "        self.weightsFile = None\n",
        "\n",
        "    def find(self, train_data, start_lr, end_lr, batch_size=32, epochs=5):\n",
        "        self.reset()\n",
        "        num_samples = tf.data.experimental.cardinality(train_data).numpy()\n",
        "        steps_per_epoch = math.ceil(num_samples / batch_size)\n",
        "        num_batches = epochs * steps_per_epoch\n",
        "\n",
        "        self.weightsFile = \"lrf_weights.weights.h5\"\n",
        "        self.model.save_weights(self.weightsFile)\n",
        "\n",
        "        # Save the original optimizer\n",
        "        self.original_optimizer = self.model.optimizer\n",
        "\n",
        "        # Replace optimizer with one that has a static learning rate\n",
        "        optimizer = Adam(learning_rate=start_lr)\n",
        "        self.model.compile(optimizer=optimizer, loss=self.model.loss)\n",
        "\n",
        "        logging.info(f\"Starting Learning Rate Finder: LR range {start_lr} to {end_lr}\")\n",
        "\n",
        "        lr_mult = (end_lr / start_lr) ** (1 / num_batches)\n",
        "\n",
        "        class LRFinderCallback(tf.keras.callbacks.Callback):\n",
        "            def __init__(self, lr_finder):\n",
        "                super().__init__()\n",
        "                self.lr_finder = lr_finder\n",
        "                self.lr = start_lr\n",
        "                self.batch_num = 0\n",
        "\n",
        "            def on_batch_end(self, batch, logs=None):\n",
        "                self.batch_num += 1\n",
        "\n",
        "                # Compute the smoothed loss\n",
        "                loss = logs[\"loss\"]\n",
        "                self.lr_finder.avg_loss = (self.lr_finder.beta * self.lr_finder.avg_loss) + ((1 - self.lr_finder.beta) * loss)\n",
        "                smooth = self.lr_finder.avg_loss / (1 - self.lr_finder.beta ** self.batch_num)\n",
        "                self.lr_finder.losses.append(smooth)\n",
        "\n",
        "                # Save the learning rate\n",
        "                self.lr_finder.lrs.append(self.lr)\n",
        "\n",
        "                # Check if the loss is diverging\n",
        "                if self.batch_num > 1 and smooth > self.lr_finder.stop_factor * self.lr_finder.best_loss:\n",
        "                    self.model.stop_training = True\n",
        "                    return\n",
        "\n",
        "                # Update best loss\n",
        "                if self.batch_num == 1 or smooth < self.lr_finder.best_loss:\n",
        "                    self.lr_finder.best_loss = smooth\n",
        "\n",
        "                # Increase the learning rate for next batch\n",
        "                self.lr *= lr_mult\n",
        "                self.model.optimizer.learning_rate.assign(self.lr)\n",
        "\n",
        "                if self.batch_num % 10 == 0:\n",
        "                    logging.debug(f\"Batch {self.batch_num}: lr = {self.lr:.6f}, loss = {smooth:.4f}\")\n",
        "\n",
        "        lr_finder_callback = LRFinderCallback(self)\n",
        "\n",
        "        self.model.fit(train_data,\n",
        "                       steps_per_epoch=steps_per_epoch,\n",
        "                       epochs=epochs,\n",
        "                       callbacks=[lr_finder_callback],\n",
        "                       verbose=0)\n",
        "\n",
        "        logging.info(\"Learning Rate Finder complete\")\n",
        "\n",
        "        self.model.load_weights(self.weightsFile)\n",
        "\n",
        "        # Restore the original optimizer\n",
        "        self.model.compile(optimizer=self.original_optimizer, loss=self.model.loss, metrics=self.model.metrics)\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(self.lrs, self.losses)\n",
        "        plt.xscale(\"log\")\n",
        "        plt.xlabel(\"Learning rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Learning Rate vs. Loss\")\n",
        "        plt.show()\n",
        "\n",
        "    def get_best_lr(self):\n",
        "        min_loss_idx = np.argmin(self.losses)\n",
        "        best_lr = self.lrs[min_loss_idx]\n",
        "        logging.info(f\"Best initial learning rate found: {best_lr:.6f}\")\n",
        "        return best_lr\n",
        "\n",
        "    def print_results(self):\n",
        "        logging.info(\"\\nLearning Rate Finder Results:\")\n",
        "        logging.info(f\"Minimum loss: {min(self.losses):.4f}\")\n",
        "        logging.info(f\"Maximum loss: {max(self.losses):.4f}\")\n",
        "        logging.info(f\"Learning rate range: {min(self.lrs):.6f} to {max(self.lrs):.6f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_model(config, num_classes):\n",
        "    architecture = ARCHITECTURES[config['model']['architecture']]\n",
        "    input_shape = tuple(config['model']['input_shape'])\n",
        "\n",
        "    base_model = architecture(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=input_shape\n",
        "    )\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(config['model']['dense_units'])(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(config['model']['dropout_rate'])(x)\n",
        "    x = Dense(config['model']['dense_units'] // 2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "def compile_model(model, config):\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        config['model']['initial_learning_rate'],\n",
        "        decay_steps=config['model']['decay_steps'],\n",
        "        decay_rate=config['model']['decay_rate'],\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    # Use the METRICS mapping instead of eval\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']])\n",
        "    logging.debug(\"Model compiled with the following parameters:\")\n",
        "    logging.debug(f\"Optimizer: {optimizer}\")\n",
        "    logging.debug(f\"Loss: categorical_crossentropy\")\n",
        "    logging.debug(f\"Metrics: {['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']]}\")\n",
        "    return model\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    if epoch < 100:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "def setup_gpu(gpu_config):\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Log the number of GPUs available\n",
        "            logging.info(f\"GPU setup complete. Found {len(gpus)} GPU(s).\")\n",
        "\n",
        "            # Optionally, you can log more details about each GPU\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logging.info(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            logging.error(f\"GPU setup failed: {e}\")\n",
        "    else:\n",
        "        logging.warning(\"No GPUs found. The model will run on CPU.\")\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "def setup_datasets(config):\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logging.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps = data_generator.get_data_generators()\n",
        "\n",
        "        # Dynamically determine class names\n",
        "        class_names = sorted(os.listdir(config['data']['train_path']))\n",
        "        class_names = [name for name in class_names if os.path.isdir(os.path.join(config['data']['train_path'], name))]\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Dataset setup failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def create_and_compile_model(config, num_classes):\n",
        "    try:\n",
        "        model = create_model(config, num_classes)\n",
        "        compiled_model = compile_model(model, config)\n",
        "        return compiled_model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Model creation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_callbacks(config, train_dataset, test_dataset, validation_steps):\n",
        "    return [\n",
        "        AccuracyCallback(target_accuracy=config['training']['target_accuracy']),\n",
        "        CustomValidationCallback(test_dataset, validation_steps),\n",
        "        DebugCallback(),\n",
        "        DatasetLogger(train_dataset, test_dataset),\n",
        "        EarlyStopping(monitor='val_loss', patience=config['training']['patience'], restore_best_weights=True),\n",
        "        LearningRateTracker(),\n",
        "        ModelCheckpoint(filepath=config['training']['model_checkpoint_path'], save_best_only=True)\n",
        "    ]\n",
        "\n",
        "def find_optimal_learning_rate(model, train_dataset, config):\n",
        "    lr_finder = LearningRateFinder(model, config)\n",
        "    lr_finder.find(\n",
        "        train_data=train_dataset,\n",
        "        start_lr=config['lr_finder']['start_lr'],\n",
        "        end_lr=config['lr_finder']['end_lr'],\n",
        "        batch_size=config['data']['batch_size'],\n",
        "        epochs=config['lr_finder']['epochs']\n",
        "    )\n",
        "    best_lr = lr_finder.get_best_lr()\n",
        "    return best_lr\n",
        "\n",
        "def get_class_counts(dataset):\n",
        "    class_counts = Counter()\n",
        "    for _, labels in dataset:\n",
        "        classes = np.argmax(labels.numpy(), axis=1)\n",
        "        class_counts.update(classes)\n",
        "    return class_counts\n",
        "\n",
        "def compute_class_weights(train_dataset):\n",
        "    class_counts = get_class_counts(train_dataset)\n",
        "    total_samples = sum(class_counts.values())\n",
        "    class_weight_dict = {i: total_samples / count for i, count in class_counts.items()}\n",
        "    logging.debug(f\"Computed class weights: {class_weight_dict}\")\n",
        "    return class_weight_dict\n",
        "\n",
        "\n",
        "def main(config_path):\n",
        "    config = load_config(config_path)\n",
        "    logging.debug(f\"Loaded configuration: {config}\")\n",
        "    setup_gpu(config.get('gpu', {}))\n",
        "    logging.debug(\"Completed GPU setup.\")\n",
        "\n",
        "    # Mount Google Drive if using Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "        logging.info(\"Running in Colab environment\")\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "        logging.info(\"Running in local environment\")\n",
        "\n",
        "    # Update the train and test paths\n",
        "    train_path = os.path.join(DATASET_PATH, config['data']['train_dir'])\n",
        "    test_path = os.path.join(DATASET_PATH, config['data']['test_dir'])\n",
        "\n",
        "    # Log the dataset paths\n",
        "    logging.debug(f\"Full training dataset path: {train_path}\")\n",
        "    logging.debug(f\"Full testing dataset path: {test_path}\")\n",
        "\n",
        "    # Update the config with the full paths\n",
        "    config['data']['train_path'] = train_path\n",
        "    config['data']['test_path'] = test_path\n",
        "\n",
        "    logging.info(f\"Train path: {train_path}\")\n",
        "    logging.info(f\"Test path: {test_path}\")\n",
        "\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logging.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names = setup_datasets(config)\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        num_classes = len(class_names)\n",
        "        model = create_and_compile_model(config, num_classes)\n",
        "        logging.debug(\"Model created and compiled successfully.\")\n",
        "        logging.debug(\"Model summary:\")\n",
        "        model.summary(print_fn=lambda x: logging.debug(x))\n",
        "\n",
        "        if config['training']['find_lr']:\n",
        "            lr_finder = LearningRateFinder(model, config)\n",
        "            lr_finder.find(\n",
        "                train_data=train_dataset,\n",
        "                start_lr=float(config['lr_finder']['start_lr']),\n",
        "                end_lr=float(config['lr_finder']['end_lr']),\n",
        "                batch_size=config['data']['batch_size'],\n",
        "                epochs=config['lr_finder']['epochs']\n",
        "            )\n",
        "            best_lr = lr_finder.get_best_lr()\n",
        "            best_lr = best_lr / 10\n",
        "            config['model']['initial_learning_rate'] = best_lr\n",
        "            model = compile_model(model, config)\n",
        "            logging.debug(f\"Updated initial learning rate to: {best_lr}\")\n",
        "        else:\n",
        "            logging.info(\"Skipping the learning rate finder as per configuration.\")\n",
        "\n",
        "        # Create LearningRateTracker instance\n",
        "        lr_tracker = LearningRateTracker()\n",
        "\n",
        "        # Get other callbacks\n",
        "        other_callbacks = get_callbacks(config, train_dataset, test_dataset, validation_steps)\n",
        "\n",
        "        # Include lr_tracker in the callbacks\n",
        "        callbacks = other_callbacks + [lr_tracker]\n",
        "\n",
        "        # Compute class weights\n",
        "        y_train = np.concatenate([y for x, y in train_dataset], axis=0)\n",
        "        classes = np.argmax(y_train, axis=1)\n",
        "        class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "        logging.debug(f\"Computed class weights: {class_weight_dict}\")\n",
        "\n",
        "        ptm_name = config['model']['architecture']\n",
        "        if config['training']['pretrain_model_eval']:\n",
        "            logging.info(\"Evaluating model before training:\")\n",
        "            evaluation = model.evaluate(test_dataset)\n",
        "            logging.info(f\"Initial evaluation: {evaluation}\")\n",
        "\n",
        "        logging.debug(f\"Starting model training using the {ptm_name} Pre-Trained Model...\")\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=config['training']['epochs'],\n",
        "                validation_data=test_dataset,\n",
        "                callbacks=callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                verbose=1\n",
        "            )\n",
        "        except tf.errors.ResourceExhaustedError as e:\n",
        "            logging.error(\"Memory error occurred during training.\")\n",
        "            logging.error(f\"Error details: {e}\")\n",
        "            logging.error(\"Consider reducing batch size or image dimensions.\")\n",
        "            return None, None, None, None, None, None\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during training: {e}\")\n",
        "            logging.debug(traceback.format_exc())\n",
        "            return None, None, None, None, None, None\n",
        "\n",
        "        logging.debug(\"Model training completed successfully.\")\n",
        "        logging.info(\"Final evaluation:\")\n",
        "        final_evaluation = model.evaluate(test_dataset)\n",
        "        logging.info(f\"Final evaluation metrics: {final_evaluation}\")\n",
        "\n",
        "        # Retrieve learning rates from lr_tracker\n",
        "        learning_rates = lr_tracker.learning_rates\n",
        "\n",
        "        return history, model, class_names, config['data']['target_size'], config['data']['preprocessing_function'], learning_rates, ptm_name\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred: {e}\")\n",
        "        logging.debug(traceback.format_exc())\n",
        "        return None, None, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "7OLPkkIYdZ_1",
        "outputId": "f666106d-9f6e-48a0-cdec-7be66be6a956"
      },
      "outputs": [],
      "source": [
        "history, compiled_model, class_names, target_size, preprocessing_function, learning_rates, ptm_name = main('config.yaml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# v2 - Modified to include Keras Tuner\n",
        "#\n",
        "import gc\n",
        "import os\n",
        "import math\n",
        "import glob\n",
        "import yaml\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import warnings\n",
        "import traceback\n",
        "from collections import Counter\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import keras.backend as K\n",
        "from keras.applications import (\n",
        "    EfficientNetB0,\n",
        "    InceptionV3,\n",
        "    MobileNetV2,\n",
        "    ResNet50V2,\n",
        "    VGG16\n",
        ")\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (\n",
        "    Callback,\n",
        "    EarlyStopping,\n",
        "    LambdaCallback,\n",
        "    LearningRateScheduler,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "from keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    GlobalAveragePooling2D,\n",
        "    Input,\n",
        "    RandomRotation,\n",
        "    RandomFlip,\n",
        "    RandomZoom,\n",
        "    RandomContrast,\n",
        "    RandomBrightness,\n",
        "    RandomTranslation,\n",
        "    Rescaling\n",
        ")\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.utils import Sequence\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Import Keras Tuner modules\n",
        "from keras_tuner import Hyperband, HyperModel, HyperParameters, BayesianOptimization, RandomSearch\n",
        "\n",
        "# Clear any existing TensorFlow session\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# To ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Set logging level to DEBUG for detailed output\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Configure Random Seed for Numpy and Tensorflow\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Define mappings for architectures and preprocessing functions\n",
        "ARCHITECTURES = {\n",
        "    'ResNet50V2': ResNet50V2,\n",
        "    'VGG16': VGG16,\n",
        "    'InceptionV3': InceptionV3,\n",
        "    'MobileNetV2': MobileNetV2,\n",
        "    'EfficientNetB0': EfficientNetB0\n",
        "}\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'efficientnet_preprocess': efficientnet_preprocess\n",
        "}\n",
        "\n",
        "# Define metrics mapping\n",
        "METRICS = {\n",
        "    'Precision': Precision(name='precision'),\n",
        "    'Recall': Recall(name='recall'),\n",
        "    'AUC': AUC(name='auc')\n",
        "}\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
        "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
        "            print()\n",
        "            print(\"--------------------\")\n",
        "            print()\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        for x, y in self.validation_data.take(self.validation_steps):\n",
        "            val_metrics = self.model.test_on_batch(x, y)\n",
        "            val_loss += val_metrics[0]\n",
        "            val_accuracy += val_metrics[1]\n",
        "\n",
        "        val_loss /= self.validation_steps\n",
        "        val_accuracy /= self.validation_steps\n",
        "\n",
        "        logs['val_loss'] = val_loss\n",
        "        logs['val_accuracy'] = val_accuracy\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        logging.debug(f\"Loss: {val_loss:.4f}\")\n",
        "        logging.debug(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, val_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEpoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset)}\")\n",
        "        logging.debug(f\"Epoch {epoch + 1} - Val samples: {tf.data.experimental.cardinality(self.val_dataset)}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            logging.debug(f\"\\nEpoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
        "            logging.debug(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nStarting epoch {epoch + 1}\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if batch % 100 == 0:\n",
        "            logging.debug(f\"\\nStarting batch {batch}\\n\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logging.debug(f\"\\nEnd of epoch {epoch + 1}\\n\")\n",
        "        if logs:\n",
        "            for key, value in logs.items():\n",
        "                logging.debug(f\"{key}: {value}\")\n",
        "        logging.debug(\"\\n--------------------\\n\")\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self, config):\n",
        "        self.train_path = config['data']['train_path']\n",
        "        self.test_path = config['data']['test_path']\n",
        "        self.batch_size = config['data']['batch_size']\n",
        "        self.target_size = tuple(config['data']['target_size'])\n",
        "        self.preprocessing_function = PREPROCESSING_FUNCTIONS[config['data']['preprocessing_function']]\n",
        "        self.augmentation_params = config['augmentation']\n",
        "        self.train_sample_count = sum([len(os.listdir(os.path.join(self.train_path, d))) for d in os.listdir(self.train_path) if os.path.isdir(os.path.join(self.train_path, d))])\n",
        "        self.test_sample_count = sum([len(os.listdir(os.path.join(self.test_path, d))) for d in os.listdir(self.test_path) if os.path.isdir(os.path.join(self.test_path, d))])\n",
        "\n",
        "        logging.debug(f\"Initialized DataGenerator with:\")\n",
        "        logging.debug(f\"Train path: {self.train_path}\")\n",
        "        logging.debug(f\"Test path: {self.test_path}\")\n",
        "        logging.debug(f\"Batch size: {self.batch_size}\")\n",
        "        logging.debug(f\"Target size: {self.target_size}\")\n",
        "        logging.debug(f\"Preprocessing function: {self.preprocessing_function}\")\n",
        "        logging.debug(f\"Augmentation params: {self.augmentation_params}\")\n",
        "\n",
        "        self.data_augmentation = self.create_data_augmentation()\n",
        "        self.rescale_layer = self.create_rescale_layer()\n",
        "\n",
        "    def create_data_augmentation(self):\n",
        "        layers = []\n",
        "        augmentation_params = self.augmentation_params\n",
        "\n",
        "        # Apply augmentations based on parameters\n",
        "        if augmentation_params.get('rotation_range'):\n",
        "            rotation_range = augmentation_params['rotation_range']\n",
        "            factor = rotation_range / 360.0  # Convert degrees to fraction of full circle\n",
        "            layers.append(RandomRotation(factor=(-factor, factor)))\n",
        "\n",
        "        if augmentation_params.get('horizontal_flip'):\n",
        "            layers.append(RandomFlip(mode='horizontal'))\n",
        "\n",
        "        if augmentation_params.get('vertical_flip'):\n",
        "            layers.append(RandomFlip(mode='vertical'))\n",
        "\n",
        "        if augmentation_params.get('zoom_range'):\n",
        "            zoom = augmentation_params['zoom_range']\n",
        "            layers.append(RandomZoom(height_factor=(-zoom, zoom), width_factor=(-zoom, zoom)))\n",
        "\n",
        "        if augmentation_params.get('width_shift_range') or augmentation_params.get('height_shift_range'):\n",
        "            width_shift = augmentation_params.get('width_shift_range', 0.0)\n",
        "            height_shift = augmentation_params.get('height_shift_range', 0.0)\n",
        "            layers.append(RandomTranslation(height_factor=height_shift, width_factor=width_shift))\n",
        "\n",
        "        if augmentation_params.get('brightness_range'):\n",
        "            brightness = augmentation_params['brightness_range']\n",
        "            layers.append(RandomBrightness(factor=brightness))\n",
        "\n",
        "        if augmentation_params.get('contrast_range'):\n",
        "            contrast = augmentation_params['contrast_range']\n",
        "            layers.append(RandomContrast(factor=contrast))\n",
        "            \n",
        "        if not layers:\n",
        "            layers.append(tf.keras.layers.Lambda(lambda x: x))\n",
        "\n",
        "        data_augmentation = tf.keras.Sequential(layers)\n",
        "        return data_augmentation\n",
        "\n",
        "    def create_rescale_layer(self):\n",
        "        # Define which preprocessing functions expect which input ranges\n",
        "        preprocess_0_255 = [resnet_preprocess, vgg_preprocess]\n",
        "        preprocess_0_1 = [efficientnet_preprocess]\n",
        "        preprocess_minus1_1 = [mobilenet_preprocess, inception_preprocess]\n",
        "\n",
        "        if self.preprocessing_function in preprocess_0_255:\n",
        "            # No rescaling needed; images are already in [0, 255]\n",
        "            return None\n",
        "        elif self.preprocessing_function in preprocess_0_1:\n",
        "            # Rescaling needed to bring images to [0, 1]\n",
        "            return Rescaling(1./255)\n",
        "        elif self.preprocessing_function in preprocess_minus1_1:\n",
        "            # Rescaling needed to bring images to [0, 1]; preprocessing function will scale to [-1, 1]\n",
        "            return Rescaling(1./255)\n",
        "        else:\n",
        "            # Default to rescaling to [0, 1]\n",
        "            return Rescaling(1./255)\n",
        "\n",
        "    def augment(self, images, labels):\n",
        "        # Different preprocessing functions expect different ranges of images after augmentation\n",
        "        # ResNet50V2 and VGG16: Expect images in the range [0, 255] with mean subtraction.\n",
        "        # InceptionV3 and MobileNetV2: Expect images scaled to [-1, 1].\n",
        "        # EfficientNetB0: Expects images scaled to [0, 1].\n",
        "        \n",
        "        images = tf.cast(images, tf.float32)\n",
        "\n",
        "        # Apply rescaling if necessary\n",
        "        if self.rescale_layer:\n",
        "            images = self.rescale_layer(images)\n",
        "\n",
        "        # Apply data augmentation\n",
        "        images = self.data_augmentation(images)\n",
        "\n",
        "        # Apply preprocessing function\n",
        "        images = self.preprocessing_function(images)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def normalize_and_preprocess(self, images, labels):\n",
        "        images = tf.cast(images, tf.float32)\n",
        "\n",
        "        # Apply rescaling if necessary\n",
        "        if self.rescale_layer:\n",
        "            images = self.rescale_layer(images)\n",
        "\n",
        "        # Apply preprocessing function\n",
        "        images = self.preprocessing_function(images)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def create_datasets(self):\n",
        "        train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.train_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        self.class_names = train_dataset.class_names  # Get class names from the dataset\n",
        "        \n",
        "        test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.test_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=self.batch_size,\n",
        "            image_size=self.target_size,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        # Consolidate train dataset transformations\n",
        "        train_dataset = (train_dataset\n",
        "                         .map(self.augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                         .shuffle(1000)\n",
        "                         .repeat()\n",
        "                         .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "        # Consolidate test dataset transformations\n",
        "        test_dataset = (test_dataset\n",
        "                        .map(self.normalize_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                        .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "        steps_per_epoch = math.ceil(self.train_sample_count / self.batch_size)\n",
        "        validation_steps = math.ceil(self.test_sample_count / self.batch_size)\n",
        "\n",
        "        logging.debug(f\"Training dataset: {train_dataset}\")\n",
        "        logging.debug(f\"Testing dataset: {test_dataset}\")\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps\n",
        "\n",
        "    def get_data_generators(self):\n",
        "        return self.create_datasets()\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            image_files = glob.glob(os.path.join(class_path, '*.*'))\n",
        "            class_counts[class_name] = len(image_files)\n",
        "    return class_counts\n",
        "\n",
        "# Define the HyperModel class\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, config, num_classes, best_hyperparameters=None):\n",
        "        self.config = config\n",
        "        self.num_classes = num_classes\n",
        "        self.best_hyperparameters = best_hyperparameters\n",
        "\n",
        "    def build(self, hp):\n",
        "        if self.best_hyperparameters:\n",
        "            hp = self.best_hyperparameters  # Use best hyperparameters if available\n",
        "        architecture = ARCHITECTURES[self.config['model']['architecture']]\n",
        "        input_shape = tuple(self.config['model']['input_shape'])\n",
        "\n",
        "        base_model = architecture(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=input_shape\n",
        "        )\n",
        "\n",
        "        # Optionally unfreeze layers based on config\n",
        "        for layer in base_model.layers[-10:]:\n",
        "            layer.trainable = True  # You can make this a hyperparameter if needed\n",
        "\n",
        "        inputs = Input(shape=input_shape)\n",
        "        x = base_model(inputs)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        # Tunable hyperparameters with defaults from config.yaml\n",
        "        dense_units = hp.Int('dense_units',\n",
        "                             min_value=self.config['hyperparameters']['dense_units']['min'],\n",
        "                             max_value=self.config['hyperparameters']['dense_units']['max'],\n",
        "                             step=self.config['hyperparameters']['dense_units']['step'],\n",
        "                             default=self.config['hyperparameters']['dense_units']['default'])\n",
        "\n",
        "        dropout_rate = hp.Float('dropout_rate',\n",
        "                                min_value=float(self.config['hyperparameters']['dropout_rate']['min']),\n",
        "                                max_value=float(self.config['hyperparameters']['dropout_rate']['max']),\n",
        "                                step=0.1,\n",
        "                                default=float(self.config['hyperparameters']['dropout_rate']['default']))\n",
        "\n",
        "        x = Dense(dense_units)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "        x = Dense(dense_units // 2)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "        output = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "        # Compile the model\n",
        "        learning_rate = hp.Float('learning_rate',\n",
        "                                 min_value=float(self.config['hyperparameters']['learning_rate']['min']),\n",
        "                                 max_value=float(self.config['hyperparameters']['learning_rate']['max']),\n",
        "                                 sampling='log',\n",
        "                                 default=float(self.config['hyperparameters']['learning_rate']['default']))\n",
        "\n",
        "        optimizer_choice = hp.Choice('optimizer',\n",
        "                                     values=self.config['hyperparameters']['optimizer']['choices'],\n",
        "                                     default=self.config['hyperparameters']['optimizer']['default'])\n",
        "\n",
        "        if optimizer_choice == 'adam':\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        elif optimizer_choice == 'sgd':\n",
        "            optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "        metrics = ['accuracy'] + [METRICS[metric] for metric in self.config['model']['additional_metrics']]\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=metrics)\n",
        "        return model\n",
        "\n",
        "def setup_gpu(gpu_config):\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Log the number of GPUs available\n",
        "            logging.info(f\"GPU setup complete. Found {len(gpus)} GPU(s).\")\n",
        "\n",
        "            # Optionally, you can log more details about each GPU\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logging.info(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            logging.error(f\"GPU setup failed: {e}\")\n",
        "    else:\n",
        "        logging.warning(\"No GPUs found. The model will run on CPU.\")\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "def setup_datasets(config):\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logging.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps = data_generator.get_data_generators()\n",
        "        class_names = data_generator.class_names  # Use class names from data generator\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Dataset setup failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_callbacks(config, train_dataset, test_dataset, validation_steps, for_tuning=False):\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=config['training']['patience'], restore_best_weights=True),\n",
        "        ModelCheckpoint(filepath=config['training']['model_checkpoint_path'], save_best_only=True)\n",
        "    ]\n",
        "    if not for_tuning:\n",
        "        # Include custom callbacks only when not tuning\n",
        "        callbacks.extend([\n",
        "            AccuracyCallback(target_accuracy=config['training']['target_accuracy']),\n",
        "            CustomValidationCallback(test_dataset, validation_steps),\n",
        "            DebugCallback(),\n",
        "            DatasetLogger(train_dataset, test_dataset)\n",
        "        ])\n",
        "    return callbacks\n",
        "\n",
        "def compute_class_weights_from_counts(class_counts, class_names):\n",
        "    total_samples = sum(class_counts.values())\n",
        "    class_weight_dict = {}\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        count = class_counts.get(class_name, 0)\n",
        "        if count > 0:\n",
        "            class_weight_dict[idx] = total_samples / (len(class_counts) * count)\n",
        "        else:\n",
        "            class_weight_dict[idx] = 0.0  # Handle classes with zero samples\n",
        "    return class_weight_dict\n",
        "\n",
        "def save_best_hyperparameters(best_hps, filepath='best_hyperparameters.json'):\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(best_hps.values, f)\n",
        "\n",
        "def load_best_hyperparameters(filepath='best_hyperparameters.json'):\n",
        "    with open(filepath, 'r') as f:\n",
        "        hps_dict = json.load(f)\n",
        "    hp = HyperParameters()\n",
        "    for key, value in hps_dict.items():\n",
        "        hp.Fixed(key, value)\n",
        "    return hp\n",
        "\n",
        "def main(config_path):\n",
        "    config = load_config(config_path)\n",
        "    logging.debug(f\"Loaded configuration: {config}\")\n",
        "    performance_tuning = config.get('tuning', {}).get('perform_tuning', True)\n",
        "    logging.debug(f\"Perform tuning: {performance_tuning}\")\n",
        "    setup_gpu(config.get('gpu', {}))\n",
        "    logging.debug(\"Completed GPU setup.\")\n",
        "\n",
        "    # Mount Google Drive if using Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "        logging.info(\"Running in Colab environment\")\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "        logging.info(\"Running in local environment\")\n",
        "\n",
        "    # Update the train and test paths\n",
        "    train_path = os.path.join(DATASET_PATH, config['data']['train_dir'])\n",
        "    test_path = os.path.join(DATASET_PATH, config['data']['test_dir'])\n",
        "\n",
        "    # Log the dataset paths\n",
        "    logging.debug(f\"Full training dataset path: {train_path}\")\n",
        "    logging.debug(f\"Full testing dataset path: {test_path}\")\n",
        "\n",
        "    # Update the config with the full paths\n",
        "    config['data']['train_path'] = train_path\n",
        "    config['data']['test_path'] = test_path\n",
        "\n",
        "    logging.info(f\"Train path: {train_path}\")\n",
        "    logging.info(f\"Test path: {test_path}\")\n",
        "\n",
        "    try:\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names = setup_datasets(config)\n",
        "        logging.debug(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "        logging.debug(f\"Validation steps: {validation_steps}\")\n",
        "        logging.debug(f\"Class names: {class_names}\")\n",
        "\n",
        "        num_classes = len(class_names)\n",
        "        \n",
        "        # Compute class weights\n",
        "        class_counts = count_samples(config['data']['train_path'])\n",
        "        class_weight_dict = compute_class_weights_from_counts(class_counts, class_names)\n",
        "        logging.debug(f\"Computed class weights: {class_weight_dict}\")        \n",
        "        \n",
        "        if performance_tuning:\n",
        "            # Instantiate the hypermodel\n",
        "            hypermodel = MyHyperModel(config, num_classes)\n",
        "            logging.debug(\"HyperModel instantiated successfully.\")\n",
        "\n",
        "            # Get callbacks for tuning (only deepcopyable ones)\n",
        "            tuning_callbacks = get_callbacks(config, train_dataset, test_dataset, validation_steps, for_tuning=True)\n",
        "\n",
        "            # Set up the tuner\n",
        "            tuner = RandomSearch(\n",
        "                hypermodel,\n",
        "                objective='val_accuracy',\n",
        "                max_trials=config['tuner']['max_trials'],\n",
        "                executions_per_trial=config['tuner']['executions_per_trial'],\n",
        "                directory='hyperparameter_tuning',\n",
        "                project_name='keras_tuner_project'\n",
        "            )\n",
        "            logging.info(\"Keras Tuner initialized successfully.\")\n",
        "\n",
        "            # Run the hyperparameter search\n",
        "            tuner.search(\n",
        "                train_dataset,\n",
        "                epochs=config['training']['epochs'],\n",
        "                validation_data=test_dataset,\n",
        "                callbacks=tuning_callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                # validation_steps=validation_steps # By omitting validation_steps, Keras will automatically determine how many batches to use for validation based on the dataset's size.\n",
        "            )\n",
        "\n",
        "            # Get the optimal hyperparameters\n",
        "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "            logging.info(f\"\"\"\n",
        "            The hyperparameter search is complete.\n",
        "            Best learning rate: {best_hps.get('learning_rate')}\n",
        "            Best dense units: {best_hps.get('dense_units')}\n",
        "            Best dropout rate: {best_hps.get('dropout_rate')}\n",
        "            Best optimizer: {best_hps.get('optimizer')}\n",
        "            \"\"\")\n",
        "\n",
        "            save_best_hyperparameters(best_hps)\n",
        "            logging.info(\"Best hyperparameters saved to file.\")\n",
        "\n",
        "            # Build the best model\n",
        "            model = tuner.hypermodel.build(best_hps)\n",
        "            logging.debug(\"Best model built with optimal hyperparameters.\")\n",
        "        else:\n",
        "            # Load the best hyperparameters from file\n",
        "            best_hps = load_best_hyperparameters()\n",
        "            logging.info(\"Loaded best hyperparameters from file.\")\n",
        "\n",
        "            # Instantiate the hypermodel with best hyperparameters\n",
        "            hypermodel = MyHyperModel(config, num_classes, best_hyperparameters=best_hps)\n",
        "            model = hypermodel.build(hp=None)  # hp is None, so it uses best_hyperparameters\n",
        "            logging.debug(\"Model built with loaded hyperparameters.\")\n",
        "\n",
        "        # Get all callbacks including custom ones for final training\n",
        "        final_callbacks = get_callbacks(config, train_dataset, test_dataset, validation_steps, for_tuning=False)\n",
        "        \n",
        "        # Retrain the model with the best hyperparameters\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=config['training']['epochs'],\n",
        "            validation_data=test_dataset,\n",
        "            callbacks=final_callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            # validation_steps=validation_steps # By omitting validation_steps, Keras will automatically determine how many batches to use for validation based on the dataset's size.\n",
        "        )\n",
        "\n",
        "        logging.debug(\"Model retrained with optimal hyperparameters.\")\n",
        "        logging.info(\"Final evaluation:\")\n",
        "        final_evaluation = model.evaluate(test_dataset)\n",
        "        logging.info(f\"Final evaluation metrics: {final_evaluation}\")\n",
        "\n",
        "        return history, model, class_names, config['data']['target_size'], config['data']['preprocessing_function'], config['model']['architecture']\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred: {e}\")\n",
        "        logging.debug(traceback.format_exc())\n",
        "\n",
        "        return None, None, None, None, None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:No GPUs found. The model will run on CPU.\n",
            "INFO:root:Running in local environment\n",
            "INFO:root:Train path: /Users/toddwalters/Development/data/1703138137_dataset/part_1/dataset_hist_structures/structures_dataset\n",
            "INFO:root:Test path: /Users/toddwalters/Development/data/1703138137_dataset/part_1/dataset_hist_structures/dataset_test\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10235 files belonging to 10 classes.\n",
            "Found 1474 files belonging to 10 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:root:Loaded best hyperparameters from file.\n",
            "2024-09-28 11:56:09.086179: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] fused(ShuffleDatasetV3:58,RepeatDataset:59): Filling up shuffle buffer (this may take a while): 231 of 1000\n",
            "2024-09-28 11:56:19.113137: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] fused(ShuffleDatasetV3:58,RepeatDataset:59): Filling up shuffle buffer (this may take a while): 468 of 1000\n",
            "2024-09-28 11:56:39.104014: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] fused(ShuffleDatasetV3:58,RepeatDataset:59): Filling up shuffle buffer (this may take a while): 943 of 1000\n",
            "2024-09-28 11:56:39.890148: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m247/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 1s/step - accuracy: 0.1713 - auc: 0.5876 - loss: 2.3657 - precision: 0.4234 - recall: 0.0296"
          ]
        }
      ],
      "source": [
        "history, compiled_model, class_names, target_size, preprocessing_function, ptm_name = main('config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZrEUMa6XxTL"
      },
      "source": [
        "**Explanations:**\n",
        "* This codebase implements a Convolutional Neural Network (CNN) using transfer learning with the ResNet50V2 ptm_name for image classification. It uses a dataset of architectural elements (e.g., altars, domes, columns) split into training and test sets. The model is compiled with Adam optimizer and categorical crossentropy loss. Data augmentation is applied to increase the diversity of training samples. The training process includes callbacks for early stopping, learning rate scheduling, and custom validation. The model's performance is evaluated and visualized over 20 epochs.\n",
        "\n",
        "1. `DataGenerator` class:\n",
        "  - Optimizable parameters:\n",
        "    - None directly, but it relies on the ImageDataGenerator settings.\n",
        "\n",
        "2. `CustomAugmentationLayer` class:\n",
        "   - Optimizable parameters:\n",
        "    - Contrast range (lower, upper)\n",
        "    - Saturation range (lower, upper)\n",
        "   - Role in optimization:\n",
        "    - Adjusting these can increase data diversity and potentially reduce overfitting.\n",
        "\n",
        "3. `create_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Number and size of dense layers\n",
        "    - Dropout rate (currently 0.5)\n",
        "    - L2 regularization strength\n",
        "   - Role in optimization:\n",
        "    - Adjusting model complexity can help balance underfitting and overfitting.\n",
        "\n",
        "4. `compile_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Initial learning rate (currently 0.001)\n",
        "    - Learning rate decay steps and rate\n",
        "    - Choice of optimizer (currently Adam)\n",
        "   - Role in optimization:\n",
        "    - Proper learning rate and optimizer settings are crucial for efficient training and convergence.\n",
        "\n",
        "5. `AccuracyCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - Target accuracy threshold\n",
        "   - Role in optimization:\n",
        "    - Adjusting this can prevent premature stopping or unnecessarily long training.\n",
        "\n",
        "6. `count_samples` function:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "7. `DebugCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - None, but frequency of debug prints could be adjusted.\n",
        "\n",
        "8. `create_data_generators` function:\n",
        "   - Optimizable parameters:\n",
        "    - Batch size (currently 32)\n",
        "    - Data augmentation parameters (rotation_range, width_shift_range, etc.)\n",
        "   - Role in optimization:\n",
        "    - Proper batch size and augmentation can improve training stability and reduce overfitting.\n",
        "\n",
        "9. `DatasetLogger` class:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "10. `CustomValidationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "      - None, but custom metrics could be added.\n",
        "\n",
        "11. `visualize_augmentation` function:\n",
        "    - Optimizable parameters:\n",
        "      - None\n",
        "\n",
        "12. `AugmentationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "     - Frequency of augmentation visualization\n",
        "    - Role in optimization:\n",
        "      - Adjusting this helps in monitoring augmentation effects without slowing training.\n",
        "\n",
        "13. Main execution block:\n",
        "    - Optimizable parameters:\n",
        "      - Number of epochs (currently 20)\n",
        "      - Early stopping patience (currently 10)\n",
        "      - Class weight calculation method\n",
        "    - Role in optimization:\n",
        "      - These parameters affect overall training duration and handling of class imbalance.\n",
        "\n",
        "Additional global optimizations:\n",
        "1. Learning rate scheduling: Implement more sophisticated schedules like cyclic learning rates or warm restarts.\n",
        "2. Model ptm_name: Experiment with different base models (e.g., EfficientNet, VGG) or custom ptm_names.\n",
        "3. Regularization: Add or adjust L2 regularization, increase dropout rates, or implement other techniques like label smoothing.\n",
        "4. Data preprocessing: Normalize input data, apply additional augmentation techniques like mixup or cutout.\n",
        "5. Ensemble methods: Train multiple models and combine their predictions for improved reliability.\n",
        "6. Cross-validation: Implement k-fold cross-validation for more robust performance estimation.\n",
        "7. Gradient clipping: Add gradient clipping to prevent exploding gradients and stabilize training.\n",
        "8. Batch normalization: Add batch normalization layers for improved training stability and potentially faster convergence.\n",
        "9. Learning rate finder: Implement a learning rate finder to determine optimal initial learning rates.\n",
        "10. Progressive resizing: Start training with smaller image sizes and gradually increase, potentially speeding up early training stages.\n",
        "\n",
        "By carefully tuning these parameters and implementing these techniques, you can work towards optimizing the model's performance, improving its reliability, and finding the right balance between underfitting and overfitting for your specific architectural element classification task.\n",
        "\n",
        "**Why It Is Important:**\n",
        "* Using a CNN with transfer learning is important because it leverages pre-trained weights on a large dataset (ImageNet), allowing the model to learn high-level features more quickly and effectively, especially when dealing with a relatively small dataset. This approach often leads to better performance and faster convergence compared to training a model from scratch, making it particularly useful for specialized image classification tasks like identifying architectural elements.\n",
        "\n",
        "**Observations:**\n",
        "* The model quickly achieves high accuracy (>80%) within the first few epochs.\n",
        "* Validation accuracy peaks around epoch 6 at about 86% and then fluctuates.\n",
        "* There's some overfitting, as training accuracy consistently exceeds validation accuracy.\n",
        "* The learning rate decreases steadily over the epochs, as designed by the exponential decay schedule.\n",
        "* Both training and validation loss decrease rapidly initially and then plateau.\n",
        "* The model's performance seems to stabilize after about 10-15 epochs.\n",
        "\n",
        "**Conclusions:**\n",
        "* The transfer learning approach is effective, allowing the model to achieve good performance quickly.\n",
        "* The model reaches a reasonable accuracy (~83% on the validation set) for classifying architectural elements.\n",
        "* There's room for improvement, as the model shows signs of overfitting and doesn't consistently improve after the initial rapid progress.\n",
        "* The current learning rate schedule and data augmentation help, but may not be optimal for this specific task.\n",
        "\n",
        "**Recommendations:**\n",
        "* Experiment with stronger regularization techniques (e.g., increased dropout, L2 regularization) to combat overfitting.\n",
        "* Try different learning rate schedules, such as cyclical learning rates or a more aggressive decay, to potentially improve convergence.\n",
        "* Increase data augmentation to provide more diverse training samples and potentially reduce overfitting.\n",
        "* Consider implementing techniques like gradient clipping to stabilize training.\n",
        "* Explore ensemble methods or cross-validation to improve overall performance and reliability.\n",
        "* Analyze misclassifications to identify patterns and potentially refine the model ptm_name or data preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXoAMHyrXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5dzjyc3XxTL"
      },
      "outputs": [],
      "source": [
        "# def plot_training_history(history, learning_rates, ptm_name):\n",
        "def plot_training_history(history, ptm_name):\n",
        "\n",
        "    # Plot training history and learning rate\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot training and validation accuracy\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title(f'Model Accuracy using the {ptm_name} Pre-Trained Model')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'Model Loss using the {ptm_name} Pre-Trained Model')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot learning rate\n",
        "    # plt.subplot(1, 3, 3)\n",
        "    # plt.plot(range(1, len(learning_rates) + 1), learning_rates, label='Learning Rate')\n",
        "    # plt.title('Learning Rate over Epochs')\n",
        "    # plt.xlabel('Epoch')\n",
        "    # plt.ylabel('Learning Rate')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# plot_training_history(history, learning_rates, ptm_name)\n",
        "plot_training_history(history, ptm_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2FxsHI4XxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_training_history` that visualizes the training and validation accuracy and loss over epochs. It then applies this function to the training history of the augmented model.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing the training and validation metrics helps us understand how the model is learning over time. It allows us to identify potential issues such as overfitting (when training accuracy continues to improve but validation accuracy plateaus or decreases) or underfitting (when both training and validation accuracy are low and not improving). This information is crucial for deciding whether to adjust the model ptm_name, change hyperparameters, or modify the training process.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsb4RIZGXxTL"
      },
      "source": [
        "#### <a id='toc1_4_2_5_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWZIK3E1XxTL"
      },
      "outputs": [],
      "source": [
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "\n",
        "# Create a dictionary mapping string names to preprocessing functions\n",
        "preprocessing_functions = {\n",
        "    'efficientnet_preprocess': efficientnet_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess\n",
        "}\n",
        "\n",
        "def predict_image_class(model, img_path, class_names, target_size, preprocessing_function_name):\n",
        "    target_size = target_size  # Get the target size for the chosen architecture\n",
        "    # Retrieve the actual preprocessing function from the dictionary\n",
        "    preprocessing_function = preprocessing_functions.get(preprocessing_function_name)\n",
        "    \n",
        "    if preprocessing_function is None:\n",
        "        raise ValueError(f\"Unknown preprocessing function: {preprocessing_function_name}\")\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocessing_function(img_array)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_index]\n",
        "    confidence = predictions[0][predicted_class_index]\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "def visualize_prediction(img_path, target_size, predicted_class, confidence):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
        "    plt.show()\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
        "\n",
        "print()\n",
        "print(\"\\nTesting model on new images:\")\n",
        "print()\n",
        "\n",
        "# Get the list of all files in the model_test_images directory\n",
        "test_image_path = f'{DATASET_PATH}/model_test_images'\n",
        "image_files = os.listdir(f'{test_image_path}/')\n",
        "\n",
        "# Optionally, filter the list to include only image files\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "image_files = [file for file in image_files if file.lower().endswith(image_extensions)]\n",
        "\n",
        "# Print the list of image files\n",
        "print(image_files)\n",
        "\n",
        "print(f'The preprocessing function is: {preprocessing_function}')\n",
        "print(f'The compiled model is: {compiled_model}')\n",
        "print(f'The class names are: {class_names}')\n",
        "\n",
        "# for img_path in test_image_paths:\n",
        "for image_file in image_files:\n",
        "    img_path = f'{test_image_path}/{image_file}'\n",
        "    predicted_class, confidence = predict_image_class(compiled_model, img_path, class_names, target_size, preprocessing_function)\n",
        "    print()\n",
        "    print(f\"Image: {img_path}\")\n",
        "    print(f\"Predicted class: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print()\n",
        "\n",
        "    # Visualize prediction\n",
        "    visualize_prediction(img_path, target_size, predicted_class, confidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlyPwmHMXxTL"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFfqGCDiXxTP"
      },
      "source": [
        "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation - v2**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uN8jUQZ81RW"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cPvdrV7dZ_3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSeZmV4YfVVE"
      },
      "source": [
        "#### <a id='toc1_4_2_4_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj8Z1KnIXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0LDLxwLXxTP"
      },
      "source": [
        "### <a id='toc1_4_1_'></a>[**Part 2 - Data Science**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVEV7tJUXxTP"
      },
      "source": [
        "### <a id='toc1_4_3_'></a>[**Data Analysis**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-haQ_GrgXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "DATASET_PATH = '/Users/toddwalters/Development/data/1703138137_dataset/part_2'\n",
        "\n",
        "# Load the datasets\n",
        "user_df = pd.read_csv(f'{DATASET_PATH}/user.csv')\n",
        "tourism_df = pd.read_excel(f'{DATASET_PATH}/tourism_with_id.xlsx')\n",
        "ratings_df = pd.read_csv(f'{DATASET_PATH}/tourism_rating.csv')\n",
        "\n",
        "# Task 1: Preliminary inspections\n",
        "print(\"1. Preliminary Inspections\")\n",
        "\n",
        "## Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(\"User data:\\n\", user_df.isnull().sum())\n",
        "print(\"\\nTourism data:\\n\", tourism_df.isnull().sum())\n",
        "print(\"\\nRatings data:\\n\", ratings_df.isnull().sum())\n",
        "\n",
        "## Check for duplicates\n",
        "print(\"\\nDuplicates:\")\n",
        "print(\"User data:\", user_df.duplicated().sum())\n",
        "print(\"Tourism data:\", tourism_df.duplicated().sum())\n",
        "print(\"Ratings data:\", ratings_df.duplicated().sum())\n",
        "\n",
        "# Task 2: Explore user group providing tourism ratings\n",
        "print(\"\\n2. User Group Analysis\")\n",
        "\n",
        "## Analyze age distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(user_df['Age'], bins=20, kde=True)\n",
        "plt.title('Age Distribution of Users')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Identify places where most users are coming from\n",
        "top_locations = user_df['Location'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_locations.plot(kind='bar')\n",
        "plt.title('Top 10 User Locations')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 3: Explore locations and categories of tourist spots\n",
        "print(\"\\n3. Tourist Spot Analysis\")\n",
        "\n",
        "## Different categories of tourist spots\n",
        "print(\"Categories of Tourist Spots:\")\n",
        "print(tourism_df['Category'].value_counts())\n",
        "\n",
        "## Analyze tourism types by location\n",
        "location_category = tourism_df.groupby('City')['Category'].value_counts().unstack()\n",
        "location_category_norm = location_category.div(location_category.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "location_category_norm.plot(kind='bar', stacked=True)\n",
        "plt.title('Tourism Categories by City')\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Proportion of Categories')\n",
        "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Identify best city for nature enthusiasts\n",
        "nature_spots = tourism_df[tourism_df['Category'] == 'Nature']\n",
        "nature_by_city = nature_spots.groupby('City').size().sort_values(ascending=False)\n",
        "print(\"Best Cities for Nature Enthusiasts:\")\n",
        "print(nature_by_city)\n",
        "\n",
        "# Task 4: Analyze combined data with places and user ratings\n",
        "print(\"\\n4. Combined Data Analysis\")\n",
        "\n",
        "## Merge tourism and ratings data\n",
        "combined_data = pd.merge(tourism_df, ratings_df, on='Place_Id')\n",
        "\n",
        "## Find spots most loved by tourists\n",
        "top_spots = combined_data.groupby('Place_Name')['Place_Ratings'].mean().sort_values(ascending=False).head(10)\n",
        "print(\"Top 10 Most Loved Tourist Spots:\")\n",
        "print(top_spots)\n",
        "\n",
        "## Find city with most loved tourist spots\n",
        "city_ratings = combined_data.groupby('City')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nCities with Most Loved Tourist Spots:\")\n",
        "print(city_ratings)\n",
        "\n",
        "## Analyze which categories users like most\n",
        "category_ratings = combined_data.groupby('Category')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nMost Liked Categories:\")\n",
        "print(category_ratings)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 5: Build a recommender model\n",
        "print(\"\\n5. Recommender Model\")\n",
        "\n",
        "## Create a pivot table for user-item ratings\n",
        "user_item_matrix = combined_data.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "## Fill NaN values with 0\n",
        "user_item_matrix = user_item_matrix.fillna(0)\n",
        "\n",
        "## Calculate cosine similarity between items\n",
        "item_similarity = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "## Create a DataFrame from the item similarity matrix\n",
        "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
        "\n",
        "def get_similar_places(place_name, user_rating):\n",
        "    similar_places = item_similarity_df[place_name] * (user_rating - 2.5)\n",
        "    similar_places = similar_places.sort_values(ascending=False)\n",
        "    return similar_places\n",
        "\n",
        "## Example recommendation\n",
        "place_name = \"Monumen Nasional\"  # You can change this to any place name\n",
        "user_rating = 5  # You can change this to any rating\n",
        "\n",
        "print(\"Recommendations based on\", place_name, \"with a rating of\", user_rating)\n",
        "print(get_similar_places(place_name, user_rating).head(5))\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\nAdditional Insights:\")\n",
        "\n",
        "## User activity analysis\n",
        "user_activity = ratings_df.groupby('User_Id')['Place_Ratings'].count().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 Most Active Users:\")\n",
        "print(user_activity.head(10))\n",
        "\n",
        "## Rating distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(ratings_df['Place_Ratings'], bins=5, kde=True)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Correlation between place features and ratings\n",
        "if 'Time_Minutes' in tourism_df.columns and 'Price' in tourism_df.columns:\n",
        "    feature_ratings = pd.merge(tourism_df[['Place_Id', 'Time_Minutes', 'Price']],\n",
        "                               ratings_df[['Place_Id', 'Place_Ratings']],\n",
        "                               on='Place_Id')\n",
        "    correlation_matrix = feature_ratings[['Time_Minutes', 'Price', 'Place_Ratings']].corr()\n",
        "    print(\"\\nCorrelation between features and ratings:\")\n",
        "    print(correlation_matrix['Place_Ratings'])\n",
        "\n",
        "print(\"\\nAnalysis Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYVotBmAXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-kiMSDOXxTP"
      },
      "source": [
        "#### <a id='toc1_4_3_1_'></a>[**4.3.1. Data Analysis v2**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJ1lXnjXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "# Tourism Data Analysis and Recommendation Engine Project\n",
        "\n",
        "## Project Context\n",
        "This project aims to analyze tourism data in Indonesia's five largest cities, comprehend and predict the demand for various tourist attractions, and build a recommendation system based on user ratings.\n",
        "\n",
        "## Project Objectives\n",
        "1. Analyze user demographics and locations\n",
        "2. Explore tourist spot categories and locations\n",
        "3. Analyze user ratings and popular spots\n",
        "4. Build a recommendation system\n",
        "5. Forecast ratings using machine learning and deep learning algorithms\n",
        "\n",
        "## Dataset Description\n",
        "- user.csv: Contains user demographic data\n",
        "- tourism_with_id.csv: Provides details on tourist attractions\n",
        "- tourism_rating.csv: Contains user ratings for tourist spots\n",
        "\"\"\"\n",
        "\n",
        "# 4.1 Preliminary analysis\n",
        "# 4.1.1 Import the datasets into the Python environment\n",
        "user_df = pd.read_csv('user.csv')\n",
        "tourism_df = pd.read_csv('tourism_with_id.csv')\n",
        "ratings_df = pd.read_csv('tourism_rating.csv')\n",
        "\n",
        "# 4.1.2 Examine the dataset's shape and structure, and look out for any outlier\n",
        "print(\"User dataset shape:\", user_df.shape)\n",
        "print(\"Tourism dataset shape:\", tourism_df.shape)\n",
        "print(\"Ratings dataset shape:\", ratings_df.shape)\n",
        "\n",
        "user_df.info()\n",
        "tourism_df.info()\n",
        "ratings_df.info()\n",
        "\n",
        "# Check for missing values and duplicates\n",
        "for df, name in zip([user_df, tourism_df, ratings_df], ['User', 'Tourism', 'Ratings']):\n",
        "    print(f\"\\nMissing values in {name} dataset:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(f\"Duplicates in {name} dataset: {df.duplicated().sum()}\")\n",
        "\n",
        "# 4.1.3 Merge the datasets\n",
        "merged_df = pd.merge(ratings_df, tourism_df, on='Place_Id')\n",
        "merged_df = pd.merge(merged_df, user_df, on='User_Id')\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.2 Exploratory Data Analysis\n",
        "\n",
        "### 4.2.1 Examine the overall date wise ratings\n",
        "\"\"\"\n",
        "\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "daily_ratings = merged_df.groupby('date')['Place_Ratings'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(daily_ratings['date'], daily_ratings['Place_Ratings'])\n",
        "plt.title('Average Daily Ratings Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.2 Rating fluctuations across different days of the week\n",
        "\"\"\"\n",
        "\n",
        "merged_df['day_of_week'] = merged_df['date'].dt.day_name()\n",
        "day_of_week_ratings = merged_df.groupby('day_of_week')['Place_Ratings'].mean().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "day_of_week_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Day of Week')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.3 Rating trends for different months of the year\n",
        "\"\"\"\n",
        "\n",
        "merged_df['month'] = merged_df['date'].dt.month_name()\n",
        "monthly_ratings = merged_df.groupby('month')['Place_Ratings'].mean()\n",
        "monthly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Month')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.4 Rating distribution across different quarters\n",
        "\"\"\"\n",
        "\n",
        "merged_df['quarter'] = merged_df['date'].dt.quarter\n",
        "quarterly_ratings = merged_df.groupby('quarter')['Place_Ratings'].mean()\n",
        "quarterly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Quarter')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.5 Compare performances of different tourist spots\n",
        "\"\"\"\n",
        "\n",
        "spot_performance = merged_df.groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
        "print(spot_performance.head(10))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.6 Identify most popular tourist spots and their locations\n",
        "\"\"\"\n",
        "\n",
        "popular_spots = merged_df.groupby(['Place_Name', 'City'])['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "print(\"Most popular tourist spots overall:\")\n",
        "print(popular_spots.head(10))\n",
        "\n",
        "# For each city\n",
        "for city in merged_df['City'].unique():\n",
        "    city_spots = merged_df[merged_df['City'] == city].groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "    print(f\"\\nMost popular spot in {city}:\")\n",
        "    print(city_spots.head(1))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.7 Determine if the spot with highest visit count has the highest average rating\n",
        "\"\"\"\n",
        "\n",
        "spot_metrics = merged_df.groupby('Place_Name').agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "top_by_visits = spot_metrics.sort_values('visit_count', ascending=False).head(1)\n",
        "top_by_rating = spot_metrics.sort_values('Place_Ratings', ascending=False).head(1)\n",
        "\n",
        "print(\"Top spot by visit count:\")\n",
        "print(top_by_visits)\n",
        "print(\"\\nTop spot by average rating:\")\n",
        "print(top_by_rating)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.8 Identify the highest rated spot in each category and its visit count\n",
        "\"\"\"\n",
        "\n",
        "category_top_spots = merged_df.groupby(['Category', 'Place_Name']).agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "for category in category_top_spots.index.get_level_values('Category').unique():\n",
        "    top_spot = category_top_spots.loc[category].sort_values('Place_Ratings', ascending=False).head(1)\n",
        "    print(f\"\\nHighest rated spot in {category}:\")\n",
        "    print(top_spot)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.3 Building a Recommendation System\n",
        "\n",
        "We'll use a simple collaborative filtering approach based on user-item interactions.\n",
        "\"\"\"\n",
        "\n",
        "# Create a user-item matrix\n",
        "user_item_matrix = merged_df.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "# Calculate cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix.fillna(0))\n",
        "\n",
        "# Function to get recommendations for a user\n",
        "def get_recommendations(user_id, n=5):\n",
        "    user_index = user_item_matrix.index.get_loc(user_id)\n",
        "    similar_users = user_similarity[user_index].argsort()[::-1][1:11]  # top 10 similar users\n",
        "\n",
        "    similar_users_ratings = user_item_matrix.iloc[similar_users]\n",
        "    user_ratings = user_item_matrix.loc[user_id]\n",
        "\n",
        "    recommendations = (similar_users_ratings.mean() - user_ratings).sort_values(ascending=False)\n",
        "    return recommendations.head(n)\n",
        "\n",
        "# Example recommendation\n",
        "example_user = user_item_matrix.index[0]\n",
        "print(f\"Recommendations for user {example_user}:\")\n",
        "print(get_recommendations(example_user))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.4 Forecasting using Deep Learning Algorithms\n",
        "\n",
        "We'll use an LSTM model to forecast future ratings for a specific tourist spot.\n",
        "\"\"\"\n",
        "\n",
        "# Choose a popular tourist spot for forecasting\n",
        "popular_spot = popular_spots.index[0][0]\n",
        "spot_ratings = merged_df[merged_df['Place_Name'] == popular_spot].set_index('date')['Place_Ratings']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_ratings = scaler.fit_transform(spot_ratings.values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length), 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X, y = create_sequences(scaled_ratings, seq_length)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build and train LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(spot_ratings.index[-len(y_test):], y_test, label='Actual')\n",
        "plt.plot(spot_ratings.index[-len(y_pred):], y_pred, label='Predicted')\n",
        "plt.title(f'Rating Forecast for {popular_spot}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Rating')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## Conclusions and Recommendations\n",
        "\n",
        "[Add your conclusions and recommendations based on the analysis]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPn8L_FCXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUgFWSFaXxTP"
      },
      "source": [
        "##### <a id='toc1_4_3_1_1_'></a>[**4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh7GUi1iXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGTR0nX0XxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfp7HDhGXxTP"
      },
      "source": [
        "##### <a id='toc1_4_3_1_2_'></a>[**4.3.1.2. Use the data from the last six months as the testing data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwFM7b7PXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLE_lgmnXxTP"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYuhiqJiXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_3_1_3_'></a>[**4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDxCuQUOXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UBJhwxNXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8iG83ylXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_3_1_4_'></a>[**4.3.1.4. Use the best-performing models to make a forecast for the next year**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFFMhnlGXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGM473tXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F27ebfAeXxTQ"
      },
      "source": [
        "### <a id='toc1_4_4_'></a>[**4.4. Forecasting using deep learning algorithms**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyYdRh5KXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_1_'></a>[**4.4.1. Use sales amount for predictions instead of item count**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNWJmttFXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OktA8CEIXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC0bbivZXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_2_'></a>[**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szr8uZOEXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNjnXH0ZXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsD-rQ5WXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_1_'></a>[**4.4.2.1. Define the train and test series**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1mnT3mKXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrnsDqbdXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCfA6lZjXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_2_'></a>[**4.4.2.2. Generate synthetic data for the last 12 months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGevcLfXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFFdG2BKXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh96jqiUXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_3_'></a>[**4.4.2.3. Build and train an LSTM model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0jK6AQvXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs_0Kbv3XxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPlmLTmeXxTQ"
      },
      "source": [
        "##### <a id='toc1_4_4_2_4_'></a>[**4.4.2.4. Use the model to make predictions for the test data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMLYgzkWXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9qMv2KbXxTQ"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_j7MZqIXxTQ"
      },
      "source": [
        "#### <a id='toc1_4_4_3_'></a>[**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Za61_FYXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHrfqEDlXxTR"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZr4879oXxTR"
      },
      "source": [
        "#### <a id='toc1_4_4_4_'></a>[**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEXfhjpUXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN9oncYmXxTR"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "capstone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
