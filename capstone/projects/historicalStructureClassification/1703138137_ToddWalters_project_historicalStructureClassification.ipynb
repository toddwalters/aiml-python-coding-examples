{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/capstone/projects/historicalStructureClassification/1703138137_ToddWalters_project_historicalStructureClassification_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uE6q3GPrXxTI"
   },
   "source": [
    "Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM2B501NXxTI"
   },
   "source": [
    "-----\n",
    "\n",
    "# <a id='toc1_'></a>[**Historical Structure Classification Project**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caA0NN0-XxTI"
   },
   "source": [
    "-----------------------------\n",
    "## <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "This project is part of an AIML Capstone and focuses on two main aspects:\n",
    "1. Classifying historical structures using deep learning techniques\n",
    "2. Developing a recommendation engine for tourism\n",
    "\n",
    "The project aims to assist the travel and tourism industries by monitoring the condition of historical structures and providing personalized recommendations to tourists.\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "1. Develop a CNN model to classify historical structures from images\n",
    "2. Perform exploratory data analysis on tourism data\n",
    "3. Create a recommendation engine for tourist attractions\n",
    "\n",
    "-----------------------------\n",
    "## <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
    "-----------------------------\n",
    "\n",
    "**Part 1: Image Classification**\n",
    "\n",
    "1. **Structures_dataset.zip**: Training set of images of historical structures\n",
    "2. **dataset_test**: Test set of images of historical structures\n",
    "\n",
    "**Part 2: Tourism Recommendation**\n",
    "\n",
    "1. **user.csv**: User demographic data\n",
    "   - Columns: User_id, location, age\n",
    "2. **tourism_with_id.csv**: Details on tourist attractions in Indonesia's five largest cities\n",
    "   - Columns: Place_id, Place_name, Description, Category, City, Price, Rating, Time_minute, Coordinate, Lat, Long\n",
    "3. **tourism_rating.csv**: User ratings for tourist attractions\n",
    "   - Columns: user_id, place_id, place_rating\n",
    "\n",
    "-----------------------------------\n",
    "## <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
    "-----------------------------------\n",
    "\n",
    "\n",
    "**Part 1 - Deep Learning**\n",
    "\n",
    "1. Plot the sample images (8–10) from each class or category to gain a better understanding\n",
    "of each class\n",
    "   > Hint: You can use the OpenCV open-source library for this task.\n",
    "2. Select an CNN architecture of your choice to train the CV model. Configure the\n",
    "architecture for transfer learning, set up a TensorFlow environment for the selected\n",
    "backbone architecture, and load pre-trained weights\n",
    "   > Note: Algorithm or architecture selection is an important step in the training of ML models,\n",
    "so select the one that performs the best on your dataset.\n",
    "3. Deep learning models tend to work well with large amounts (millions of images) of data, but\n",
    "we may not always have enough data to train a deep learning model from scratch. Transfer\n",
    "learning techniques allow us to train and fine-tune large deep learning architectures using\n",
    "limited data.\n",
    "   > Hint: For transfer learning, use pre-trained CNN weights and freeze all convolutional\n",
    "layers' weights.\n",
    "4. As of now, CNN architecture has been configured for our model. Modify the top of this\n",
    "architecture to work with our dataset by:\n",
    "• Adding an appropriate number of dense layers with an activation function.\n",
    "• Using dropout for regularization.\n",
    "   > Note: It is important to understand that these parameters are hyperparameters that\n",
    "must be tuned.\n",
    "5. Compile the model with the right set of parameters like optimizer, loss function, and metric\n",
    "6. Define your callback class to stop the training once validation accuracy reaches a certain\n",
    "number of your choice\n",
    "7. Setup the train or test dataset directories and review the number of image samples for the train\n",
    "and test datasets for each class\n",
    "8. Train the model without augmentation while continuously monitoring the validation accuracy\n",
    "9. Train the model with augmentation and keep monitoring validation accuracy\n",
    "   > Note: Choose carefully the number of epochs, steps per epoch, and validation steps based on\n",
    "your computer configuration\n",
    "10. Visualize training and validation accuracy on the y-axis against each epoch on the x-axis to\n",
    "see if the model overfits after a certain epoch\n",
    "Deep learning\n",
    "\n",
    "**Part 2 - Data Science**\n",
    "\n",
    "1. Import all the datasets and perform preliminary inspections, such as:\n",
    "   1. Check for missing values and duplicates\n",
    "   2. Remove any anomalies found in the data\n",
    "2. To understand the tourism highlights better, we should explore the data in depth.\n",
    "   1. Explore the user group that provides the tourism ratings by:\n",
    "      - Analyzing the age distribution of users visiting the places and rating them\n",
    "      - Identifying the places where most of these users (tourists) are coming from\n",
    "3. Next, let's explore the locations and categories of tourist spots.\n",
    "   1. What are the different categories of tourist spots?\n",
    "   2. What kind of tourism each location is most famous or suitable for?\n",
    "   3. Which city would be best for a nature enthusiast to visit?\n",
    "4. To better understand tourism, we need to create a combined data with places and their user ratings.\n",
    "   1. Use this data to figure out the spots that are most loved by the tourists. Also, which city\n",
    "has the most loved tourist spots?\n",
    "   2. Indonesia provides a wide range of tourist spots ranging from historical and cultural\n",
    "beauties to advanced amusement parks. Among these, which category of places are users\n",
    "liking the most?\n",
    "5. Build a recommender model for the system\n",
    "   1. Use the above data to develop a collaborative filtering model for recommendation and\n",
    "use that to recommend other places to visit using the current tourist location(place name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2r6c89oQXxTJ"
   },
   "source": [
    "### <a id='toc1_4_1_'></a>[**Part 1 - Deep Learning**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmv8a41OXxTJ"
   },
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**Import Modules and Set Default Environment Variables**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FS3KJkebXxTJ",
    "outputId": "ce1f44eb-2f87-42bc-a3eb-2823d68a55b9"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Enable if having issues with Tensorflow\n",
    "# os.environ['TF_DISABLE_GRAPPLER'] = '1'\n",
    "\n",
    "# Load the environment variables\n",
    "# load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "# DATASET_PATH = f'{DATASET_PATH}/structures_dataset'\n",
    "\n",
    "# Uncomment when using Google Colab and Mounting Google Drive\n",
    "DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
    "\n",
    "# Mount Google Drive if using Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
    "except ImportError:\n",
    "    DATASET_PATH = '/Users/toddwalters/Library/CloudStorage/GoogleDrive-toddw4271@gmail.com/My Drive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
    "\n",
    "# Check TensorFlow Verision\n",
    "print()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Srk6fELyXxTJ"
   },
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**Look for corrupt files**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhNzewajXxTJ"
   },
   "outputs": [],
   "source": [
    "def verify_images(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = Image.open(file_path) # open the image file\n",
    "                    img.verify() # verify that it is, in fact an image\n",
    "                except (IOError, SyntaxError) as e:\n",
    "                    print(f'Bad file: {file_path}') # print out the names of corrupt files\n",
    "                    os.remove(file_path)\n",
    "                    print(f'Deleted bad file: {file_path}')\n",
    "\n",
    "# verify_images(f'{DATASET_PATH}/dataset_test')\n",
    "# verify_images(f'{DATASET_PATH}/structure_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NNsXI_UcLBJ"
   },
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**Rename Files**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCtxGIuocJVV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import string\n",
    "\n",
    "def random_string(length):\n",
    "    \"\"\"Generate a random string of fixed length\"\"\"\n",
    "    letters = string.ascii_lowercase + string.digits\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "def rename_files(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for filename in files:\n",
    "            # Generate a unique name\n",
    "            new_name = str(uuid.uuid4())\n",
    "\n",
    "            # Get the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "\n",
    "            # Create the new filename\n",
    "            new_filename = f\"{new_name}.jpg\"\n",
    "\n",
    "            # Full paths\n",
    "            old_file = os.path.join(root, filename)\n",
    "            new_file = os.path.join(root, new_filename)\n",
    "\n",
    "            # Rename the file\n",
    "            os.rename(old_file, new_file)\n",
    "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "\n",
    "# Uncomment if you want to rename all of the files in the dataset\n",
    "#\n",
    "# Specify the directory to start from\n",
    "# start_directory = f'{DATASET_PATH}/structures_dataset'\n",
    "# Run the function\n",
    "# rename_files(start_directory)\n",
    "# Specify the directory to start from\n",
    "# start_directory = f'{DATASET_PATH}/dataset_test'\n",
    "# # Run the function\n",
    "# rename_files(start_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy9L31YIcWPX"
   },
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**Plot The Sample Images**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j2knLkSoXxTJ",
    "outputId": "8d7c08e5-5e2b-4bb4-93ee-432517077e59"
   },
   "outputs": [],
   "source": [
    "def plot_sample_images(dataset_path, num_samples=8):\n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
    "\n",
    "    for i, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
    "\n",
    "        for j, image_name in enumerate(images):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            img = cv2.imread(image_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            axes[i, j].imshow(img)\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "        # Set the class name as the title for the first image in the row\n",
    "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images(f'{DATASET_PATH}/structures_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNb-3xO8XxTK"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a function `plot_sample_images` that takes a dataset path and plots a specified number of sample images from each class. It uses OpenCV to read the images and matplotlib to display them in a grid.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Visualizing sample images from each class helps us understand the nature of the data we're working with. It allows us to identify any potential issues with the images, such as inconsistent sizes, color schemes, or quality. This step is crucial for determining if any preprocessing steps are needed before training the model.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bv_nJW-ae5J"
   },
   "source": [
    "#### <a id='toc1_4_1_1_'></a>[**Define Utility Functions and Classes**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf0dLfVQaicV"
   },
   "outputs": [],
   "source": [
    "# Define utility functions and classes\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, generator, steps_per_epoch):\n",
    "        self.generator = generator\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.generator)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.generator.reset()\n",
    "\n",
    "def count_samples(dataset_path):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_counts[class_name] = len(os.listdir(class_path))\n",
    "    return class_counts\n",
    "\n",
    "class DebugCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
    "        print()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch % 50 == 0:\n",
    "            print()\n",
    "            print(f\"Starting batch {batch}\")\n",
    "            print()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"End of epoch {epoch + 1}\")\n",
    "        print()\n",
    "        if logs:\n",
    "            for key, value in logs.items():\n",
    "                print()\n",
    "                print(f\"{key}: {value}\")\n",
    "                print()\n",
    "        print()\n",
    "        print(\"--------------------\")\n",
    "        print()\n",
    "\n",
    "def create_data_generators(train_path, test_path, batch_size=32):\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        # target_size=(128, 128),\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        # target_size=(128, 128),\n",
    "        target_size=(224, 224),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(test_generator)\n",
    "\n",
    "    return DataGenerator(train_generator, steps_per_epoch), DataGenerator(test_generator, validation_steps), steps_per_epoch, validation_steps\n",
    "\n",
    "class DatasetLogger(Callback):\n",
    "    def __init__(self, train_generator, val_generator):\n",
    "        super().__init__()\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"Epoch {epoch + 1} - Train samples: {len(self.train_generator.generator.classes)}\")\n",
    "        print(f\"Epoch {epoch + 1} - Val samples: {len(self.val_generator.generator.classes)}\")\n",
    "        print()\n",
    "        print(\"--------------------\")\n",
    "        print()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            print()\n",
    "            print(f\"Epoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "            print(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
    "            print()\n",
    "            print(\"--------------------\")\n",
    "            print()\n",
    "\n",
    "class CustomValidationCallback(Callback):\n",
    "    def __init__(self, validation_data, validation_steps):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.validation_steps = validation_steps\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for i in range(self.validation_steps):\n",
    "            x, y = next(self.validation_data.generator)\n",
    "            val_metrics = self.model.test_on_batch(x, y)\n",
    "            val_loss += val_metrics[0]\n",
    "            val_accuracy += val_metrics[1]\n",
    "        val_loss /= self.validation_steps\n",
    "        val_accuracy /= self.validation_steps\n",
    "        logs['val_loss'] = val_loss\n",
    "        logs['val_accuracy'] = val_accuracy\n",
    "        print()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txsdv3rOXxTK"
   },
   "source": [
    "#### <a id='toc1_4_1_2_'></a>[**Select an CNN architecture**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35yzkV_1XxTK"
   },
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    # base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
    "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # inputs = Input(shape=(128, 128, 3))\n",
    "    inputs = Input(shape=(224, 224, 3))\n",
    "    x = CustomAugmentationLayer()(inputs)\n",
    "    x = base_model(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_gyywPTXxTK"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a function `create_model` that creates a CNN using transfer learning. It uses ResNet50 as the base model with pre-trained ImageNet weights. The base model layers are frozen, and custom dense layers are added on top for fine-tuning.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Transfer learning allows us to leverage pre-trained models on large datasets, which is particularly useful when we have limited data. By using a pre-trained model as a feature extractor and adding custom layers, we can adapt the model to our specific classification task while benefiting from the general features learned by the base model.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swTs85TTXxTK"
   },
   "source": [
    "#### <a id='toc1_4_1_3_'></a>[**Compile The Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJqh425GXxTK"
   },
   "outputs": [],
   "source": [
    "def compile_model(model):\n",
    "    initial_learning_rate = 0.001\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        # decay_steps=1000,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDCqMO3TXxTK"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a function `compile_model` that compiles the model with the Adam optimizer, categorical crossentropy loss function, and accuracy metric.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Compiling the model is a crucial step that defines how the model will be trained. The choice of optimizer, loss function, and metrics affects the training process and the model's performance. Categorical crossentropy is appropriate for multi-class classification tasks, and accuracy provides a clear measure of the model's performance.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VC0DVFsAXxTK"
   },
   "source": [
    "#### <a id='toc1_4_2_'></a>[**Define Callback Class**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EEh6fvpXxTK"
   },
   "outputs": [],
   "source": [
    "class AccuracyCallback(Callback):\n",
    "    def __init__(self, target_accuracy):\n",
    "        super().__init__()\n",
    "        self.target_accuracy = target_accuracy\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
    "            print()\n",
    "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
    "            print()\n",
    "            print(\"--------------------\")\n",
    "            print()\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o501BpdXxTK"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a custom callback class `AccuracyCallback` that stops the training when a target validation accuracy is reached.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Callbacks allow us to customize the training process. In this case, we're using a callback to prevent overfitting by stopping the training once we've reached a satisfactory level of validation accuracy. This helps us avoid wasting computational resources and reduces the risk of the model memorizing the training data.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1MRDUL0XxTK"
   },
   "source": [
    "#### <a id='toc1_4_2_1_'></a>[**Set Up Dataset Directories And Review Sample Numbers**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNIxwiL7XxTK",
    "outputId": "6de72021-0a59-4273-c161-8896fad03aba"
   },
   "outputs": [],
   "source": [
    "# Setup Dataset Directories and Review Sample Numbers\n",
    "train_path = f'{DATASET_PATH}/structures_dataset'\n",
    "test_path = f'{DATASET_PATH}/dataset_test'\n",
    "\n",
    "train_counts = count_samples(train_path)\n",
    "test_counts = count_samples(test_path)\n",
    "\n",
    "print()\n",
    "print(\"\\nTraining samples per class:\")\n",
    "for class_name, count in sorted(train_counts.items()):\n",
    "    print(f\"{class_name}: {count}\")\n",
    "\n",
    "print(\"\\nTest samples per class:\")\n",
    "for class_name, count in sorted(test_counts.items()):\n",
    "    print(f\"{class_name}: {count}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Create and compile the model\n",
    "num_classes = len(train_counts)\n",
    "model = create_model(num_classes)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "compiled_model = compile_model(model)\n",
    "\n",
    "# Set up data generators\n",
    "batch_size = 32\n",
    "train_generator, test_generator, steps_per_epoch, validation_steps = create_data_generators(train_path, test_path, batch_size)\n",
    "\n",
    "print()\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAM9ERJ2XxTK"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a function `count_samples` that counts the number of samples in each class for a given dataset. It then applies this function to both the training and test datasets and prints the results.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Understanding the distribution of samples across classes is crucial for several reasons:\n",
    "\n",
    "    1. It helps identify any class imbalance issues that may need to be addressed.\n",
    "    2. It ensures we have enough samples in each class for both training and testing.\n",
    "    3. It helps in setting appropriate batch sizes and steps per epoch during training.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsRYKfuzXxTL"
   },
   "source": [
    "#### <a id='toc1_4_2_2_'></a>[**Train The Model Without Augmentation**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "N4YBG86DXxTL",
    "outputId": "5b39e12a-5d17-45e2-ac07-c674ac2d74e5"
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up callbacks\n",
    "    accuracy_callback = AccuracyCallback(target_accuracy=0.95)\n",
    "    debug_callback = DebugCallback()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    dataset_logger = DatasetLogger(train_generator, test_generator)\n",
    "    custom_validation = CustomValidationCallback(test_generator, validation_steps)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.generator.classes), y=train_generator.generator.classes)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    try:\n",
    "        # Separate evaluation before training\n",
    "        model.summary\n",
    "        print()\n",
    "        print(\"Evaluating model before training:\")\n",
    "        evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "        print(f\"Initial evaluation: {evaluation}\")\n",
    "        print()\n",
    "\n",
    "        # Training\n",
    "        history = compiled_model.fit(\n",
    "            train_generator.generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=20,\n",
    "            validation_data=test_generator.generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation],\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        print()\n",
    "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f\"An error occurred during training: {e}\")\n",
    "        print()\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Final evaluation\n",
    "    print()\n",
    "    print(\"Final evaluation:\")\n",
    "    print(\"-----------------\")\n",
    "    final_evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "    print()\n",
    "    print(f\"Final evaluation: {final_evaluation}\")\n",
    "    print()\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDg5KrjcXxTL"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code sets up data generators for the training and test datasets, then trains the model using these generators. The `ImageDataGenerator` is used to load and preprocess images in batches, which is memory-efficient for large datasets.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Training the model without augmentation provides a baseline performance. It allows us to see how well the model performs with the original data before applying any data augmentation techniques. This step is crucial for understanding if the model has enough capacity to learn from the data and if there are any immediate issues like overfitting or underfitting.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEtTGXrVXxTL"
   },
   "source": [
    "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zuoPxxZpXxTL",
    "outputId": "ba27e578-4c73-4ed6-b05b-93f689365b94"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
    "from tensorflow.keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
    "from tensorflow.keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# To ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class AccuracyCallback(Callback):\n",
    "    def __init__(self, target_accuracy):\n",
    "        super().__init__()\n",
    "        self.target_accuracy = target_accuracy\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
    "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
    "            print()\n",
    "            print(\"--------------------\")\n",
    "            print()\n",
    "            self.model.stop_training = True\n",
    "\n",
    "class AugmentationCallback(Callback):\n",
    "    def __init__(self, train_generator):\n",
    "        super().__init__()\n",
    "        self.train_generator = train_generator\n",
    "\n",
    "class CustomAugmentationLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomAugmentationLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            augmented = tf.image.random_contrast(inputs, lower=0.8, upper=1.2)\n",
    "            augmented = tf.image.random_saturation(augmented, lower=0.8, upper=1.2)\n",
    "            return augmented\n",
    "        return inputs\n",
    "\n",
    "class CustomValidationCallback(Callback):\n",
    "    def __init__(self, validation_data, validation_steps):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.validation_steps = validation_steps\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for i in range(self.validation_steps):\n",
    "            x, y = next(self.validation_data.generator)\n",
    "            val_metrics = self.model.test_on_batch(x, y)\n",
    "            val_loss += val_metrics[0]\n",
    "            val_accuracy += val_metrics[1]\n",
    "        val_loss /= self.validation_steps\n",
    "        val_accuracy /= self.validation_steps\n",
    "        logs['val_loss'] = val_loss\n",
    "        logs['val_accuracy'] = val_accuracy\n",
    "        print()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print()\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, generator, steps_per_epoch):\n",
    "        self.generator = generator\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.generator)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.generator.reset()\n",
    "\n",
    "class DatasetLogger(Callback):\n",
    "    def __init__(self, train_generator, val_generator):\n",
    "        super().__init__()\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"Epoch {epoch + 1} - Train samples: {len(self.train_generator.generator.classes)}\")\n",
    "        print(f\"Epoch {epoch + 1} - Val samples: {len(self.val_generator.generator.classes)}\")\n",
    "        print()\n",
    "        print(\"--------------------\")\n",
    "        print()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            print()\n",
    "            print(f\"Epoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "            print(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
    "            print()\n",
    "            print(\"--------------------\")\n",
    "            print()\n",
    "\n",
    "class DebugCallback(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
    "        print()\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        if batch % 50 == 0:\n",
    "            print()\n",
    "            print(f\"Starting batch {batch}\")\n",
    "            print()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print()\n",
    "        print(f\"End of epoch {epoch + 1}\")\n",
    "        print()\n",
    "        if logs:\n",
    "            for key, value in logs.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "        print()\n",
    "        print(\"--------------------\")\n",
    "        print()\n",
    "\n",
    "class LearningRateFinder:\n",
    "    def __init__(self, model, stop_factor=4, beta=0.98):\n",
    "        self.model = model\n",
    "        self.stop_factor = stop_factor\n",
    "        self.beta = beta\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.best_loss = 1e9\n",
    "        self.avg_loss = 0\n",
    "        self.batch_num = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.best_loss = 1e9\n",
    "        self.avg_loss = 0\n",
    "        self.batch_num = 0\n",
    "        self.weightsFile = None\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr = lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        loss = logs[\"loss\"]\n",
    "        self.batch_num += 1\n",
    "        self.avg_loss = (self.beta * self.avg_loss) + ((1 - self.beta) * loss)\n",
    "        smooth = self.avg_loss / (1 - (self.beta ** self.batch_num))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        stop_loss = self.stop_factor * self.best_loss\n",
    "\n",
    "        if self.batch_num > 1 and smooth < self.best_loss:\n",
    "            self.best_loss = smooth\n",
    "\n",
    "        if smooth > stop_loss:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        if self.batch_num % 10 == 0:  # Print every 10 batches\n",
    "            print(f\"Batch {self.batch_num}: lr = {lr:.6f}, loss = {smooth:.4f}\")\n",
    "\n",
    "    def find(self, train_data, start_lr, end_lr, batch_size=32, epochs=5):\n",
    "        self.reset()\n",
    "        num_samples = len(train_data.generator.classes)\n",
    "        steps_per_epoch = math.ceil(num_samples / batch_size)\n",
    "\n",
    "        self.weightsFile = \"lrf_weights.h5\"\n",
    "        self.model.save_weights(self.weightsFile)\n",
    "\n",
    "        orig_lr = self.model.optimizer.lr.numpy()\n",
    "        self.model.optimizer.lr.assign(start_lr)\n",
    "\n",
    "        print()\n",
    "        print(f\"Starting Learning Rate Finder: LR range {start_lr} to {end_lr}\")\n",
    "        print()\n",
    "\n",
    "        lr_schedule = lambda epoch, batch: start_lr * (end_lr / start_lr) ** (batch / (epochs * steps_per_epoch))\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "        self.model.fit(train_data.generator,\n",
    "                       steps_per_epoch=steps_per_epoch,\n",
    "                       epochs=epochs,\n",
    "                       callbacks=[callback, LearningRateScheduler(lr_schedule)],\n",
    "                       verbose=0)\n",
    "\n",
    "        print()\n",
    "        print(\"Learning Rate Finder complete\")\n",
    "        print()\n",
    "\n",
    "        self.model.load_weights(self.weightsFile)\n",
    "        self.model.optimizer.lr.assign(orig_lr)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Learning Rate vs. Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    def get_best_lr(self):\n",
    "        min_grad_idx = np.argmin(np.gradient(np.array(self.losses)))\n",
    "        best_lr = self.lrs[min_grad_idx]\n",
    "        print()\n",
    "        print(f\"Best initial learning rate found: {best_lr:.6f}\")\n",
    "        print()\n",
    "        return best_lr\n",
    "\n",
    "    def print_results(self):\n",
    "        print()\n",
    "        print(\"\\nLearning Rate Finder Results:\")\n",
    "        print(f\"Minimum loss: {min(self.losses):.4f}\")\n",
    "        print(f\"Maximum loss: {max(self.losses):.4f}\")\n",
    "        print(f\"Learning rate range: {min(self.lrs):.6f} to {max(self.lrs):.6f}\")\n",
    "        print()\n",
    "\n",
    "def count_samples(dataset_path):\n",
    "    class_counts = {}\n",
    "    for class_name in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            class_counts[class_name] = len(os.listdir(class_path))\n",
    "    return class_counts\n",
    "\n",
    "def create_model(num_classes, architecture, input_shape):\n",
    "\n",
    "    base_model = architecture(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = CustomAugmentationLayer()(inputs)\n",
    "    x = base_model(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "def compile_model(model, learning_rate):\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        learning_rate,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  steps_per_execution=8)\n",
    "    return model\n",
    "\n",
    "def create_data_generators(train_path, test_path, batch_size, target_size, preprocessing_function):\n",
    "    preprocess_func = preprocessing_function\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_func,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_func)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(test_generator)\n",
    "\n",
    "    return DataGenerator(train_generator, steps_per_epoch), DataGenerator(test_generator, validation_steps), steps_per_epoch, validation_steps\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "def main(find_lr=True, pretrain_model_eval=True, transfer_learning_model='Any'):\n",
    "\n",
    "    # Mount Google Drive if using Colab\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
    "    except ImportError:\n",
    "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "        DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "    try:\n",
    "        ARCHITECTURES = {\n",
    "            'ResNet50V2': ResNet50V2,\n",
    "            'VGG16': VGG16,\n",
    "            'InceptionV3': InceptionV3,\n",
    "            'MobileNetV2': MobileNetV2,\n",
    "            'EfficientNetB0': EfficientNetB0\n",
    "        }\n",
    "\n",
    "        ARCHITECTURE_INPUT_SHAPES = {\n",
    "            'ResNet50V2': (128, 128, 3),\n",
    "            'VGG16': (224, 224, 3),\n",
    "            'InceptionV3': (299, 299, 3),\n",
    "            'MobileNetV2': (224, 224, 3),\n",
    "            'EfficientNetB0': (224, 224, 3)\n",
    "        }\n",
    "\n",
    "        PREPROCESSING_FUNCTIONS = {\n",
    "            'ResNet50V2': resnet_preprocess,\n",
    "            'VGG16': vgg_preprocess,\n",
    "            'InceptionV3': inception_preprocess,\n",
    "            'MobileNetV2': mobilenet_preprocess,\n",
    "            'EfficientNetB0': efficientnet_preprocess\n",
    "        }\n",
    "\n",
    "        # ----------- HYPERPARAMETERS -----------\n",
    "        #\n",
    "        batch_size = 32\n",
    "        # Learning Rate Epochs\n",
    "        lr_epochs = 5\n",
    "        # Default Learning Rate\n",
    "        initial_learning_rate = 0.001\n",
    "        # Early Stopping Patience\n",
    "        patience = 5\n",
    "        # Accuracy Callback\n",
    "        target_accuracy = 0.95\n",
    "        # Training Epochs\n",
    "        trn_epochs = 20\n",
    "        #\n",
    "        # ----------- HYPERPARAMETERS -----------\n",
    "\n",
    "        # Architecture - Change this to switch architectures\n",
    "        tl_model = transfer_learning_model\n",
    "        architecture = ARCHITECTURES[tl_model]\n",
    "        input_shape = ARCHITECTURE_INPUT_SHAPES[architecture.__name__]\n",
    "        target_size = ARCHITECTURE_INPUT_SHAPES[architecture.__name__][:2]\n",
    "        preprocessing_function = PREPROCESSING_FUNCTIONS[architecture.__name__]\n",
    "\n",
    "        # Set up dataset directories\n",
    "        train_path = f'{DATASET_PATH}/structures_dataset'\n",
    "        test_path = f'{DATASET_PATH}/dataset_test'\n",
    "\n",
    "        train_counts = count_samples(train_path)\n",
    "        test_counts = count_samples(test_path)\n",
    "\n",
    "        print()\n",
    "        print(f\"TensorFlow version: {tf.__version__}\")\n",
    "        print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
    "        print()\n",
    "\n",
    "        print()\n",
    "        print(\"\\nTraining samples per class:\")\n",
    "        for class_name, count in sorted(train_counts.items()):\n",
    "            print(f\"{class_name}: {count}\")\n",
    "        print()\n",
    "\n",
    "        print()\n",
    "        print(\"\\nTest samples per class:\")\n",
    "        for class_name, count in sorted(test_counts.items()):\n",
    "            print(f\"{class_name}: {count}\")\n",
    "        print()\n",
    "\n",
    "        # Create the model\n",
    "        num_classes = len(train_counts)\n",
    "        model = create_model(num_classes, architecture, input_shape)\n",
    "\n",
    "        print()\n",
    "        print(f\"Using {architecture} architecture\")\n",
    "        print(model.summary())\n",
    "        print()\n",
    "\n",
    "        # Set up data generators\n",
    "        train_generator, test_generator, steps_per_epoch, validation_steps = create_data_generators(\n",
    "            train_path, test_path, batch_size=batch_size, target_size=target_size, preprocessing_function=preprocessing_function)\n",
    "\n",
    "        print()\n",
    "        print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "        print(f\"Validation steps: {validation_steps}\")\n",
    "        print()\n",
    "\n",
    "        # Compile the model with a temporary learning rate\n",
    "        compiled_model = compile_model(model, learning_rate=initial_learning_rate)\n",
    "\n",
    "        if find_lr:\n",
    "            # Find optimal learning rate\n",
    "            print()\n",
    "            print(\"Finding optimal learning rate...\")\n",
    "            print()\n",
    "            lr_finder = LearningRateFinder(compiled_model)\n",
    "            lr_finder.find(train_generator, start_lr=1e-7, end_lr=1, epochs=5)\n",
    "            lr_finder.plot_loss()\n",
    "            lr_finder.print_results()\n",
    "            best_lr = lr_finder.get_best_lr()\n",
    "            best_lr = float(best_lr)  # Convert to float\n",
    "            print()\n",
    "            print(f\"Best initial learning rate: {best_lr}\")\n",
    "            print()\n",
    "            # Recompile the model with the best learning rate\n",
    "            print(\"Recompiling model with best learning rate...\")\n",
    "            print()\n",
    "            compiled_model = compile_model(model, learning_rate=best_lr)\n",
    "            \n",
    "        else:\n",
    "            best_lr = initial_learning_rate\n",
    "\n",
    "        # Set up callbacks\n",
    "        accuracy_callback = AccuracyCallback(target_accuracy=target_accuracy)\n",
    "        debug_callback = DebugCallback()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        dataset_logger = DatasetLogger(train_generator, test_generator)\n",
    "        custom_validation = CustomValidationCallback(test_generator, validation_steps)\n",
    "        augmentation_callback = AugmentationCallback(train_generator)\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.generator.classes), y=train_generator.generator.classes)\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "        if pretrain_model_eval:\n",
    "            # Separate evaluation before training\n",
    "            print()\n",
    "            print(\"Evaluating model before training:\")\n",
    "            print()\n",
    "            evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "            print()\n",
    "            print(f\"Initial evaluation: {evaluation}\")\n",
    "            print()\n",
    "            \n",
    "        try:\n",
    "            # Training\n",
    "            history = compiled_model.fit(\n",
    "                train_generator.generator,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                epochs=trn_epochs,\n",
    "                validation_data=test_generator.generator,\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[accuracy_callback, augmentation_callback, custom_validation, debug_callback, dataset_logger, early_stopping, lr_callback],\n",
    "                class_weight=class_weight_dict,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            print()\n",
    "            print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print()\n",
    "            print(f\"An error occurred during training: {e}\")\n",
    "            print()\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Final evaluation\n",
    "        print()\n",
    "        print(\"Final evaluation:\")\n",
    "        print(\"-----------------\")\n",
    "        final_evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "        print()\n",
    "        print(f\"Final evaluation: {final_evaluation}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print()\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(find_lr=True, pretrain_model_eval=True, transfer_learning_model='EfficientNetB0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZrEUMa6XxTL"
   },
   "source": [
    "**Explanations:**\n",
    "* This codebase implements a Convolutional Neural Network (CNN) using transfer learning with the ResNet50V2 architecture for image classification. It uses a dataset of architectural elements (e.g., altars, domes, columns) split into training and test sets. The model is compiled with Adam optimizer and categorical crossentropy loss. Data augmentation is applied to increase the diversity of training samples. The training process includes callbacks for early stopping, learning rate scheduling, and custom validation. The model's performance is evaluated and visualized over 20 epochs.\n",
    "\n",
    "1. `DataGenerator` class:\n",
    "  - Optimizable parameters:\n",
    "    - None directly, but it relies on the ImageDataGenerator settings.\n",
    "\n",
    "2. `CustomAugmentationLayer` class:\n",
    "   - Optimizable parameters:\n",
    "    - Contrast range (lower, upper)\n",
    "    - Saturation range (lower, upper)\n",
    "   - Role in optimization:\n",
    "    - Adjusting these can increase data diversity and potentially reduce overfitting.\n",
    "\n",
    "3. `create_model` function:\n",
    "   - Optimizable parameters:\n",
    "    - Number and size of dense layers\n",
    "    - Dropout rate (currently 0.5)\n",
    "    - L2 regularization strength\n",
    "   - Role in optimization:\n",
    "    - Adjusting model complexity can help balance underfitting and overfitting.\n",
    "\n",
    "4. `compile_model` function:\n",
    "   - Optimizable parameters:\n",
    "    - Initial learning rate (currently 0.001)\n",
    "    - Learning rate decay steps and rate\n",
    "    - Choice of optimizer (currently Adam)\n",
    "   - Role in optimization:\n",
    "    - Proper learning rate and optimizer settings are crucial for efficient training and convergence.\n",
    "\n",
    "5. `AccuracyCallback` class:\n",
    "   - Optimizable parameters:\n",
    "    - Target accuracy threshold\n",
    "   - Role in optimization:\n",
    "    - Adjusting this can prevent premature stopping or unnecessarily long training.\n",
    "\n",
    "6. `count_samples` function:\n",
    "   - Optimizable parameters:\n",
    "    - None\n",
    "\n",
    "7. `DebugCallback` class:\n",
    "   - Optimizable parameters:\n",
    "    - None, but frequency of debug prints could be adjusted.\n",
    "\n",
    "8. `create_data_generators` function:\n",
    "   - Optimizable parameters:\n",
    "    - Batch size (currently 32)\n",
    "    - Data augmentation parameters (rotation_range, width_shift_range, etc.)\n",
    "   - Role in optimization:\n",
    "    - Proper batch size and augmentation can improve training stability and reduce overfitting.\n",
    "\n",
    "9. `DatasetLogger` class:\n",
    "   - Optimizable parameters:\n",
    "    - None\n",
    "\n",
    "10. `CustomValidationCallback` class:\n",
    "    - Optimizable parameters:\n",
    "      - None, but custom metrics could be added.\n",
    "\n",
    "11. `visualize_augmentation` function:\n",
    "    - Optimizable parameters:\n",
    "      - None\n",
    "\n",
    "12. `AugmentationCallback` class:\n",
    "    - Optimizable parameters:\n",
    "     - Frequency of augmentation visualization\n",
    "    - Role in optimization:\n",
    "      - Adjusting this helps in monitoring augmentation effects without slowing training.\n",
    "\n",
    "13. Main execution block:\n",
    "    - Optimizable parameters:\n",
    "      - Number of epochs (currently 20)\n",
    "      - Early stopping patience (currently 10)\n",
    "      - Class weight calculation method\n",
    "    - Role in optimization:\n",
    "      - These parameters affect overall training duration and handling of class imbalance.\n",
    "\n",
    "Additional global optimizations:\n",
    "1. Learning rate scheduling: Implement more sophisticated schedules like cyclic learning rates or warm restarts.\n",
    "2. Model architecture: Experiment with different base models (e.g., EfficientNet, VGG) or custom architectures.\n",
    "3. Regularization: Add or adjust L2 regularization, increase dropout rates, or implement other techniques like label smoothing.\n",
    "4. Data preprocessing: Normalize input data, apply additional augmentation techniques like mixup or cutout.\n",
    "5. Ensemble methods: Train multiple models and combine their predictions for improved reliability.\n",
    "6. Cross-validation: Implement k-fold cross-validation for more robust performance estimation.\n",
    "7. Gradient clipping: Add gradient clipping to prevent exploding gradients and stabilize training.\n",
    "8. Batch normalization: Add batch normalization layers for improved training stability and potentially faster convergence.\n",
    "9. Learning rate finder: Implement a learning rate finder to determine optimal initial learning rates.\n",
    "10. Progressive resizing: Start training with smaller image sizes and gradually increase, potentially speeding up early training stages.\n",
    "\n",
    "By carefully tuning these parameters and implementing these techniques, you can work towards optimizing the model's performance, improving its reliability, and finding the right balance between underfitting and overfitting for your specific architectural element classification task.\n",
    "\n",
    "**Why It Is Important:**\n",
    "* Using a CNN with transfer learning is important because it leverages pre-trained weights on a large dataset (ImageNet), allowing the model to learn high-level features more quickly and effectively, especially when dealing with a relatively small dataset. This approach often leads to better performance and faster convergence compared to training a model from scratch, making it particularly useful for specialized image classification tasks like identifying architectural elements.\n",
    "\n",
    "**Observations:**\n",
    "* The model quickly achieves high accuracy (>80%) within the first few epochs.\n",
    "* Validation accuracy peaks around epoch 6 at about 86% and then fluctuates.\n",
    "* There's some overfitting, as training accuracy consistently exceeds validation accuracy.\n",
    "* The learning rate decreases steadily over the epochs, as designed by the exponential decay schedule.\n",
    "* Both training and validation loss decrease rapidly initially and then plateau.\n",
    "* The model's performance seems to stabilize after about 10-15 epochs.\n",
    "\n",
    "**Conclusions:**\n",
    "* The transfer learning approach is effective, allowing the model to achieve good performance quickly.\n",
    "* The model reaches a reasonable accuracy (~83% on the validation set) for classifying architectural elements.\n",
    "* There's room for improvement, as the model shows signs of overfitting and doesn't consistently improve after the initial rapid progress.\n",
    "* The current learning rate schedule and data augmentation help, but may not be optimal for this specific task.\n",
    "\n",
    "**Recommendations:**\n",
    "* Experiment with stronger regularization techniques (e.g., increased dropout, L2 regularization) to combat overfitting.\n",
    "* Try different learning rate schedules, such as cyclical learning rates or a more aggressive decay, to potentially improve convergence.\n",
    "* Increase data augmentation to provide more diverse training samples and potentially reduce overfitting.\n",
    "* Consider implementing techniques like gradient clipping to stabilize training.\n",
    "* Explore ensemble methods or cross-validation to improve overall performance and reliability.\n",
    "* Analyze misclassifications to identify patterns and potentially refine the model architecture or data preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXoAMHyrXxTL"
   },
   "source": [
    "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "k5dzjyc3XxTL",
    "outputId": "9363f534-526a-40c0-cda4-2da9b6086bf0"
   },
   "outputs": [],
   "source": [
    "# def plot_training_history(history, lr_logger):\n",
    "def plot_training_history(history):\n",
    "\n",
    "    # Plot training history and learning rate\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # # Plot learning rate\n",
    "    # plt.subplot(1, 3, 3)\n",
    "    # plt.plot(range(1, len(lr_logger.lrs) + 1), lr_logger.lrs)\n",
    "    # plt.title('Learning Rate over Epochs')\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylabel('Learning Rate')\n",
    "    # \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "# plot_training_history(history, lr_logger)\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2FxsHI4XxTL"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- This code defines a function `plot_training_history` that visualizes the training and validation accuracy and loss over epochs. It then applies this function to the training history of the augmented model.\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- Visualizing the training and validation metrics helps us understand how the model is learning over time. It allows us to identify potential issues such as overfitting (when training accuracy continues to improve but validation accuracy plateaus or decreases) or underfitting (when both training and validation accuracy are low and not improving). This information is crucial for deciding whether to adjust the model architecture, change hyperparameters, or modify the training process.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsb4RIZGXxTL"
   },
   "source": [
    "#### <a id='toc1_4_2_5_'></a>[**Test Trained Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lWZIK3E1XxTL",
    "outputId": "4e7a6be3-e719-4268-a1e9-cef8401e1fb0"
   },
   "outputs": [],
   "source": [
    "def predict_image_class(model, img_path, class_names, target_size, preprocessing_function):\n",
    "    target_size = target_size  # Get the target size for the chosen architecture\n",
    "    preprocess_func = preprocessing_function  # Get the preprocessing function for the chosen architecture\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_func(img_array)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions[0])\n",
    "    predicted_class = class_names[predicted_class_index]\n",
    "    confidence = predictions[0][predicted_class_index]\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "def visualize_prediction(img_path, target_size, predicted_class, confidence):\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
    "    plt.show()\n",
    "\n",
    "# Save the class names\n",
    "class_names = list(train_generator.generator.class_indices.keys())\n",
    "\n",
    "# Test the model on new images\n",
    "test_image_paths = [\n",
    "    f'{DATASET_PATH}/model_test_images/gargoyle.jpg',\n",
    "    f'{DATASET_PATH}/model_test_images/purdue-bell-tower.jpg',\n",
    "    f'{DATASET_PATH}/model_test_images/purdue-memorial-union-stained-glass-2.jpg',\n",
    "]\n",
    "print()\n",
    "print(\"\\nTesting model on new images:\")\n",
    "print()\n",
    "\n",
    "for img_path in test_image_paths:\n",
    "    predicted_class, confidence = predict_image_class(compiled_model, img_path, class_names, target_size, preprocessing_function)\n",
    "    print()\n",
    "    print(f\"Image: {img_path}\")\n",
    "    print(f\"Predicted class: {predicted_class}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Visualize prediction\n",
    "    visualize_prediction(img_path, target_size, predicted_class, confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlyPwmHMXxTL"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFfqGCDiXxTP"
   },
   "source": [
    "#### <a id='toc1_4_2_3_'></a>[**Train The Model With Augmentation - v2**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Dict, Tuple, List\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import (\n",
    "    EfficientNetB0,\n",
    "    InceptionV3,\n",
    "    MobileNetV2,\n",
    "    ResNet50V2,\n",
    "    VGG16,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback,\n",
    "    EarlyStopping,\n",
    "    LambdaCallback,\n",
    "    LearningRateScheduler,\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling2D,\n",
    "    Input,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load configuration\n",
    "with open('config.yaml', 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    architecture: str\n",
    "    input_shape: Tuple[int, int, int]\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    batch_size: int\n",
    "    initial_learning_rate: float\n",
    "    epochs: int\n",
    "    early_stopping_patience: int\n",
    "    target_accuracy: float\n",
    "    lr_epochs: int\n",
    "\n",
    "model_config = ModelConfig(**config['model'])\n",
    "training_config = TrainingConfig(**config['training'])\n",
    "\n",
    "# Custom exceptions\n",
    "class DatasetError(Exception):\n",
    "    \"\"\"Exception raised for errors in the dataset.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ModelError(Exception):\n",
    "    \"\"\"Exception raised for errors in the model creation or training.\"\"\"\n",
    "    pass\n",
    "\n",
    "class AccuracyCallback(Callback):\n",
    "    \"\"\"Callback to stop training when a target accuracy is reached.\"\"\"\n",
    "\n",
    "    def __init__(self, target_accuracy: float):\n",
    "        super().__init__()\n",
    "        self.target_accuracy = target_accuracy\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
    "            logger.info(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
    "            logger.info(\"\\n--------------------\\n\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "class AugmentationCallback(Callback):\n",
    "    \"\"\"Callback to work with the CustomAugmentationLayer.\"\"\"\n",
    "\n",
    "    def __init__(self, train_generator: DataGenerator):\n",
    "        super().__init__()\n",
    "        self.train_generator = train_generator\n",
    "\n",
    "    def on_batch_begin(self, batch: int, logs: Dict[str, float] = None) -> None:\n",
    "        # You can add custom logic here if needed\n",
    "        pass\n",
    "\n",
    "class CustomValidationCallback(Callback):\n",
    "    \"\"\"Callback for custom validation after each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, validation_data: DataGenerator, validation_steps: int):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.validation_steps = validation_steps\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for i in range(self.validation_steps):\n",
    "            x, y = next(self.validation_data.generator)\n",
    "            val_metrics = self.model.test_on_batch(x, y)\n",
    "            val_loss += val_metrics[0]\n",
    "            val_accuracy += val_metrics[1]\n",
    "        val_loss /= self.validation_steps\n",
    "        val_accuracy /= self.validation_steps\n",
    "        logs['val_loss'] = val_loss\n",
    "        logs['val_accuracy'] = val_accuracy\n",
    "        logger.info(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "class DatasetLogger(Callback):\n",
    "    \"\"\"Callback to log dataset information at the beginning of each epoch.\"\"\"\n",
    "\n",
    "    def __init__(self, train_generator: DataGenerator, val_generator: DataGenerator):\n",
    "        super().__init__()\n",
    "        self.train_generator = train_generator\n",
    "        self.val_generator = val_generator\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        logger.info(f\"Epoch {epoch + 1} - Train samples: {len(self.train_generator.generator.classes)}\")\n",
    "        logger.info(f\"Epoch {epoch + 1} - Val samples: {len(self.val_generator.generator.classes)}\")\n",
    "        logger.info(\"--------------------\")\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        if logs is not None:\n",
    "            logger.info(f\"Epoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
    "            logger.info(\"--------------------\")\n",
    "\n",
    "class DebugCallback(Callback):\n",
    "    \"\"\"Callback for debugging purposes, providing detailed information about the training process.\"\"\"\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        logger.info(f\"\\nStarting epoch {epoch + 1}\")\n",
    "\n",
    "    def on_batch_begin(self, batch: int, logs: Dict[str, float] = None) -> None:\n",
    "        if batch % 50 == 0:\n",
    "            logger.info(f\"Starting batch {batch}\")\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, logs: Dict[str, float] = None) -> None:\n",
    "        logger.info(f\"End of epoch {epoch + 1}\")\n",
    "        if logs:\n",
    "            for key, value in logs.items():\n",
    "                logger.info(f\"{key}: {value}\")\n",
    "        logger.info(\"--------------------\")\n",
    "\n",
    "class LRLogger(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lrs = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = float(K.get_value(self.model.optimizer.lr))\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "class CustomAugmentationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"Custom augmentation layer for image preprocessing.\"\"\"\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool = None) -> tf.Tensor:\n",
    "        if training:\n",
    "            augmented = tf.image.random_contrast(inputs, lower=0.8, upper=1.2)\n",
    "            augmented = tf.image.random_saturation(augmented, lower=0.8, upper=1.2)\n",
    "            return augmented\n",
    "        return inputs\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    \"\"\"Custom data generator for feeding data to the model.\"\"\"\n",
    "\n",
    "    def __init__(self, generator, steps_per_epoch: int):\n",
    "        self.generator = generator\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.steps_per_epoch\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        return next(self.generator)\n",
    "\n",
    "    def on_epoch_end(self) -> None:\n",
    "        self.generator.reset()\n",
    "\n",
    "class LearningRateFinder:\n",
    "    \"\"\"Class to find the optimal learning rate.\"\"\"\n",
    "\n",
    "    def __init__(self, model: Model, stop_factor: float = 4, beta: float = 0.98):\n",
    "        self.model = model\n",
    "        self.stop_factor = stop_factor\n",
    "        self.beta = beta\n",
    "        self.lrs: List[float] = []\n",
    "        self.losses: List[float] = []\n",
    "        self.best_loss = float('inf')\n",
    "        self.avg_loss = 0\n",
    "        self.batch_num = 0\n",
    "        self.weights_file = None\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        \"\"\"Reset the state of the learning rate finder.\"\"\"\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "        self.best_loss = float('inf')\n",
    "        self.avg_loss = 0\n",
    "        self.batch_num = 0\n",
    "        self.weights_file = None\n",
    "\n",
    "    def on_batch_end(self, batch: int, logs: Dict[str, float]) -> None:\n",
    "        \"\"\"Callback method to update learning rate and loss after each batch.\"\"\"\n",
    "        lr = self.model.optimizer.lr\n",
    "        if hasattr(lr, 'numpy'):\n",
    "            lr = lr.numpy()\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        loss = logs[\"loss\"]\n",
    "        self.batch_num += 1\n",
    "        self.avg_loss = (self.beta * self.avg_loss) + ((1 - self.beta) * loss)\n",
    "        smooth = self.avg_loss / (1 - (self.beta ** self.batch_num))\n",
    "        self.losses.append(smooth)\n",
    "\n",
    "        stop_loss = self.stop_factor * self.best_loss\n",
    "\n",
    "        if self.batch_num > 1 and smooth < self.best_loss:\n",
    "            self.best_loss = smooth\n",
    "\n",
    "        if smooth > stop_loss:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        if self.batch_num % 100 == 0:\n",
    "            logger.info(f\"Batch {self.batch_num}: lr = {lr:.6f}, loss = {smooth:.4f}\")\n",
    "\n",
    "    def find(self, train_data: DataGenerator, start_lr: float, end_lr: float, batch_size: int = 32, epochs: int = 5) -> None:\n",
    "        \"\"\"Find the optimal learning rate.\"\"\"\n",
    "        self.reset()\n",
    "        num_samples = len(train_data.generator.classes)\n",
    "        steps_per_epoch = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "        self.weights_file = \"lrf_weights.h5\"\n",
    "        self.model.save_weights(self.weights_file)\n",
    "\n",
    "        orig_lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, start_lr)\n",
    "\n",
    "        logger.info(f\"Starting Learning Rate Finder: LR range {start_lr} to {end_lr}\")\n",
    "\n",
    "        lr_schedule = lambda epoch, batch: start_lr * (end_lr / start_lr) ** (batch / (epochs * steps_per_epoch))\n",
    "\n",
    "        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n",
    "        self.model.fit(\n",
    "            train_data.generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=epochs,\n",
    "            callbacks=[callback, LearningRateScheduler(lr_schedule)],\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        logger.info(\"Learning Rate Finder complete\")\n",
    "\n",
    "        self.model.load_weights(self.weights_file)\n",
    "        K.set_value(self.model.optimizer.lr, orig_lr)\n",
    "\n",
    "    def plot_loss(self) -> None:\n",
    "        \"\"\"Plot the loss against the learning rate.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.lrs, self.losses)\n",
    "        plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Learning Rate vs. Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    def get_best_lr(self) -> float:\n",
    "        \"\"\"Get the best learning rate based on the minimum gradient of the loss curve.\"\"\"\n",
    "        min_grad_idx = np.argmin(np.gradient(np.array(self.losses)))\n",
    "        return self.lrs[min_grad_idx]\n",
    "\n",
    "    def print_results(self) -> None:\n",
    "        \"\"\"Print the results of the learning rate finder.\"\"\"\n",
    "        logger.info(\"\\nLearning Rate Finder Results:\")\n",
    "        logger.info(f\"Minimum loss: {min(self.losses):.4f}\")\n",
    "        logger.info(f\"Maximum loss: {max(self.losses):.4f}\")\n",
    "        logger.info(f\"Learning rate range: {min(self.lrs):.6f} to {max(self.lrs):.6f}\")\n",
    "\n",
    "def count_samples(dataset_path: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count the number of samples in each class of the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the dataset directory.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, int]: Dictionary with class names as keys and sample counts as values.\n",
    "    \"\"\"\n",
    "    class_counts = {}\n",
    "    try:\n",
    "        for class_name in os.listdir(dataset_path):\n",
    "            class_path = os.path.join(dataset_path, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                class_counts[class_name] = len(os.listdir(class_path))\n",
    "    except OSError as e:\n",
    "        raise DatasetError(f\"Error reading dataset directory: {e}\")\n",
    "    return class_counts\n",
    "\n",
    "def create_model(num_classes: int, architecture: callable, input_shape: Tuple[int, int, int]) -> Model:\n",
    "    \"\"\"\n",
    "    Create a transfer learning model based on the given architecture.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): Number of classes in the dataset.\n",
    "        architecture (callable): Base model architecture (e.g., ResNet50V2).\n",
    "        input_shape (Tuple[int, int, int]): Input shape for the model.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    base_model = architecture(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = CustomAugmentationLayer()(inputs)\n",
    "    x = base_model(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "def compile_model(model: Model, learning_rate: float) -> Model:\n",
    "    \"\"\"\n",
    "    Compile the model with the given learning rate.\n",
    "\n",
    "    Args:\n",
    "        model (Model): The model to compile.\n",
    "        learning_rate (float): The initial learning rate.\n",
    "\n",
    "    Returns:\n",
    "        Model: Compiled Keras model.\n",
    "    \"\"\"\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        learning_rate,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True)\n",
    "\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        steps_per_execution=8\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_data_generators(train_path: str, test_path: str, batch_size: int, target_size: Tuple[int, int], preprocessing_function: callable) -> Tuple[DataGenerator, DataGenerator, int, int]:\n",
    "    \"\"\"\n",
    "    Create data generators for training and testing.\n",
    "\n",
    "    Args:\n",
    "        train_path (str): Path to the training data.\n",
    "        test_path (str): Path to the test data.\n",
    "        batch_size (int): Batch size for training.\n",
    "        target_size (Tuple[int, int]): Target size for the input images.\n",
    "        preprocessing_function (callable): Preprocessing function for the images.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[DataGenerator, DataGenerator, int, int]: Train generator, test generator, steps per epoch, and validation steps.\n",
    "    \"\"\"\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocessing_function,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(preprocessing_function=preprocessing_function)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        test_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    steps_per_epoch = len(train_generator)\n",
    "    validation_steps = len(test_generator)\n",
    "\n",
    "    return DataGenerator(train_generator, steps_per_epoch), DataGenerator(test_generator, validation_steps), steps_per_epoch, validation_steps\n",
    "\n",
    "def lr_schedule(epoch: int, lr: float) -> float:\n",
    "    \"\"\"\n",
    "    Learning rate schedule function.\n",
    "\n",
    "    Args:\n",
    "        epoch (int): Current epoch number.\n",
    "        lr (float): Current learning rate.\n",
    "\n",
    "    Returns:\n",
    "        float: Updated learning rate.\n",
    "    \"\"\"\n",
    "    if epoch < 100:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "def validate_config(config: Dict[str, Any], valid_architectures: Dict[str, callable]) -> None:\n",
    "    \"\"\"\n",
    "    Validate the entire configuration.\n",
    "\n",
    "    Args:\n",
    "        config (Dict[str, Any]): The entire configuration dictionary.\n",
    "        valid_architectures (Dict[str, callable]): Dictionary of valid architecture names and their corresponding functions.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any configuration parameter is invalid.\n",
    "    \"\"\"\n",
    "    # Validate model configuration\n",
    "    validate_model_config(config.get('model', {}), valid_architectures)\n",
    "\n",
    "    # Validate training configuration\n",
    "    validate_training_config(config.get('training', {}))\n",
    "\n",
    "    # Validate data configuration\n",
    "    validate_data_config(config.get('data', {}))\n",
    "\n",
    "def validate_model_config(model_config: Dict[str, Any], valid_architectures: Dict[str, callable]) -> None:\n",
    "    \"\"\"Validate the model configuration.\"\"\"\n",
    "    if 'architecture' not in model_config:\n",
    "        raise ValueError(\"Model configuration must include 'architecture'\")\n",
    "    \n",
    "    architecture = model_config['architecture']\n",
    "    if architecture not in valid_architectures:\n",
    "        raise ValueError(f\"Invalid architecture: {architecture}. Valid options are: {', '.join(valid_architectures.keys())}\")\n",
    "\n",
    "def validate_training_config(training_config: Dict[str, Any]) -> None:\n",
    "    \"\"\"Validate the training configuration.\"\"\"\n",
    "    required_fields = ['batch_size', 'initial_learning_rate', 'epochs', 'early_stopping_patience', 'target_accuracy']\n",
    "    for field in required_fields:\n",
    "        if field not in training_config:\n",
    "            raise ValueError(f\"Training configuration must include '{field}'\")\n",
    "\n",
    "    if training_config['batch_size'] <= 0:\n",
    "        raise ValueError(f\"Invalid batch size: {training_config['batch_size']}. Must be a positive integer.\")\n",
    "    \n",
    "    if training_config['initial_learning_rate'] <= 0:\n",
    "        raise ValueError(f\"Invalid initial learning rate: {training_config['initial_learning_rate']}. Must be a positive float.\")\n",
    "    \n",
    "    if training_config['epochs'] <= 0:\n",
    "        raise ValueError(f\"Invalid number of epochs: {training_config['epochs']}. Must be a positive integer.\")\n",
    "    \n",
    "    if training_config['early_stopping_patience'] < 0:\n",
    "        raise ValueError(f\"Invalid early stopping patience: {training_config['early_stopping_patience']}. Must be a non-negative integer.\")\n",
    "    \n",
    "    if not 0 < training_config['target_accuracy'] <= 1:\n",
    "        raise ValueError(f\"Invalid target accuracy: {training_config['target_accuracy']}. Must be between 0 and 1.\")\n",
    "\n",
    "def validate_data_config(data_config: Dict[str, Any]) -> None:\n",
    "    \"\"\"Validate the data configuration.\"\"\"\n",
    "    required_fields = ['train_path', 'test_path']\n",
    "    for field in required_fields:\n",
    "        if field not in data_config:\n",
    "            raise ValueError(f\"Data configuration must include '{field}'\")\n",
    "\n",
    "    if not os.path.exists(data_config['train_path']):\n",
    "        raise ValueError(f\"Train path does not exist: {data_config['train_path']}\")\n",
    "    \n",
    "    if not os.path.exists(data_config['test_path']):\n",
    "        raise ValueError(f\"Test path does not exist: {data_config['test_path']}\")\n",
    "\n",
    "def main(find_lr: bool = True, pretrain_model_eval: bool = True):\n",
    "    \"\"\"\n",
    "    Main function to run the training pipeline.\n",
    "\n",
    "    Args:\n",
    "        find_lr (bool): Whether to find the optimal learning rate.\n",
    "        pretrain_model_eval (bool): Whether to evaluate the model before training.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
    "    except ImportError:\n",
    "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "        DATASET_PATH = os.getenv('DATASET_PATH')\n",
    "\n",
    "    ARCHITECTURES = {\n",
    "        'ResNet50V2': ResNet50V2,\n",
    "        'VGG16': VGG16,\n",
    "        'InceptionV3': InceptionV3,\n",
    "        'MobileNetV2': MobileNetV2,\n",
    "        'EfficientNetB0': EfficientNetB0\n",
    "    }\n",
    "\n",
    "    ARCHITECTURE_INPUT_SHAPES = {\n",
    "        'ResNet50V2': (128, 128, 3),\n",
    "        'VGG16': (224, 224, 3),\n",
    "        'InceptionV3': (299, 299, 3),\n",
    "        'MobileNetV2': (224, 224, 3),\n",
    "        'EfficientNetB0': (224, 224, 3)\n",
    "    }\n",
    "\n",
    "    PREPROCESSING_FUNCTIONS = {\n",
    "        'ResNet50V2': tf.keras.applications.resnet_v2.preprocess_input,\n",
    "        'VGG16': tf.keras.applications.vgg16.preprocess_input,\n",
    "        'InceptionV3': tf.keras.applications.inception_v3.preprocess_input,\n",
    "        'MobileNetV2': tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        'EfficientNetB0': tf.keras.applications.efficientnet.preprocess_input\n",
    "    }\n",
    "\n",
    "    # Load configuration\n",
    "    with open('config.yaml', 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "\n",
    "    # Validate the entire configuration\n",
    "    try:\n",
    "        validate_config(config, ARCHITECTURES)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Configuration error: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract configuration values\n",
    "    model_config = config['model']\n",
    "    training_config = config['training']\n",
    "    data_config = config['data']\n",
    "\n",
    "    architecture = ARCHITECTURES[model_config['architecture']]\n",
    "    input_shape = ARCHITECTURE_INPUT_SHAPES[architecture_name]\n",
    "    target_size = input_shape[:2]\n",
    "    preprocessing_function = PREPROCESSING_FUNCTIONS[architecture_name]\n",
    "\n",
    "    train_path = os.path.join(DATASET_PATH, data_config['train_path'])\n",
    "    test_path = os.path.join(DATASET_PATH, data_config['test_path']))\n",
    "        \n",
    "    try:\n",
    "        train_counts = count_samples(train_path)\n",
    "        test_counts = count_samples(test_path)\n",
    "    except DatasetError as e:\n",
    "        logger.error(f\"Error in dataset: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"TensorFlow version: {tf.__version__}\")\n",
    "    logger.info(f\"Eager execution: {tf.executing_eagerly()}\")\n",
    "\n",
    "    logger.info(\"\\nTraining samples per class:\")\n",
    "    for class_name, count in sorted(train_counts.items()):\n",
    "        logger.info(f\"{class_name}: {count}\")\n",
    "\n",
    "    logger.info(\"\\nTest samples per class:\")\n",
    "    for class_name, count in sorted(test_counts.items()):\n",
    "        logger.info(f\"{class_name}: {count}\")\n",
    "\n",
    "    num_classes = len(train_counts)\n",
    "    model = create_model(num_classes, architecture, input_shape)\n",
    "\n",
    "    logger.info(f\"Using {architecture.__name__} architecture\")\n",
    "    model.summary()\n",
    "\n",
    "    train_generator, test_generator, steps_per_epoch, validation_steps = create_data_generators(\n",
    "        train_path, test_path, batch_size=training_config.batch_size, target_size=target_size, preprocessing_function=preprocessing_function)\n",
    "\n",
    "    logger.info(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    logger.info(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "    compiled_model = compile_model(model, learning_rate=training_config.initial_learning_rate)\n",
    "\n",
    "    if find_lr:\n",
    "        logger.info(\"Finding optimal learning rate...\")\n",
    "        lr_finder = LearningRateFinder(compiled_model)\n",
    "        lr_finder.find(train_generator, start_lr=1e-7, end_lr=1, epochs=training_config.lr_epochs)\n",
    "        lr_finder.plot_loss()\n",
    "        lr_finder.print_results()\n",
    "        best_lr = lr_finder.get_best_lr()\n",
    "        logger.info(f\"Best initial learning rate: {best_lr}\")\n",
    "        logger.info(\"Recompiling model with best learning rate...\")\n",
    "        compiled_model = compile_model(model, learning_rate=best_lr)\n",
    "    else:\n",
    "        best_lr = training_config.initial_learning_rate\n",
    "\n",
    "    accuracy_callback = AccuracyCallback(target_accuracy=training_config.target_accuracy)\n",
    "    debug_callback = DebugCallback()\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=training_config.early_stopping_patience, restore_best_weights=True)\n",
    "    dataset_logger = DatasetLogger(train_generator, test_generator)\n",
    "    custom_validation = CustomValidationCallback(test_generator, validation_steps)\n",
    "    augmentation_callback = AugmentationCallback(train_generator)\n",
    "    lr_callback = LearningRateScheduler(lr_schedule)\n",
    "    lr_logger = LRLogger()\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_generator.generator.classes), y=train_generator.generator.classes)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "    if pretrain_model_eval:\n",
    "        logger.info(\"Evaluating model before training:\")\n",
    "        evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "        logger.info(f\"Initial evaluation: {evaluation}\")\n",
    "\n",
    "    try:\n",
    "        history = compiled_model.fit(\n",
    "            train_generator.generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=training_config.epochs,\n",
    "            validation_data=test_generator.generator,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[\n",
    "                accuracy_callback,\n",
    "                debug_callback,\n",
    "                early_stopping,\n",
    "                dataset_logger,\n",
    "                custom_validation,\n",
    "                augmentation_callback,\n",
    "                lr_callback,\n",
    "                lr_logger\n",
    "            ],\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    except tf.errors.ResourceExhaustedError:\n",
    "        logger.error(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during training: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "\n",
    "    logger.info(\"Final evaluation:\")\n",
    "    final_evaluation = compiled_model.evaluate(test_generator.generator, steps=validation_steps)\n",
    "    logger.info(f\"Final evaluation: {final_evaluation}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uN8jUQZ81RW"
   },
   "source": [
    "#### <a id='toc1_4_2_4_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSeZmV4YfVVE"
   },
   "source": [
    "#### <a id='toc1_4_2_4_'></a>[**Test Trained Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj8Z1KnIXxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0LDLxwLXxTP"
   },
   "source": [
    "#### <a id='toc1_4_2_3_'></a>[**Next Section**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_W-u9ykXxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVEV7tJUXxTP"
   },
   "source": [
    "### <a id='toc1_4_3_'></a>[**4.3. Forecasting using machine learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-haQ_GrgXxTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYVotBmAXxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-kiMSDOXxTP"
   },
   "source": [
    "#### <a id='toc1_4_3_1_'></a>[**4.3.1. Forecasting using machine learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljJ1lXnjXxTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPn8L_FCXxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUgFWSFaXxTP"
   },
   "source": [
    "##### <a id='toc1_4_3_1_1_'></a>[**4.3.1.1. Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hh7GUi1iXxTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGTR0nX0XxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfp7HDhGXxTP"
   },
   "source": [
    "##### <a id='toc1_4_3_1_2_'></a>[**4.3.1.2. Use the data from the last six months as the testing data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwFM7b7PXxTP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLE_lgmnXxTP"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYuhiqJiXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_3_1_3_'></a>[**4.3.1.3. Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDxCuQUOXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UBJhwxNXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8iG83ylXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_3_1_4_'></a>[**4.3.1.4. Use the best-performing models to make a forecast for the next year**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFFMhnlGXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lGM473tXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F27ebfAeXxTQ"
   },
   "source": [
    "### <a id='toc1_4_4_'></a>[**4.4. Forecasting using deep learning algorithms**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyYdRh5KXxTQ"
   },
   "source": [
    "#### <a id='toc1_4_4_1_'></a>[**4.4.1. Use sales amount for predictions instead of item count**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNWJmttFXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OktA8CEIXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BC0bbivZXxTQ"
   },
   "source": [
    "#### <a id='toc1_4_4_2_'></a>[**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szr8uZOEXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNjnXH0ZXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsD-rQ5WXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_4_2_1_'></a>[**4.4.2.1. Define the train and test series**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1mnT3mKXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrnsDqbdXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCfA6lZjXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_4_2_2_'></a>[**4.4.2.2. Generate synthetic data for the last 12 months**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsGevcLfXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFFdG2BKXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh96jqiUXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_4_2_3_'></a>[**4.4.2.3. Build and train an LSTM model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0jK6AQvXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vs_0Kbv3XxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPlmLTmeXxTQ"
   },
   "source": [
    "##### <a id='toc1_4_4_2_4_'></a>[**4.4.2.4. Use the model to make predictions for the test data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMLYgzkWXxTQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9qMv2KbXxTQ"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_j7MZqIXxTQ"
   },
   "source": [
    "#### <a id='toc1_4_4_3_'></a>[**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Za61_FYXxTR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHrfqEDlXxTR"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZr4879oXxTR"
   },
   "source": [
    "#### <a id='toc1_4_4_4_'></a>[**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEXfhjpUXxTR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN9oncYmXxTR"
   },
   "source": [
    "**Explanations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Why It Is Important:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- [Placeholder for observations after running the code]\n",
    "\n",
    "**Conclusions:**\n",
    "\n",
    "- [Placeholder for conclusions based on initial data view]\n",
    "\n",
    "**Recommendations:**\n",
    "\n",
    "- [Placeholder for recommendations based on initial data examination]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
