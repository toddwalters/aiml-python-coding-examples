{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced AI-Powered HR Assistant for Nestle\n",
    "\n",
    "## Nestlé HR Policy Chatbot Overview\n",
    "\n",
    "This Jupyter Notebook implements a conversational chatbot designed to answer user queries based on information contained within Nestlé's HR policy documents. It leverages several powerful technologies from the field of Natural Language Processing (NLP) and Large Language Models (LLMs) to achieve this:\n",
    "\n",
    "1.  **Document Loading and Chunking:** The notebook begins by loading the HR policy document (in PDF format) and splitting it into smaller, manageable text chunks. This is crucial because LLMs have limitations on the amount of text they can process at once.\n",
    "\n",
    "2.  **Text Embeddings:** Each text chunk is then converted into a numerical vector representation called an \"embedding.\" These embeddings capture the semantic meaning of the text, allowing the system to understand the relationships between different pieces of information. OpenAI's embedding models are used for this purpose.\n",
    "\n",
    "3.  **Vector Database:** The embeddings are stored in a vector database (Chroma), which allows for efficient similarity search. This means that when a user asks a question, the system can quickly find the most relevant text chunks from the HR policy.\n",
    "\n",
    "4.  **Question Answering with LLM:** The most relevant text chunks are then passed to a large language model (OpenAI's GPT model) along with the user's question. The LLM uses this context to generate a coherent and informative answer.\n",
    "\n",
    "5.  **User Interface:** Finally, a user-friendly interface is created using Gradio, allowing users to easily interact with the chatbot.\n",
    "\n",
    "In summary, this notebook demonstrates a complete workflow for building a question-answering system over a PDF document using state-of-the-art NLP and LLM techniques. This approach can be generalized to other document types and domains, making it a valuable tool for information retrieval and knowledge management. The notebook is structured with Markdown explanations and code blocks separated by functionality, making it easy to follow and understand the implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Setup Guide\n",
    "\n",
    "### Prerequisites:\n",
    "1. **Environment Setup:**\n",
    "   - Python 3.8 or higher\n",
    "   - Required packages installed\n",
    "   - .env file configured\n",
    "\n",
    "2. **Required Environment Variables:**\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_api_key_here\n",
    "   PDF_DOC_PATH=path_to_your_pdf_file\n",
    "   ```\n",
    "\n",
    "### Running the Application:\n",
    "1. Ensure all cells are run in order\n",
    "2. Wait for the Gradio interface to launch\n",
    "3. Interface will be available at http://127.0.0.1:7860\n",
    "\n",
    "### Features:\n",
    "- Real-time question answering\n",
    "- Conversation history tracking\n",
    "- Error handling and recovery\n",
    "- Rate limiting protection\n",
    "\n",
    "### Best Practices:\n",
    "- Keep API keys secure\n",
    "- Monitor usage rates\n",
    "- Regular logging review\n",
    "- Backup document sources\n",
    "\n",
    "### Troubleshooting:\n",
    "- Check environment variables\n",
    "- Verify PDF file accessibility\n",
    "- Monitor API rate limits\n",
    "- Review error logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of Libraries\n",
    "\n",
    "This code block uses `pip` to install the necessary Python libraries. These libraries are essential for the chatbot's functionality:\n",
    "\n",
    "- `openai`: For interacting with OpenAI's API services and models\n",
    "- `langchain`: Core framework for building applications with language models\n",
    "- `langchain-community`: Community-maintained integrations and tools for LangChain\n",
    "- `langchain-openai`: Official OpenAI integration package for LangChain\n",
    "- `chromadb`: Vector database for efficient storage and retrieval of text embeddings\n",
    "- `pypdf`: PDF document loading and processing utility\n",
    "- `gradio`: Framework for creating user-friendly web interfaces\n",
    "- `tiktoken`: OpenAI's tokenizer for accurate token counting and management\n",
    "- `python-dotenv`: For secure environment variable management\n",
    "- `psutil`: For system resource monitoring and management\n",
    "- `pathlib`: For cross-platform file path handling\n",
    "\n",
    "**Required Additional Packages:**\n",
    "- Standard library packages: `logging`, `json`, `datetime`, `collections`\n",
    "- System utilities: `platform`, `sys`\n",
    "\n",
    "**Installation Command:**\n",
    "```bash\n",
    "pip install openai langchain langchain-community langchain-openai chromadb pypdf gradio tiktoken python-dotenv psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this in your notebook environment if needed)\n",
    "# !pip install openai langchain langchain-community langchain-openai chromadb pypdf gradio tiktoken python-dotenv psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced AI-Powered HR Assistant for Nestle\n",
    "## Core Imports and Error Handling Classes\n",
    "\n",
    "This section establishes the foundation of our application with comprehensive imports and error handling. Let's examine each component:\n",
    "\n",
    "### Import Section Breakdown:\n",
    "- **Standard Library Imports:**\n",
    "  - `os`: File and environment variable operations\n",
    "  - `logging`: Application-wide logging system\n",
    "  - `typing`: Type hints for better code documentation\n",
    "  - `datetime`, `time`: Timestamp and performance tracking\n",
    "  - `collections.defaultdict`: Used in rate limiting implementation\n",
    "  - `json`: For error and metrics serialization\n",
    "\n",
    "- **External Dependencies:**\n",
    "  - `openai`: OpenAI API interface\n",
    "  - `dotenv`: Secure environment variable management\n",
    "  - `langchain` components: Document processing and AI operations\n",
    "  - `gradio`: Web interface creation\n",
    "\n",
    "### Logging Configuration:\n",
    "- **Format Details:**\n",
    "  - Timestamp: When the log entry occurred\n",
    "  - Logger name: Source of the log entry\n",
    "  - Log level: Severity of the entry\n",
    "  - File and line: Precise location in code\n",
    "  - Message: Actual log content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/1720690009_20241228_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import platform\n",
    "import psutil\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Core dependencies\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Document processing\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('chatbot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Configuration Classes\n",
    "\n",
    "This section defines our custom error hierarchy and configuration management. Let's examine each component:\n",
    "\n",
    "### Custom Error Classes:\n",
    "1. **ChatbotError (Base Class):**\n",
    "   - Custom initialization with message and error code\n",
    "   - Timestamp tracking for error occurrence\n",
    "   - JSON serialization for logging\n",
    "   - Error code system for categorization\n",
    "\n",
    "2. **Specialized Error Classes:**\n",
    "   - `DocumentLoadError`: Document-specific issues\n",
    "   - `ModelError`: AI model operation failures\n",
    "   - `ConfigurationError`: Setup and config problems\n",
    "\n",
    "### Key Features:\n",
    "- Every error includes:\n",
    "  - Timestamp of occurrence\n",
    "  - Error code for categorization\n",
    "  - Detailed message\n",
    "  - JSON serialization for logging\n",
    "  - Error type identification\n",
    "\n",
    "### Configuration Class Features:\n",
    "\n",
    "1. **Config Class Structure:**\n",
    "  - Uses class attributes for configuration settings\n",
    "  - All settings are centralized in one location\n",
    "  - Makes it easy to modify system-wide settings\n",
    "\n",
    "2. **Configuration Parameters:**\n",
    "  - `CHUNK_SIZE`: Controls the size of text segments (1000 characters)\n",
    "  - `CHUNK_OVERLAP`: Amount of overlap between chunks (0 characters)\n",
    "  - `MODEL_TEMPERATURE`: Controls AI response randomness (0 for consistent responses)\n",
    "  - `EMBEDDING_MODEL`: Specifies which OpenAI embedding model to use\n",
    "\n",
    "3. **get_settings Method:**\n",
    "  - Returns all configuration as a dictionary\n",
    "  - Filters out private attributes (starting with '_')\n",
    "  - Useful for logging and debugging\n",
    "\n",
    "4. **Default Configuration:**\n",
    "   - Predefined settings with sensible defaults\n",
    "   - Type-safe configuration values\n",
    "   - Centralized configuration management\n",
    "\n",
    "5. **Environment Integration:**\n",
    "   - Loads settings from environment variables\n",
    "   - Type conversion for numeric values\n",
    "   - Configuration validation\n",
    "\n",
    "6. **Access Methods:**\n",
    "   - Safe configuration retrieval\n",
    "   - Error handling for missing keys\n",
    "   - Configuration state tracking\n",
    "\n",
    "### Environment Loading:\n",
    "1. **Variable Validation:**\n",
    "   - Required variable checking\n",
    "   - Type validation\n",
    "   - Path existence verification\n",
    "\n",
    "2. **Security Features:**\n",
    "   - Secure API key handling\n",
    "   - Path validation and sanitization\n",
    "   - Error handling for missing/invalid values\n",
    "\n",
    "### Metrics Tracking:\n",
    "1. **Performance Metrics:**\n",
    "   - Operation duration tracking\n",
    "   - Error frequency monitoring\n",
    "   - Statistical aggregation\n",
    "\n",
    "2. **Error Tracking:**\n",
    "   - Error type categorization\n",
    "   - Timestamp recording\n",
    "   - Error frequency analysis\n",
    "\n",
    "### Key Components:\n",
    "- Configuration versioning\n",
    "- Load time tracking\n",
    "- Type validation\n",
    "- Secure variable handling\n",
    "- Comprehensive error tracking\n",
    "- Statistical analysis tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotError(Exception):\n",
    "    \"\"\"Base exception class for chatbot errors\"\"\"\n",
    "    def __init__(self, message: str = None, error_code: str = None):\n",
    "        self.message = message or \"An unexpected error occurred in the chatbot\"\n",
    "        self.error_code = error_code or \"CHATBOT_ERROR\"\n",
    "        self.timestamp = datetime.now()\n",
    "        \n",
    "        formatted_message = f\"[{self.error_code}] {self.timestamp}: {self.message}\"\n",
    "        super().__init__(formatted_message)\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert error details to dictionary for logging or API responses\"\"\"\n",
    "        return {\n",
    "            \"error_code\": self.error_code,\n",
    "            \"message\": self.message,\n",
    "            \"timestamp\": self.timestamp.isoformat(),\n",
    "            \"error_type\": self.__class__.__name__\n",
    "        }\n",
    "    \n",
    "    def log_error(self, log_level: int = logging.ERROR):\n",
    "        \"\"\"Log the error with specified logging level\"\"\"\n",
    "        logger.log(log_level, json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "class ConfigurationError(ChatbotError):\n",
    "    \"\"\"Raised when there are configuration-related issues\"\"\"\n",
    "    def __init__(self, message: str = None, config_key: str = None):\n",
    "        self.config_key = config_key\n",
    "        error_message = f\"Configuration error{f' for {config_key}' if config_key else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"CONFIG_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add configuration-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict[\"config_key\"] = self.config_key\n",
    "        return error_dict\n",
    "\n",
    "class DocumentLoadError(ChatbotError):\n",
    "    \"\"\"Raised when there are issues loading documents\"\"\"\n",
    "    def __init__(self, message: str = None, file_path: str = None):\n",
    "        self.file_path = file_path\n",
    "        error_message = f\"Failed to load document{f' {file_path}' if file_path else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"DOC_LOAD_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add document-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict[\"file_path\"] = self.file_path\n",
    "        return error_dict\n",
    "\n",
    "class ModelError(ChatbotError):\n",
    "    \"\"\"Raised when there are issues with the AI model\"\"\"\n",
    "    def __init__(self, message: str = None, model_name: str = None, query: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.query = query\n",
    "        error_message = f\"Model error{f' for {model_name}' if model_name else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"MODEL_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add model-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict.update({\n",
    "            \"model_name\": self.model_name,\n",
    "            \"query\": self.query if self.query else None\n",
    "        })\n",
    "        return error_dict\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the chatbot\"\"\"\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 0\n",
    "    MODEL_TEMPERATURE = 0\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_settings(cls) -> Dict[str, Any]:\n",
    "        \"\"\"Returns all configuration settings as a dictionary\"\"\"\n",
    "        return {k: v for k, v in cls.__dict__.items() \n",
    "                if not k.startswith('_')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and API Key Management\n",
    "\n",
    "This section handles the secure loading and validation of environment variables. Let's examine the components:\n",
    "\n",
    "### load_env Function:\n",
    "- **Purpose:**\n",
    "  - Safely loads environment variables from a .env file\n",
    "  - Validates that all required variables are present\n",
    "  - Returns API key and document path as a tuple\n",
    "\n",
    "### Implementation Details:\n",
    "- **load_dotenv:**\n",
    "  - Loads environment variables from .env file\n",
    "  - `verbose=True` enables logging of the loading process\n",
    "\n",
    "- **Required Variables Dictionary:**\n",
    "  - Maps environment variable names to human-readable descriptions\n",
    "  - Makes error messages more user-friendly\n",
    "  - Currently checks for:\n",
    "    * OPENAI_API_KEY: Required for API authentication\n",
    "    * PDF_DOC_PATH: Path to the document being processed\n",
    "\n",
    "- **Validation Process:**\n",
    "  - Iterates through required variables\n",
    "  - Checks if each variable exists and has a value\n",
    "  - Raises ValueError with specific message if any are missing\n",
    "\n",
    "- **Logging:**\n",
    "  - Success message logged when all variables are loaded\n",
    "  - Helps with debugging environment setup issues\n",
    "\n",
    "### Error Handling:\n",
    "- Raises ValueError if required variables are missing\n",
    "- Includes specific variable name in error message\n",
    "- Helps quickly identify configuration issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration settings for the chatbot\"\"\"\n",
    "    \n",
    "    # Default configuration values\n",
    "    DEFAULT_CONFIG = {\n",
    "        'CHUNK_SIZE': 1000,\n",
    "        'CHUNK_OVERLAP': 0,\n",
    "        'MODEL_TEMPERATURE': 0,\n",
    "        'EMBEDDING_MODEL': \"text-embedding-ada-002\",\n",
    "        'MAX_RETRIES': 3,\n",
    "        'RATE_LIMIT_PER_SECOND': 5,\n",
    "        'LOG_LEVEL': logging.INFO\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._config = self.DEFAULT_CONFIG.copy()\n",
    "        self._load_time = datetime.now()\n",
    "        \n",
    "    def load_from_env(self) -> None:\n",
    "        \"\"\"Load configuration from environment variables\"\"\"\n",
    "        try:\n",
    "            for key in self.DEFAULT_CONFIG:\n",
    "                env_value = os.getenv(f'CHATBOT_{key}')\n",
    "                if env_value:\n",
    "                    # Convert string values to appropriate types\n",
    "                    if isinstance(self.DEFAULT_CONFIG[key], int):\n",
    "                        self._config[key] = int(env_value)\n",
    "                    elif isinstance(self.DEFAULT_CONFIG[key], float):\n",
    "                        self._config[key] = float(env_value)\n",
    "                    else:\n",
    "                        self._config[key] = env_value\n",
    "                        \n",
    "            logger.info(\"Configuration loaded successfully\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Invalid configuration value: {str(e)}\",\n",
    "                config_key=key\n",
    "            )\n",
    "    \n",
    "    def get(self, key: str) -> Any:\n",
    "        \"\"\"Get configuration value\"\"\"\n",
    "        if key not in self._config:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Configuration key not found: {key}\",\n",
    "                config_key=key\n",
    "            )\n",
    "        return self._config[key]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert configuration to dictionary\"\"\"\n",
    "        return {\n",
    "            'config': self._config,\n",
    "            'load_time': self._load_time.isoformat(),\n",
    "            'age_seconds': (datetime.now() - self._load_time).total_seconds()\n",
    "        }\n",
    "\n",
    "def load_env() -> tuple:\n",
    "    \"\"\"\n",
    "    Load and validate environment variables\n",
    "    Returns:\n",
    "        tuple: (api_key, pdf_doc_path)\n",
    "    Raises:\n",
    "        ConfigurationError: If required environment variables are missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        load_dotenv(verbose=True)\n",
    "        \n",
    "        required_vars = {\n",
    "            'OPENAI_API_KEY': 'OpenAI API key',\n",
    "            'PDF_DOC_PATH': 'PDF document path'\n",
    "        }\n",
    "        \n",
    "        env_vars = {}\n",
    "        for var, description in required_vars.items():\n",
    "            value = os.getenv(var)\n",
    "            if not value:\n",
    "                raise ConfigurationError(\n",
    "                    message=f\"Missing {description}\",\n",
    "                    config_key=var\n",
    "                )\n",
    "            env_vars[var] = value\n",
    "            \n",
    "        # Validate PDF path exists\n",
    "        pdf_path = Path(env_vars['PDF_DOC_PATH'])\n",
    "        if not pdf_path.exists():\n",
    "            raise ConfigurationError(\n",
    "                message=f\"PDF file does not exist: {pdf_path}\",\n",
    "                config_key='PDF_DOC_PATH'\n",
    "            )\n",
    "        \n",
    "        logger.info(\"Environment variables loaded successfully\")\n",
    "        return env_vars['OPENAI_API_KEY'], str(pdf_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ConfigurationError(\n",
    "                message=f\"Environment loading failed: {str(e)}\"\n",
    "            )\n",
    "        e.log_error()\n",
    "        raise\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Track performance metrics for the chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "    def record_duration(self, operation: str, duration: float):\n",
    "        \"\"\"Record duration of an operation\"\"\"\n",
    "        self.metrics[f\"{operation}_duration\"].append(duration)\n",
    "        logger.debug(f\"{operation} took {duration:.2f} seconds\")\n",
    "    \n",
    "    def record_error(self, error: ChatbotError):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.metrics[\"errors\"].append(error.to_dict())\n",
    "        error.log_error()\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistical summary of metrics\"\"\"\n",
    "        stats = {}\n",
    "        for metric, values in self.metrics.items():\n",
    "            if metric.endswith('_duration'):\n",
    "                if values:\n",
    "                    stats[metric] = {\n",
    "                        'mean': sum(values) / len(values),\n",
    "                        'min': min(values),\n",
    "                        'max': max(values),\n",
    "                        'count': len(values)\n",
    "                    }\n",
    "            elif metric == \"errors\":\n",
    "                stats[\"error_count\"] = len(values)\n",
    "                stats[\"error_types\"] = defaultdict(int)\n",
    "                for error in values:\n",
    "                    stats[\"error_types\"][error[\"error_type\"]] += 1\n",
    "                    \n",
    "        return stats\n",
    "\n",
    "# Global instances\n",
    "config = Config()\n",
    "metrics = MetricsTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing and Vector Store Creation\n",
    "\n",
    "This section implements the text processing pipeline, converting documents into searchable vector embeddings. Let's break down the components:\n",
    "\n",
    "### TextProcessor Class:\n",
    "- **Purpose:**\n",
    "  - Handles all text processing operations\n",
    "  - Creates and manages embeddings\n",
    "  - Implements caching for performance optimization\n",
    "\n",
    "### Class Components:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Creates text splitter with configured chunk size and overlap\n",
    "   - Initializes OpenAI embeddings client\n",
    "   - Sets up caching dictionary for frequently accessed embeddings\n",
    "\n",
    "2. **Text Splitting Configuration:**\n",
    "   - Uses CharacterTextSplitter for consistent chunk sizes\n",
    "   - Chunk size and overlap from Config class\n",
    "   - Helps manage token limits for AI models\n",
    "\n",
    "3. **Embedding Creation:**\n",
    "   - Uses OpenAI's embedding model\n",
    "   - Converts text chunks to numerical vectors\n",
    "   - Enables semantic search capabilities\n",
    "\n",
    "4. **Vector Store:**\n",
    "   - Creates Chroma database from documents\n",
    "   - Stores embeddings for efficient retrieval\n",
    "   - Enables similarity search operations\n",
    "\n",
    "### Error Handling:\n",
    "- Load time measurement\n",
    "- Page count logging\n",
    "- Success/failure metrics\n",
    "- Duration tracking\n",
    "\n",
    "\n",
    "### Performance Features:\n",
    "- Caching mechanism for repeated queries\n",
    "- Batch processing of documents\n",
    "- Efficient vector storage and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Load and process document based on file type with enhanced error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the document\n",
    "    Returns:\n",
    "        List: Loaded document pages\n",
    "    Raises:\n",
    "        DocumentLoadError: If document loading fails\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    try:\n",
    "        logger.info(f\"Starting document load: {file_path}\")\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # Validate file existence and permissions\n",
    "        if not file_path.exists():\n",
    "            raise DocumentLoadError(\n",
    "                message=\"File does not exist\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        if not file_path.is_file():\n",
    "            raise DocumentLoadError(\n",
    "                message=\"Path is not a file\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "            \n",
    "        # Select appropriate loader based on file extension\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "        else:\n",
    "            raise DocumentLoadError(\n",
    "                message=f\"Unsupported file type: {file_path.suffix}\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        \n",
    "        # Load document with retries\n",
    "        max_retries = config.get('MAX_RETRIES')\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Log success metrics\n",
    "                duration = time() - start_time\n",
    "                metrics.record_duration('document_load', duration)\n",
    "                logger.info(\n",
    "                    f\"Successfully loaded document with {len(documents)} pages in {duration:.2f} seconds\"\n",
    "                )\n",
    "                return documents\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.warning(f\"Retry {attempt + 1}/{max_retries} failed: {str(e)}\")\n",
    "                    continue\n",
    "                raise DocumentLoadError(\n",
    "                    message=f\"Failed after {max_retries} attempts: {str(e)}\",\n",
    "                    file_path=str(file_path)\n",
    "                )\n",
    "                \n",
    "    except Exception as e:\n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = DocumentLoadError(\n",
    "                message=str(e),\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        metrics.record_error(e)\n",
    "        raise\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Handles text splitting and embedding creation with enhanced error handling and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.text_splitter = CharacterTextSplitter(\n",
    "                chunk_size=config.get('CHUNK_SIZE'),\n",
    "                chunk_overlap=config.get('CHUNK_OVERLAP')\n",
    "            )\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            self._cache = {}\n",
    "            self._stats = defaultdict(int)\n",
    "            logger.info(\"TextProcessor initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Failed to initialize TextProcessor: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _split_documents(self, documents: List) -> List:\n",
    "        \"\"\"\n",
    "        Split documents into chunks with error handling\n",
    "        \n",
    "        Args:\n",
    "            documents (List): List of documents to split\n",
    "        Returns:\n",
    "            List: Split text chunks\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            texts = self.text_splitter.split_documents(documents)\n",
    "            duration = time() - start_time\n",
    "            metrics.record_duration('text_splitting', duration)\n",
    "            self._stats['total_chunks'] = len(texts)\n",
    "            logger.info(f\"Split documents into {len(texts)} chunks in {duration:.2f} seconds\")\n",
    "            return texts\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = ModelError(\n",
    "                message=f\"Failed to split documents: {str(e)}\",\n",
    "                model_name=\"text_splitter\"\n",
    "            )\n",
    "            metrics.record_error(error)\n",
    "            raise error\n",
    "    \n",
    "    def process_documents(self, documents: List) -> Chroma:\n",
    "        \"\"\"\n",
    "        Process documents into embeddings and store in Chroma\n",
    "        \n",
    "        Args:\n",
    "            documents (List): List of document pages\n",
    "        Returns:\n",
    "            Chroma: Vector store with processed documents\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            logger.info(\"Starting document processing\")\n",
    "            \n",
    "            # Split documents\n",
    "            texts = self._split_documents(documents)\n",
    "            \n",
    "            # Create vector store\n",
    "            logger.info(\"Creating vector store\")\n",
    "            db = Chroma.from_documents(texts, self.embeddings)\n",
    "            \n",
    "            # Record success metrics\n",
    "            duration = time() - start_time\n",
    "            metrics.record_duration('document_processing', duration)\n",
    "            self._stats['processing_time'] = duration\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Successfully created vector store in {duration:.2f} seconds. \"\n",
    "                f\"Stats: {json.dumps(self._stats, indent=2)}\"\n",
    "            )\n",
    "            return db\n",
    "            \n",
    "        except Exception as e:\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                e = ModelError(\n",
    "                    message=f\"Failed to process documents: {str(e)}\",\n",
    "                    model_name=\"document_processor\"\n",
    "                )\n",
    "            metrics.record_error(e)\n",
    "            raise e\n",
    "            \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        return {\n",
    "            'stats': self._stats,\n",
    "            'cache_size': len(self._cache),\n",
    "            'embedding_model': config.get('EMBEDDING_MODEL')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Limiting and Question-Answering System\n",
    "\n",
    "This section implements the core QA functionality with rate limiting protection. Let's examine each component:\n",
    "\n",
    "### Rate Limiting Decorator:\n",
    "- **Purpose:**\n",
    "  - Prevents API abuse\n",
    "  - Manages resource utilization\n",
    "  - Ensures fair system usage\n",
    "\n",
    "- **Implementation Details:**\n",
    "  - Uses defaultdict to track function calls\n",
    "  - Maintains timestamp history for each function\n",
    "  - Enforces maximum calls per second limit\n",
    "\n",
    "- **Features:**\n",
    "  - Rolling window implementation\n",
    "  - Function-specific limiting\n",
    "  - Automatic cleanup of old timestamps\n",
    "\n",
    "### QASystem Class:\n",
    "- **Purpose:**\n",
    "  - Manages question-answering functionality\n",
    "  - Handles interaction with OpenAI's API\n",
    "  - Processes user queries\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Model configuration\n",
    "   - Retriever setup\n",
    "   - History tracking\n",
    "   - Performance monitoring\n",
    "\n",
    "2. **Query Processing:**\n",
    "   - Input validation\n",
    "   - Context retrieval\n",
    "   - Response generation\n",
    "   - Performance tracking\n",
    "\n",
    "3. **History Management:**\n",
    "   - Query logging\n",
    "   - Response tracking\n",
    "   - Duration monitoring\n",
    "   - Pattern analysis\n",
    "\n",
    "### Class Components:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Sets up RetrievalQA chain\n",
    "   - Configures OpenAI model parameters\n",
    "   - Connects to vector store retriever\n",
    "\n",
    "2. **Query Processing:**\n",
    "   - Rate-limited query handling\n",
    "   - Performance timing for each query\n",
    "   - Comprehensive error handling\n",
    "\n",
    "3. **Response Generation:**\n",
    "   - Uses LangChain's chain mechanism\n",
    "   - Retrieves relevant context\n",
    "   - Generates coherent responses\n",
    "\n",
    "### Performance Monitoring:\n",
    "- Query execution time tracking\n",
    "- Detailed logging of operations\n",
    "- Error tracking and reporting\n",
    "\n",
    "### Error Handling:\n",
    "- Custom error types for different failures\n",
    "- Detailed error messages\n",
    "- Logging of all operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limit(max_per_second: int):\n",
    "    \"\"\"\n",
    "    Enhanced rate limiting decorator with metrics tracking\n",
    "    \n",
    "    Args:\n",
    "        max_per_second (int): Maximum number of calls allowed per second\n",
    "    \"\"\"\n",
    "    calls = defaultdict(list)\n",
    "    \n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time()\n",
    "            func_name = func.__name__\n",
    "            \n",
    "            # Clean old calls\n",
    "            calls[func_name] = [c for c in calls[func_name] if c > now - 1]\n",
    "            \n",
    "            # Check rate limit\n",
    "            if len(calls[func_name]) >= max_per_second:\n",
    "                error = ChatbotError(\n",
    "                    message=f\"Rate limit of {max_per_second} calls per second exceeded for {func_name}\",\n",
    "                    error_code=\"RATE_LIMIT_ERROR\"\n",
    "                )\n",
    "                metrics.record_error(error)\n",
    "                raise error\n",
    "                \n",
    "            # Record call\n",
    "            calls[func_name].append(now)\n",
    "            \n",
    "            # Execute function\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                metrics.record_duration(func_name, time() - now)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                if not isinstance(e, ChatbotError):\n",
    "                    e = ModelError(\n",
    "                        message=str(e),\n",
    "                        model_name=func_name\n",
    "                    )\n",
    "                metrics.record_error(e)\n",
    "                raise e\n",
    "                \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class QASystem:\n",
    "    \"\"\"Enhanced question-answering system with metrics and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: Chroma):\n",
    "        \"\"\"Initialize QA system with vector store\"\"\"\n",
    "        try:\n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=OpenAI(temperature=config.get('MODEL_TEMPERATURE')),\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=vector_store.as_retriever()\n",
    "            )\n",
    "            self.query_history = []\n",
    "            logger.info(\"QA system initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = ModelError(\n",
    "                message=f\"Failed to initialize QA system: {str(e)}\",\n",
    "                model_name=\"QASystem\"\n",
    "            )\n",
    "            metrics.record_error(error)\n",
    "            raise error\n",
    "    \n",
    "    @rate_limit(max_per_second=5)\n",
    "    def get_response(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Get response for user query with enhanced error handling\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question\n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            # Validate query\n",
    "            if not query or not query.strip():\n",
    "                raise ModelError(\n",
    "                    message=\"Empty query provided\",\n",
    "                    query=query\n",
    "                )\n",
    "            \n",
    "            # Process query\n",
    "            response = self.qa_chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Record successful query\n",
    "            duration = time() - start_time\n",
    "            self.query_history.append({\n",
    "                'query': query,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Query processed successfully in {duration:.2f} seconds\")\n",
    "            return response['result'] if isinstance(response, dict) else response\n",
    "            \n",
    "        except Exception as e:\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                e = ModelError(\n",
    "                    message=f\"Failed to process query: {str(e)}\",\n",
    "                    model_name=\"QASystem\",\n",
    "                    query=query\n",
    "                )\n",
    "            metrics.record_error(e)\n",
    "            raise e\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get QA system statistics\"\"\"\n",
    "        return {\n",
    "            'total_queries': len(self.query_history),\n",
    "            'average_duration': sum(q['duration'] for q in self.query_history) / len(self.query_history) if self.query_history else 0,\n",
    "            'query_history': self.query_history[-10:]  # Last 10 queries\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface Implementation with Gradio\n",
    "\n",
    "This section creates an interactive web interface using Gradio. Let's examine the components in detail:\n",
    "\n",
    "### ChatInterface Class:\n",
    "- **Purpose:**\n",
    "  - Creates and manages the web-based user interface\n",
    "  - Handles conversation state management\n",
    "  - Processes user interactions\n",
    "\n",
    "### Class Components:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Stores reference to QA system\n",
    "   - Initializes empty conversation history list\n",
    "   - Sets up interface state management\n",
    "\n",
    "2. **Response Handler (respond method):**\n",
    "   - **Functionality:**\n",
    "     * Processes incoming user messages\n",
    "     * Manages conversation history\n",
    "     * Handles error states\n",
    "   \n",
    "   - **Parameters:**\n",
    "     * message: The user's input text\n",
    "     * history: Current conversation history\n",
    "   \n",
    "   - **Return Values:**\n",
    "     * Tuple containing (response, updated_history)\n",
    "     * Formats error messages for display if needed\n",
    "\n",
    "3. **Interface Creation (create_interface method):**\n",
    "   - **Components:**\n",
    "     * Chatbot display area for conversation history\n",
    "     * Text input box for user questions\n",
    "     * Clear button for resetting conversation\n",
    "   \n",
    "   - **Event Handlers:**\n",
    "     * Submit: Processes user input and updates display\n",
    "     * Clear: Resets conversation state\n",
    "   \n",
    "   - **Layout:**\n",
    "     * Clean, intuitive design\n",
    "     * Clear labels and placeholders\n",
    "     * Responsive interface elements\n",
    "\n",
    "### Error Handling:\n",
    "- Graceful error message display\n",
    "- Maintains conversation state during errors\n",
    "- Prevents interface crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    \"\"\"Enhanced chat interface with error handling and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_system: QASystem):\n",
    "        self.qa_system = qa_system\n",
    "        self.conversation_history = []\n",
    "        self.interface_metrics = {\n",
    "            'total_interactions': 0,\n",
    "            'error_count': 0,\n",
    "            'start_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def respond(self, message: str, history: List) -> tuple:\n",
    "        \"\"\"\n",
    "        Process user message with enhanced error handling\n",
    "        \n",
    "        Args:\n",
    "            message (str): User's message\n",
    "            history (List): Conversation history\n",
    "        Returns:\n",
    "            tuple: (message, updated_history)\n",
    "        \"\"\"\n",
    "        self.interface_metrics['total_interactions'] += 1\n",
    "        \n",
    "        try:\n",
    "            response = self.qa_system.get_response(message)\n",
    "            self.conversation_history.append({\n",
    "                'user_message': message,\n",
    "                'bot_response': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            return response, history + [[message, response]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.interface_metrics['error_count'] += 1\n",
    "            error_message = f\"Error: {str(e)}\"\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                metrics.record_error(ChatbotError(error_message))\n",
    "            return error_message, history + [[message, error_message]]\n",
    "    \n",
    "    def create_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Create enhanced Gradio interface\"\"\"\n",
    "        with gr.Blocks() as demo:\n",
    "            # Header\n",
    "            gr.Markdown(\"# Nestlé HR Chatbot\")\n",
    "            \n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot()\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Ask about Nestle's HR policies\",\n",
    "                placeholder=\"Type your question here...\",\n",
    "                show_label=True\n",
    "            )\n",
    "            \n",
    "            # Control buttons\n",
    "            with gr.Row():\n",
    "                clear = gr.Button(\"Clear Conversation\")\n",
    "                show_stats = gr.Button(\"Show Statistics\")\n",
    "            \n",
    "            # Statistics display\n",
    "            stats_output = gr.JSON(label=\"System Statistics\", visible=False)\n",
    "            \n",
    "            # Event handlers\n",
    "            msg.submit(\n",
    "                self.respond,\n",
    "                [msg, chatbot],\n",
    "                [msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            clear.click(\n",
    "                lambda: (None, None),\n",
    "                None,\n",
    "                [msg, chatbot],\n",
    "                queue=False\n",
    "            )\n",
    "            \n",
    "            show_stats.click(\n",
    "                lambda: {\n",
    "                    'interface': self.interface_metrics,\n",
    "                    'qa_system': self.qa_system.get_stats(),\n",
    "                    'overall_metrics': metrics.get_statistics()\n",
    "                },\n",
    "                None,\n",
    "                stats_output\n",
    "            )\n",
    "            \n",
    "        return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Application Execution\n",
    "\n",
    "This section orchestrates the entire application startup and execution process. Let's break down the implementation:\n",
    "\n",
    "### Main Function Components:\n",
    "\n",
    "1. **Application Setup:**\n",
    "   - **Environment Initialization:**\n",
    "     * Loads environment variables\n",
    "     * Validates API keys and paths\n",
    "     * Sets up logging system\n",
    "   \n",
    "   - **Document Processing:**\n",
    "     * Loads specified document\n",
    "     * Processes text into manageable chunks\n",
    "     * Creates vector embeddings\n",
    "\n",
    "2. **System Initialization:**\n",
    "   - **Vector Store:**\n",
    "     * Creates Chroma database\n",
    "     * Indexes document embeddings\n",
    "     * Prepares for efficient retrieval\n",
    "   \n",
    "   - **QA System:**\n",
    "     * Initializes question-answering chain\n",
    "     * Connects to OpenAI API\n",
    "     * Sets up rate limiting\n",
    "\n",
    "3. **Interface Launch:**\n",
    "   - Creates Gradio interface\n",
    "   - Configures web server\n",
    "   - Launches interactive interface\n",
    "\n",
    "### Error Handling:\n",
    "- **Comprehensive error catching:**\n",
    "  * Environment setup errors\n",
    "  * Document processing failures\n",
    "  * Model initialization issues\n",
    "  * Interface creation problems\n",
    "\n",
    "- **Logging:**\n",
    "  * Detailed error messages\n",
    "  * Application state logging\n",
    "  * Startup process tracking\n",
    "\n",
    "### Execution Flow:\n",
    "1. Loads environment configuration\n",
    "2. Processes input documents\n",
    "3. Initializes AI components\n",
    "4. Creates and launches interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 17:44:45,709 - __main__ - INFO - [2334330675.py:43] - Starting application initialization\n",
      "2024-12-29 17:44:45,710 - __main__ - INFO - [2334330675.py:46] - Loading environment and configuration\n",
      "2024-12-29 17:44:45,713 - __main__ - INFO - [465277660.py:92] - Environment variables loaded successfully\n",
      "2024-12-29 17:44:45,714 - __main__ - INFO - [465277660.py:33] - Configuration loaded successfully\n",
      "2024-12-29 17:44:45,715 - __main__ - INFO - [2334330675.py:52] - Loading documents\n",
      "2024-12-29 17:44:45,715 - __main__ - INFO - [5827190.py:14] - Starting document load: dataset/the_nestle_hr_policy_pdf_2012.pdf\n",
      "2024-12-29 17:44:46,152 - __main__ - INFO - [5827190.py:47] - Successfully loaded document with 8 pages in 0.44 seconds\n",
      "2024-12-29 17:44:46,153 - __main__ - INFO - [2334330675.py:57] - Initializing text processor\n",
      "2024-12-29 17:44:46,164 - __main__ - INFO - [5827190.py:82] - TextProcessor initialized successfully\n",
      "2024-12-29 17:44:46,165 - __main__ - INFO - [5827190.py:126] - Starting document processing\n",
      "2024-12-29 17:44:46,165 - __main__ - INFO - [5827190.py:104] - Split documents into 7 chunks in 0.00 seconds\n",
      "2024-12-29 17:44:46,165 - __main__ - INFO - [5827190.py:132] - Creating vector store\n",
      "2024-12-29 17:44:49,918 - chromadb.telemetry.product.posthog - INFO - [posthog.py:22] - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-12-29 17:44:51,399 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:44:51,492 - __main__ - INFO - [5827190.py:140] - Successfully created vector store in 5.33 seconds. Stats: {\n",
      "  \"total_chunks\": 7,\n",
      "  \"processing_time\": 5.327102899551392\n",
      "}\n",
      "2024-12-29 17:44:51,493 - __main__ - INFO - [2334330675.py:64] - Initializing QA system\n",
      "2024-12-29 17:44:51,514 - __main__ - INFO - [3138667305.py:60] - QA system initialized successfully\n",
      "2024-12-29 17:44:51,515 - __main__ - INFO - [2334330675.py:69] - Setting up chat interface\n",
      "2024-12-29 17:44:51,515 - __main__ - INFO - [2334330675.py:20] - Health status updated: healthy - Application initialized successfully\n",
      "2024-12-29 17:44:51,516 - __main__ - INFO - [2334330675.py:77] - Application initialization completed successfully\n",
      "/opt/homebrew/anaconda3/envs/1720690009_20241228_v2/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "2024-12-29 17:44:52,081 - httpx - INFO - [_client.py:1025] - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:44:52,104 - httpx - INFO - [_client.py:1025] - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:44:52,121 - httpx - INFO - [_client.py:1025] - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 17:44:52,125 - __main__ - INFO - [2334330675.py:125] - Application started successfully\n",
      "{\n",
      "  \"health\": {\n",
      "    \"status\": \"healthy\",\n",
      "    \"last_check\": \"2024-12-29T17:44:51.515899\",\n",
      "    \"message\": \"Application initialized successfully\"\n",
      "  },\n",
      "  \"uptime_seconds\": 6.415666,\n",
      "  \"initialized\": true,\n",
      "  \"components\": [\n",
      "    \"config\",\n",
      "    \"documents\",\n",
      "    \"text_processor\",\n",
      "    \"vector_store\",\n",
      "    \"qa_system\",\n",
      "    \"interface\"\n",
      "  ],\n",
      "  \"config\": {\n",
      "    \"config\": {\n",
      "      \"CHUNK_SIZE\": 1000,\n",
      "      \"CHUNK_OVERLAP\": 0,\n",
      "      \"MODEL_TEMPERATURE\": 0,\n",
      "      \"EMBEDDING_MODEL\": \"text-embedding-ada-002\",\n",
      "      \"MAX_RETRIES\": 3,\n",
      "      \"RATE_LIMIT_PER_SECOND\": 5,\n",
      "      \"LOG_LEVEL\": 20\n",
      "    },\n",
      "    \"load_time\": \"2024-12-29T17:43:49.946347\",\n",
      "    \"age_seconds\": 62.178805\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"document_load_duration\": {\n",
      "      \"mean\": 0.4368932247161865,\n",
      "      \"min\": 0.4368932247161865,\n",
      "      \"max\": 0.4368932247161865,\n",
      "      \"count\": 1\n",
      "    },\n",
      "    \"text_splitting_duration\": {\n",
      "      \"mean\": 0.00013685226440429688,\n",
      "      \"min\": 0.00013685226440429688,\n",
      "      \"max\": 0.00013685226440429688,\n",
      "      \"count\": 1\n",
      "    },\n",
      "    \"document_processing_duration\": {\n",
      "      \"mean\": 5.327102899551392,\n",
      "      \"min\": 5.327102899551392,\n",
      "      \"max\": 5.327102899551392,\n",
      "      \"count\": 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "2024-12-29 17:44:52,126 - __main__ - INFO - [2334330675.py:93] - Cleaning up application resources\n",
      "2024-12-29 17:44:52,126 - __main__ - INFO - [2334330675.py:100] - Application shutdown complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 17:46:33,856 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:46:34,696 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:46:34,715 - __main__ - INFO - [3138667305.py:100] - Query processed successfully in 1.54 seconds\n"
     ]
    }
   ],
   "source": [
    "class ApplicationState:\n",
    "    \"\"\"Manages global application state and initialization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.startup_time = datetime.now()\n",
    "        self.initialized = False\n",
    "        self.components = {}\n",
    "        self.health_status = {\n",
    "            'status': 'initializing',\n",
    "            'last_check': self.startup_time.isoformat()\n",
    "        }\n",
    "    \n",
    "    def update_health(self, status: str, message: str = None):\n",
    "        \"\"\"Update application health status\"\"\"\n",
    "        self.health_status.update({\n",
    "            'status': status,\n",
    "            'last_check': datetime.now().isoformat(),\n",
    "            'message': message\n",
    "        })\n",
    "        logger.info(f\"Health status updated: {status} - {message}\")\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get complete application status\"\"\"\n",
    "        return {\n",
    "            'health': self.health_status,\n",
    "            'uptime_seconds': (datetime.now() - self.startup_time).total_seconds(),\n",
    "            'initialized': self.initialized,\n",
    "            'components': list(self.components.keys()),\n",
    "            'config': config.to_dict(),\n",
    "            'metrics': metrics.get_statistics()\n",
    "        }\n",
    "\n",
    "def initialize_application() -> ApplicationState:\n",
    "    \"\"\"\n",
    "    Initialize all application components with comprehensive error handling\n",
    "    \n",
    "    Returns:\n",
    "        ApplicationState: Application state manager\n",
    "    \"\"\"\n",
    "    app_state = ApplicationState()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting application initialization\")\n",
    "        \n",
    "        # Load environment variables and configuration\n",
    "        logger.info(\"Loading environment and configuration\")\n",
    "        api_key, pdf_path = load_env()\n",
    "        config.load_from_env()\n",
    "        app_state.components['config'] = config\n",
    "        \n",
    "        # Load and process documents\n",
    "        logger.info(\"Loading documents\")\n",
    "        documents = load_document(pdf_path)\n",
    "        app_state.components['documents'] = f\"Loaded {len(documents)} pages\"\n",
    "        \n",
    "        # Initialize text processor\n",
    "        logger.info(\"Initializing text processor\")\n",
    "        processor = TextProcessor()\n",
    "        vector_store = processor.process_documents(documents)\n",
    "        app_state.components['text_processor'] = processor\n",
    "        app_state.components['vector_store'] = vector_store\n",
    "        \n",
    "        # Initialize QA system\n",
    "        logger.info(\"Initializing QA system\")\n",
    "        qa_system = QASystem(vector_store)\n",
    "        app_state.components['qa_system'] = qa_system\n",
    "        \n",
    "        # Initialize chat interface\n",
    "        logger.info(\"Setting up chat interface\")\n",
    "        interface = ChatInterface(qa_system)\n",
    "        app_state.components['interface'] = interface\n",
    "        \n",
    "        # Mark initialization as complete\n",
    "        app_state.initialized = True\n",
    "        app_state.update_health('healthy', 'Application initialized successfully')\n",
    "        \n",
    "        logger.info(\"Application initialization completed successfully\")\n",
    "        return app_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Application initialization failed: {str(e)}\"\n",
    "        logger.error(error_message)\n",
    "        app_state.update_health('error', error_message)\n",
    "        \n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ChatbotError(error_message)\n",
    "        metrics.record_error(e)\n",
    "        raise\n",
    "\n",
    "def cleanup_resources():\n",
    "    \"\"\"Cleanup application resources\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Cleaning up application resources\")\n",
    "        # Add cleanup logic here (e.g., closing file handles, database connections)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during cleanup: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        logger.info(\"Application shutdown complete\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main application entry point with enhanced error handling and monitoring\"\"\"\n",
    "    app_state = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize application\n",
    "        app_state = initialize_application()\n",
    "        \n",
    "        # Create and launch interface\n",
    "        demo = app_state.components['interface'].create_interface()\n",
    "        \n",
    "        # Configure shutdown handling\n",
    "        def handle_shutdown():\n",
    "            logger.info(\"Shutdown initiated\")\n",
    "            cleanup_resources()\n",
    "        \n",
    "        # Launch interface with shutdown handling\n",
    "        demo.launch(\n",
    "            share=False,  # Set to True to create a public URL\n",
    "            prevent_thread_lock=True\n",
    "        )\n",
    "        \n",
    "        # Log successful startup\n",
    "        logger.info(\n",
    "            \"Application started successfully\\n\" +\n",
    "            json.dumps(app_state.get_status(), indent=2)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Application startup failed: {str(e)}\"\n",
    "        logger.error(error_message)\n",
    "        \n",
    "        if app_state:\n",
    "            app_state.update_health('error', error_message)\n",
    "        \n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ChatbotError(error_message)\n",
    "        metrics.record_error(e)\n",
    "        \n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cleanup_resources()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Fatal error: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and System Reporting Utilities\n",
    "\n",
    "This section implements comprehensive testing and system reporting capabilities for application monitoring and maintenance.\n",
    "\n",
    "### Test Runner Implementation:\n",
    "1. **Test Categories:**\n",
    "   - Environment Validation:\n",
    "     * API key verification\n",
    "     * Path validation\n",
    "     * Configuration checks\n",
    "     * Permission testing\n",
    "   \n",
    "   - Document Processing:\n",
    "     * Loading verification\n",
    "     * Processing validation\n",
    "     * Format handling\n",
    "     * Error catching\n",
    "   \n",
    "   - QA System Testing:\n",
    "     * Query processing\n",
    "     * Response validation\n",
    "     * Performance metrics\n",
    "     * Error handling\n",
    "\n",
    "2. **Test Flow Management:**\n",
    "   - Sequential execution\n",
    "   - Dependency handling\n",
    "   - Error capture\n",
    "   - Result reporting\n",
    "\n",
    "3. **Performance Testing:**\n",
    "   - Response timing\n",
    "   - Resource usage\n",
    "   - Load handling\n",
    "   - Bottleneck identification\n",
    "\n",
    "### System Reporting Features:\n",
    "1. **Report Components:**\n",
    "   - System Configuration:\n",
    "     * Current settings\n",
    "     * Environment state\n",
    "     * Component status\n",
    "     * Resource allocation\n",
    "   \n",
    "   - Performance Metrics:\n",
    "     * Operation timings\n",
    "     * Resource usage\n",
    "     * Error rates\n",
    "     * Response times\n",
    "   \n",
    "   - Error Analysis:\n",
    "     * Error patterns\n",
    "     * Frequency analysis\n",
    "     * Impact assessment\n",
    "     * Resolution tracking\n",
    "\n",
    "2. **Report Generation:**\n",
    "   - Real-time updates\n",
    "   - Historical tracking\n",
    "   - Trend analysis\n",
    "   - Alert generation\n",
    "\n",
    "3. **Monitoring Integration:**\n",
    "   - Log aggregation\n",
    "   - Metric collection\n",
    "   - Alert triggering\n",
    "   - Status tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"Basic application tests\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting application tests\")\n",
    "        \n",
    "        # Test environment loading\n",
    "        api_key, pdf_path = load_env()\n",
    "        assert api_key, \"API key not loaded\"\n",
    "        assert pdf_path, \"PDF path not loaded\"\n",
    "        \n",
    "        # Test document loading\n",
    "        documents = load_document(pdf_path)\n",
    "        assert documents, \"Documents not loaded\"\n",
    "        \n",
    "        # Test text processing\n",
    "        processor = TextProcessor()\n",
    "        vector_store = processor.process_documents(documents)\n",
    "        assert vector_store, \"Vector store not created\"\n",
    "        \n",
    "        # Test QA system\n",
    "        qa_system = QASystem(vector_store)\n",
    "        test_query = \"What are the working hours?\"\n",
    "        response = qa_system.get_response(test_query)\n",
    "        assert response, \"No response received\"\n",
    "        \n",
    "        logger.info(\"All tests passed successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def generate_system_report() -> Dict[str, Any]:\n",
    "    \"\"\"Generate comprehensive system report\"\"\"\n",
    "    return {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'configuration': config.to_dict(),\n",
    "        'metrics': metrics.get_statistics(),\n",
    "        'environment': {\n",
    "            'python_version': sys.version,\n",
    "            'platform': platform.platform(),\n",
    "            'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        },\n",
    "        'log_summary': {\n",
    "            'error_count': len([r for r in metrics.metrics['errors']]),\n",
    "            'last_errors': metrics.metrics['errors'][-5:]  # Last 5 errors\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 17:48:53,527 - __main__ - INFO - [2109332838.py:4] - Starting application tests\n",
      "2024-12-29 17:48:53,531 - __main__ - INFO - [465277660.py:92] - Environment variables loaded successfully\n",
      "2024-12-29 17:48:53,532 - __main__ - INFO - [5827190.py:14] - Starting document load: dataset/the_nestle_hr_policy_pdf_2012.pdf\n",
      "2024-12-29 17:48:53,703 - __main__ - INFO - [5827190.py:47] - Successfully loaded document with 8 pages in 0.17 seconds\n",
      "2024-12-29 17:48:53,714 - __main__ - INFO - [5827190.py:82] - TextProcessor initialized successfully\n",
      "2024-12-29 17:48:53,714 - __main__ - INFO - [5827190.py:126] - Starting document processing\n",
      "2024-12-29 17:48:53,715 - __main__ - INFO - [5827190.py:104] - Split documents into 7 chunks in 0.00 seconds\n",
      "2024-12-29 17:48:53,715 - __main__ - INFO - [5827190.py:132] - Creating vector store\n",
      "2024-12-29 17:48:54,116 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:48:54,188 - __main__ - INFO - [5827190.py:140] - Successfully created vector store in 0.47 seconds. Stats: {\n",
      "  \"total_chunks\": 7,\n",
      "  \"processing_time\": 0.47304606437683105\n",
      "}\n",
      "2024-12-29 17:48:54,207 - __main__ - INFO - [3138667305.py:60] - QA system initialized successfully\n",
      "2024-12-29 17:48:54,335 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:48:55,330 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 17:48:55,335 - __main__ - INFO - [3138667305.py:100] - Query processed successfully in 1.13 seconds\n",
      "2024-12-29 17:48:55,337 - __main__ - INFO - [2109332838.py:26] - All tests passed successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2024-12-29T17:48:55.339909',\n",
       " 'configuration': {'config': {'CHUNK_SIZE': 1000,\n",
       "   'CHUNK_OVERLAP': 0,\n",
       "   'MODEL_TEMPERATURE': 0,\n",
       "   'EMBEDDING_MODEL': 'text-embedding-ada-002',\n",
       "   'MAX_RETRIES': 3,\n",
       "   'RATE_LIMIT_PER_SECOND': 5,\n",
       "   'LOG_LEVEL': 20},\n",
       "  'load_time': '2024-12-29T17:43:49.946347',\n",
       "  'age_seconds': 305.393591},\n",
       " 'metrics': {'document_load_duration': {'mean': 0.2923696041107178,\n",
       "   'min': 0.1708228588104248,\n",
       "   'max': 0.4368932247161865,\n",
       "   'count': 3},\n",
       "  'text_splitting_duration': {'mean': 0.00011658668518066406,\n",
       "   'min': 8.487701416015625e-05,\n",
       "   'max': 0.00013685226440429688,\n",
       "   'count': 3},\n",
       "  'document_processing_duration': {'mean': 2.147731383641561,\n",
       "   'min': 0.47304606437683105,\n",
       "   'max': 5.327102899551392,\n",
       "   'count': 3},\n",
       "  'get_response_duration': {'mean': 1.4640367825826008,\n",
       "   'min': 1.1291050910949707,\n",
       "   'max': 1.7221591472625732,\n",
       "   'count': 3}},\n",
       " 'environment': {'python_version': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]',\n",
       "  'platform': 'macOS-15.2-arm64-arm-64bit',\n",
       "  'memory_usage': 342.734375},\n",
       " 'log_summary': {'error_count': 0, 'last_errors': []}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_tests()\n",
    "generate_system_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1720690009_20241228_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
