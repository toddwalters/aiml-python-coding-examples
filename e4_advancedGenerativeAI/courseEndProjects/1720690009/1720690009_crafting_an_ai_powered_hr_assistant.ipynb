{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered HR Assistant: A Use Case for Nestle’s HR Policy Documents\n",
    "\n",
    "## Nestlé HR Policy Chatbot Overview\n",
    "\n",
    "This Jupyter Notebook implements a conversational chatbot designed to answer user queries based on information contained within Nestlé's HR policy documents. It leverages several powerful technologies from the field of Natural Language Processing (NLP) and Large Language Models (LLMs) to achieve this:\n",
    "\n",
    "1.  **Document Loading and Chunking:** The notebook begins by loading the HR policy document (in PDF format) and splitting it into smaller, manageable text chunks. This is crucial because LLMs have limitations on the amount of text they can process at once.\n",
    "\n",
    "2.  **Text Embeddings:** Each text chunk is then converted into a numerical vector representation called an \"embedding.\" These embeddings capture the semantic meaning of the text, allowing the system to understand the relationships between different pieces of information. OpenAI's embedding models are used for this purpose.\n",
    "\n",
    "3.  **Vector Database:** The embeddings are stored in a vector database (Chroma), which allows for efficient similarity search. This means that when a user asks a question, the system can quickly find the most relevant text chunks from the HR policy.\n",
    "\n",
    "4.  **Question Answering with LLM:** The most relevant text chunks are then passed to a large language model (OpenAI's GPT model) along with the user's question. The LLM uses this context to generate a coherent and informative answer.\n",
    "\n",
    "5.  **User Interface:** Finally, a user-friendly interface is created using Gradio, allowing users to easily interact with the chatbot.\n",
    "\n",
    "In summary, this notebook demonstrates a complete workflow for building a question-answering system over a PDF document using state-of-the-art NLP and LLM techniques. This approach can be generalized to other document types and domains, making it a valuable tool for information retrieval and knowledge management. The notebook is structured with Markdown explanations and code blocks separated by functionality, making it easy to follow and understand the implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of Libraries\n",
    "\n",
    "This code block uses `pip` to install the necessary Python libraries. These libraries are essential for the chatbot's functionality:\n",
    "\n",
    "- `openai`: For interacting with OpenAI's models (GPT).\n",
    "- `langchain`: A framework for developing applications powered by language models.\n",
    "- `chromadb`: A vector database for storing and retrieving text embeddings.\n",
    "- `pypdf`: For loading and processing PDF documents.\n",
    "- `gradio`: For creating the user interface.\n",
    "- `tiktoken`: For tokenizing text, especially important for managing context windows with large language models.\n",
    "\n",
    "**Note:** You only need to run this cell once. If you've already installed these libraries, you can skip this step. If you are running this in a Google Colab notebook, you will need to run this section every time you connect to a new runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this in your notebook environment if needed)\n",
    "# !pip install openai langchain chromadb pypdf gradio tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Set API Key\n",
    "\n",
    "This block imports the required libraries and sets up the OpenAI API key.\n",
    "\n",
    "- **Imports:** Imports the necessary classes and functions from the installed libraries.\n",
    "- **API Key:** Sets the OpenAI API key using environment variables. This is the **recommended** way to manage API keys for security reasons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/1720690009_20241228_v2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set OpenAI API key (replace with your actual key - use environment variables)\n",
    "def load_env():\n",
    "    \"\"\"Load and validate environment variables\"\"\"\n",
    "    load_dotenv(verbose=True)\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    pdf_doc = os.getenv('PDF_DOC_PATH')\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "    if not pdf_doc:\n",
    "        raise ValueError(\"PDF_DOC_PATH note found in environment variables\")\n",
    "    return api_key, pdf_doc\n",
    "\n",
    "api_key, pdf_doc = load_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the PDF Document\n",
    "\n",
    "This block loads the PDF document using `PyPDFLoader`.\n",
    "\n",
    "- **`PyPDFLoader`:** This class from `langchain` loads the PDF file.\n",
    "- **`loader.load()`:** This method reads the PDF and extracts the text content.\n",
    "- **Error Handling:** The `try-except` block handles the case where the specified PDF file is not found, preventing the code from crashing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_doc) # Replace with your PDF file name\n",
    "    documents = loader.load()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: {pdf_doc} not found. Please ensure the file is in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Document into Chunks\n",
    "\n",
    "Large documents are often split into smaller chunks for processing by language models. This block uses `CharacterTextSplitter` to do this.\n",
    "\n",
    "- **`CharacterTextSplitter`:** This class splits the text into chunks of a specified size.\n",
    "- **`chunk_size`:** The maximum number of characters in each chunk (1000 in this case).\n",
    "- **`chunk_overlap`:** The number of overlapping characters between adjacent chunks (0 in this case). Overlapping chunks can help maintain context.\n",
    "- **`texts = text_splitter.split_documents(documents)`:** This line performs the actual splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings and Store in Chroma\n",
    "\n",
    "This block creates vector embeddings of the text chunks and stores them in a Chroma vector database.\n",
    "\n",
    "*   **`OpenAIEmbeddings()`:** This creates an embedding model from OpenAI, which converts text into numerical vectors.\n",
    "*   **`Chroma.from_documents(texts, embeddings)`:** This creates a Chroma database and adds the text embeddings to it. The embeddings allow for efficient semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 14:45:18,275 - chromadb.telemetry.product.posthog - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "2024-12-29 14:45:19,523 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings and store them in Chroma\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a RetrievalQA Chain\n",
    "\n",
    "This block creates the question-answering chain using `RetrievalQA`.\n",
    "\n",
    "*   **`RetrievalQA`:** This chain combines a language model with a retriever (in this case, our Chroma database) to answer questions based on the retrieved information.\n",
    "*   **`llm=OpenAI(temperature=0)`:** Specifies the OpenAI language model to use. `temperature=0` makes the responses more deterministic.\n",
    "*   **`chain_type=\"stuff\"`:** This chain type \"stuffs\" all the retrieved context into the prompt to the language model.\n",
    "*   **`retriever=db.as_retriever()`:** Sets the retriever to our Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/h_10706j24qc8qjh7x3yy0g40000gn/T/ipykernel_81788/2092767558.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=db.as_retriever())\n"
     ]
    }
   ],
   "source": [
    "# Create a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(temperature=0), chain_type=\"stuff\", retriever=db.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Chatbot Function\n",
    "\n",
    "This block defines the `chatbot` function, which takes a user query as input and returns the chatbot's response.\n",
    "\n",
    "*   **`qa.run(query)`:** This runs the question-answering chain with the user's query.\n",
    "*   **Error Handling:** The `try-except` block handles potential errors during query processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the chatbot function\n",
    "def chatbot(query):\n",
    "    try:\n",
    "        result = qa.run(query)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Launch the Gradio Interface\n",
    "\n",
    "This block creates and launches the Gradio user interface.\n",
    "\n",
    "*   **`gr.Interface(...)`:** Creates the Gradio interface.\n",
    "*   **`fn=chatbot`:** Specifies the function to call when the user submits a query.\n",
    "*   **`inputs`:** Defines the input component (a textbox).\n",
    "*   **`outputs`:** Defines the output component (another textbox).\n",
    "*   **`title` and `description`:** Set the title and description of the interface.\n",
    "*   **`iface.launch()`:** Launches the Gradio interface, making it accessible in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 14:45:19,844 - httpx - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:45:19,853 - httpx - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 14:45:20,061 - httpx - INFO - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
      "/var/folders/1c/h_10706j24qc8qjh7x3yy0g40000gn/T/ipykernel_81788/2060248823.py:4: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(query)\n",
      "2024-12-29 14:45:54,382 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:45:55,035 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:47:11,305 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:47:12,837 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:47:49,624 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 14:47:51,001 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=chatbot,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your query about Nestlé's HR policy here...\"),\n",
    "    outputs=gr.Textbox(lines=5),\n",
    "    title=\"Nestlé HR Chatbot\",\n",
    "    description=\"Ask me anything about Nestlé's HR policies.\",\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1720690009_20241228_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
