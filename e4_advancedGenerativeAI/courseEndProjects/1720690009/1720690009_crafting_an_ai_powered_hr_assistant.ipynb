{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered HR Assistant: A Use Case for Nestle’s HR Policy Documents\n",
    "\n",
    "## Nestlé HR Policy Chatbot Overview\n",
    "\n",
    "This Jupyter Notebook implements a conversational chatbot designed to answer user queries based on information contained within Nestlé's HR policy documents. It leverages several powerful technologies from the field of Natural Language Processing (NLP) and Large Language Models (LLMs) to achieve this:\n",
    "\n",
    "1.  **Document Loading and Chunking:** The notebook begins by loading the HR policy document (in PDF format) and splitting it into smaller, manageable text chunks. This is crucial because LLMs have limitations on the amount of text they can process at once.\n",
    "\n",
    "2.  **Text Embeddings:** Each text chunk is then converted into a numerical vector representation called an \"embedding.\" These embeddings capture the semantic meaning of the text, allowing the system to understand the relationships between different pieces of information. OpenAI's embedding models are used for this purpose.\n",
    "\n",
    "3.  **Vector Database:** The embeddings are stored in a vector database (Chroma), which allows for efficient similarity search. This means that when a user asks a question, the system can quickly find the most relevant text chunks from the HR policy.\n",
    "\n",
    "4.  **Question Answering with LLM:** The most relevant text chunks are then passed to a large language model (OpenAI's GPT model) along with the user's question. The LLM uses this context to generate a coherent and informative answer.\n",
    "\n",
    "5.  **User Interface:** Finally, a user-friendly interface is created using Gradio, allowing users to easily interact with the chatbot.\n",
    "\n",
    "In summary, this notebook demonstrates a complete workflow for building a question-answering system over a PDF document using state-of-the-art NLP and LLM techniques. This approach can be generalized to other document types and domains, making it a valuable tool for information retrieval and knowledge management. The notebook is structured with Markdown explanations and code blocks separated by functionality, making it easy to follow and understand the implementation details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation of Libraries\n",
    "\n",
    "This code block uses `pip` to install the necessary Python libraries. These libraries are essential for the chatbot's functionality:\n",
    "\n",
    "*   `openai`: For interacting with OpenAI's models (GPT).\n",
    "*   `langchain`: A framework for developing applications powered by language models.\n",
    "*   `chromadb`: A vector database for storing and retrieving text embeddings.\n",
    "*   `pypdf`: For loading and processing PDF documents.\n",
    "*   `gradio`: For creating the user interface.\n",
    "*   `tiktoken`: For tokenizing text, especially important for managing context windows with large language models.\n",
    "\n",
    "**Note:** You only need to run this cell once. If you've already installed these libraries, you can skip this step. If you are running this in a Google Colab notebook, you will need to run this section every time you connect to a new runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this in your notebook environment if needed)\n",
    "# !pip install openai langchain chromadb pypdf gradio tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Set API Key\n",
    "\n",
    "This block imports the required libraries and sets up the OpenAI API key.\n",
    "\n",
    "*   **Imports:** Imports the necessary classes and functions from the installed libraries.\n",
    "*   **API Key:** Sets the OpenAI API key using environment variables. This is the **recommended** way to manage API keys for security reasons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import logging\n",
    "import gradio as gr\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set OpenAI API key (replace with your actual key - use environment variables)\n",
    "def load_env():\n",
    "    \"\"\"Load and validate environment variables\"\"\"\n",
    "    load_dotenv(verbose=True)\n",
    "    api_key = os.getenv('OPENAI_API_KEY')\n",
    "    pdf_doc = os.getenv('PDF_DOC_PATH')\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "    if not pdf_doc:\n",
    "        raise ValueError(\"PDF_DOC_PATH note found in environment variables\")\n",
    "    return api_key, pdf_doc\n",
    "\n",
    "api_key, pdf_doc = load_env()\n",
    "\n",
    "print(f'api_key: {api_key}')\n",
    "print(f'pdf doc path: {pdf_doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the PDF Document\n",
    "\n",
    "This block loads the PDF document using `PyPDFLoader`.\n",
    "\n",
    "*   **`PyPDFLoader`:** This class from `langchain` loads the PDF file.\n",
    "*   **`loader.load()`:** This method reads the PDF and extracts the text content.\n",
    "*   **Error Handling:** The `try-except` block handles the case where the specified PDF file is not found, preventing the code from crashing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF document\n",
    "try:\n",
    "    loader = PyPDFLoader(pdf_doc) # Replace with your PDF file name\n",
    "    documents = loader.load()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: {pdf_doc} not found. Please ensure the file is in the correct directory or provide the correct path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Document into Chunks\n",
    "\n",
    "Large documents are often split into smaller chunks for processing by language models. This block uses `CharacterTextSplitter` to do this.\n",
    "\n",
    "*   **`CharacterTextSplitter`:** This class splits the text into chunks of a specified size.\n",
    "*   **`chunk_size`:** The maximum number of characters in each chunk (1000 in this case).\n",
    "*   **`chunk_overlap`:** The number of overlapping characters between adjacent chunks (0 in this case). Overlapping chunks can help maintain context.\n",
    "*   **`texts = text_splitter.split_documents(documents)`:** This line performs the actual splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
