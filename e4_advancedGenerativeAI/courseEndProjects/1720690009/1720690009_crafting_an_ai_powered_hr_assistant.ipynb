{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced AI-Powered HR Assistant for Nestle\n",
    "\n",
    "## Nestlé HR Policy Chatbot Overview\n",
    "\n",
    "This Jupyter Notebook implements a conversational chatbot designed to answer user queries based on information contained within Nestlé's HR policy documents. It leverages several powerful technologies from the field of Natural Language Processing (NLP) and Large Language Models (LLMs) to achieve this:\n",
    "\n",
    "1.  **Document Loading and Chunking:** The notebook begins by loading the HR policy document (in PDF format) and splitting it into smaller, manageable text chunks. This is crucial because LLMs have limitations on the amount of text they can process at once.\n",
    "\n",
    "2.  **Text Embeddings:** Each text chunk is then converted into a numerical vector representation called an \"embedding.\" These embeddings capture the semantic meaning of the text, allowing the system to understand the relationships between different pieces of information. OpenAI's embedding models are used for this purpose.\n",
    "\n",
    "3.  **Vector Database:** The embeddings are stored in a vector database (Chroma), which allows for efficient similarity search. This means that when a user asks a question, the system can quickly find the most relevant text chunks from the HR policy.\n",
    "\n",
    "4.  **Question Answering with LLM:** The most relevant text chunks are then passed to a large language model (OpenAI's GPT model) along with the user's question. The LLM uses this context to generate a coherent and informative answer.\n",
    "\n",
    "5.  **User Interface:** Finally, a user-friendly interface is created using Gradio, allowing users to easily interact with the chatbot.\n",
    "\n",
    "In summary, this notebook demonstrates a complete workflow for building a question-answering system over a PDF document using state-of-the-art NLP and LLM techniques. This approach can be generalized to other document types and domains, making it a valuable tool for information retrieval and knowledge management. The notebook is structured with Markdown explanations and code blocks separated by functionality, making it easy to follow and understand the implementation details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage and Setup Guide\n",
    "\n",
    "### Prerequisites:\n",
    "1. **Environment Setup:**\n",
    "   - Python 3.8 or higher\n",
    "   - Required packages installed\n",
    "   - .env file configured\n",
    "\n",
    "2. **Required Environment Variables:**\n",
    "   ```\n",
    "   OPENAI_API_KEY=your_api_key_here\n",
    "   PDF_DOC_PATH=path_to_your_pdf_file\n",
    "   ```\n",
    "\n",
    "### Running the Application:\n",
    "1. Ensure all cells are run in order\n",
    "2. Wait for the Gradio interface to launch\n",
    "3. Interface will be available at http://127.0.0.1:7860\n",
    "\n",
    "### Features:\n",
    "- Real-time question answering\n",
    "- Conversation history tracking\n",
    "- Error handling and recovery\n",
    "- Rate limiting protection\n",
    "\n",
    "### Best Practices:\n",
    "- Keep API keys secure\n",
    "- Monitor usage rates\n",
    "- Regular logging review\n",
    "- Backup document sources\n",
    "\n",
    "### Troubleshooting:\n",
    "- Check environment variables\n",
    "- Verify PDF file accessibility\n",
    "- Monitor API rate limits\n",
    "- Review error logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation of Libraries\n",
    "\n",
    "This code block manages the installation of required Python libraries and their dependencies. Let's examine each component:\n",
    "\n",
    "### Core Dependencies:\n",
    "- `openai` (>=1.0.0): Core API interface for OpenAI services\n",
    "  * Required for: API authentication, model interactions\n",
    "  * Dependencies: requests, typing-extensions\n",
    "\n",
    "- `langchain` (>=0.1.0): Framework for LLM applications\n",
    "  * Requires: openai, numpy\n",
    "  * Components: Chains, agents, memory systems\n",
    "  * Sub-packages:\n",
    "    - `langchain-community`: Community integrations\n",
    "    - `langchain-openai`: OpenAI-specific modules\n",
    "\n",
    "### Vector Storage and Document Processing:\n",
    "- `chromadb`: Vector database for embedding storage\n",
    "  * Handles: Similarity search, vector indexing\n",
    "  * Requires: numpy, sqlalchemy\n",
    "\n",
    "- `pypdf`: PDF document processing\n",
    "  * Features: Text extraction, document parsing\n",
    "  * Used for: Loading and processing HR documents\n",
    "\n",
    "### Interface and Utilities:\n",
    "- `gradio`: Web interface framework\n",
    "  * Purpose: Interactive chat interface\n",
    "  * Dependencies: fastapi, websockets\n",
    "\n",
    "- `tiktoken`: OpenAI's tokenizer\n",
    "  * Used for: Token counting, context management\n",
    "  * Critical for: Rate limiting, cost management\n",
    "\n",
    "### System Utilities:\n",
    "- `python-dotenv`: Environment variable management\n",
    "  * Security: API key and configuration handling\n",
    "  * Local development: .env file support\n",
    "\n",
    "- `psutil`: System resource monitoring\n",
    "  * Memory tracking\n",
    "  * Performance metrics\n",
    "\n",
    "### Standard Library Requirements:\n",
    "- `logging`: Application monitoring and debugging\n",
    "- `json`: Data serialization\n",
    "- `datetime`: Timestamp management\n",
    "- `collections`: Advanced data structures\n",
    "- `typing`: Type hints and validation\n",
    "- `pathlib`: Cross-platform path handling\n",
    "\n",
    "### Installation Command:\n",
    "```bash\n",
    "pip install openai>=1.0.0 langchain>=0.1.0 langchain-community langchain-openai chromadb pypdf gradio tiktoken python-dotenv psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (run this in your notebook environment if needed)\n",
    "# !pip install openai langchain langchain-community langchain-openai chromadb pypdf gradio tiktoken python-dotenv psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Imports and Error Handling Classes\n",
    "\n",
    "This section establishes the foundation of our application through comprehensive import management and logging configuration. Let's examine each component in detail:\n",
    "\n",
    "### Import Structure and Purpose:\n",
    "\n",
    "1. **System and OS Operations:**\n",
    "   - `os`: Environment variables, path operations\n",
    "   - `sys`: Python runtime operations\n",
    "   - `platform`: System identification\n",
    "   - `psutil`: Resource monitoring\n",
    "\n",
    "2. **Data Type Management:**\n",
    "   - `typing`: Type hints for code reliability\n",
    "     * List: For document collections\n",
    "     * Dict: For configuration and responses\n",
    "     * Any: For flexible type handling\n",
    "     * Optional: For nullable parameters\n",
    "\n",
    "3. **Time and Data Handling:**\n",
    "   - `datetime`: Timestamp generation and tracking\n",
    "   - `collections.defaultdict`: Automatic dictionary initialization\n",
    "   - `time`: Performance measurements\n",
    "   - `json`: Data serialization\n",
    "\n",
    "4. **File Operations:**\n",
    "   - `pathlib.Path`: Cross-platform path handling\n",
    "     * File existence checking\n",
    "     * Path manipulation\n",
    "     * Extension handling\n",
    "\n",
    "### External Dependencies:\n",
    "\n",
    "1. **AI and Language Processing:**\n",
    "   - `openai`: OpenAI API interface\n",
    "   - `langchain` components:\n",
    "     * Document loaders: PDF processing\n",
    "     * Text splitters: Content chunking\n",
    "     * Embeddings: Vector creation\n",
    "     * Chains: Processing pipeline\n",
    "\n",
    "2. **User Interface:**\n",
    "   - `gradio`: Web interface components\n",
    "     * Chatbot: Conversation display\n",
    "     * Textbox: User input\n",
    "     * Button: Control elements\n",
    "\n",
    "### Logging Configuration:\n",
    "\n",
    "1. **Handler Setup:**\n",
    "   - File Handler:\n",
    "     * Location: 'chatbot.log'\n",
    "     * Purpose: Persistent logging\n",
    "     * Mode: Append\n",
    "   \n",
    "   - Stream Handler:\n",
    "     * Purpose: Real-time console output\n",
    "     * Level: INFO and above\n",
    "\n",
    "2. **Format Pattern:**\n",
    "    - %(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s\n",
    "        - `asctime`: Timestamp with milliseconds\n",
    "        - `name`: Logger component identifier\n",
    "        - `levelname`: Severity (INFO/WARNING/ERROR)\n",
    "        - `filename:lineno`: Source location\n",
    "        - `message`: Log content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import platform\n",
    "import psutil\n",
    "from typing import List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from functools import wraps\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Core dependencies\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Document processing\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('chatbot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Configuration System\n",
    "\n",
    "This section implements a comprehensive error handling hierarchy and configuration management system. Let's examine each component in detail:\n",
    "\n",
    "### Error Handling Architecture:\n",
    "\n",
    "1. **Base Error Class (ChatbotError):**\n",
    "   - **Core Features:**\n",
    "     * Custom message formatting\n",
    "     * Error code management\n",
    "     * Timestamp tracking\n",
    "     * JSON serialization\n",
    "     * Logging integration\n",
    "   \n",
    "   - **Implementation Details:**\n",
    "     * Inherits from Python's Exception\n",
    "     * Automated timestamp generation\n",
    "     * Standardized error formatting\n",
    "     * Logging level configuration\n",
    "\n",
    "2. **Specialized Error Classes:**\n",
    "   \n",
    "   A. **ConfigurationError:**\n",
    "   - **Purpose:** Configuration and setup issues\n",
    "   - **Additional Fields:**\n",
    "     * config_key: Affected configuration parameter\n",
    "     * Validation context\n",
    "     * Setting details\n",
    "   \n",
    "   B. **DocumentLoadError:**\n",
    "   - **Purpose:** Document processing failures\n",
    "   - **Additional Fields:**\n",
    "     * file_path: Problematic document location\n",
    "     * Format details\n",
    "     * Processing stage information\n",
    "   \n",
    "   C. **ModelError:**\n",
    "   - **Purpose:** AI model operation issues\n",
    "   - **Additional Fields:**\n",
    "     * model_name: Affected AI model\n",
    "     * query: Problematic input\n",
    "     * Context information\n",
    "     * Performance metrics\n",
    "\n",
    "### Configuration Management System:\n",
    "\n",
    "1. **Default Configuration:**\n",
    "   - **Core Settings:**\n",
    "     * CHUNK_SIZE: Text segment length (1000 chars)\n",
    "     * CHUNK_OVERLAP: Segment overlap (0 chars)\n",
    "     * MODEL_TEMPERATURE: Response randomness (0)\n",
    "     * EMBEDDING_MODEL: Vector model selection\n",
    "   \n",
    "   - **Advanced Settings:**\n",
    "     * MAX_RETRIES: Operation retry limit\n",
    "     * RATE_LIMIT_PER_SECOND: API call throttling\n",
    "     * LOG_LEVEL: Logging detail control\n",
    "\n",
    "2. **Configuration Methods:**\n",
    "   - **load_from_env:**\n",
    "     * Environment variable processing\n",
    "     * Type conversion handling\n",
    "     * Validation enforcement\n",
    "     * Error reporting\n",
    "   \n",
    "   - **get_settings:**\n",
    "     * Configuration serialization\n",
    "     * Private attribute filtering\n",
    "     * Format standardization\n",
    "\n",
    "3. **Type Handling:**\n",
    "   - String to numeric conversion\n",
    "   - Boolean interpretation\n",
    "   - List/dictionary parsing\n",
    "   - Default value management\n",
    "\n",
    "### Metrics Tracking System:\n",
    "\n",
    "1. **Performance Metrics:**\n",
    "   - Operation duration tracking\n",
    "   - Resource utilization\n",
    "   - API call monitoring\n",
    "   - Error rate calculation\n",
    "\n",
    "2. **Statistical Analysis:**\n",
    "   - Mean/min/max calculations\n",
    "   - Error pattern detection\n",
    "   - Performance trending\n",
    "   - Resource usage analysis\n",
    "\n",
    "### Implementation Features:\n",
    "\n",
    "1. **Error Handling:**\n",
    "   - Hierarchical error classification\n",
    "   - Detailed error context capture\n",
    "   - Automated logging integration\n",
    "   - Recovery mechanism support\n",
    "\n",
    "2. **Configuration Management:**\n",
    "   - Centralized settings control\n",
    "   - Environment-based configuration\n",
    "   - Type-safe value handling\n",
    "   - Validation enforcement\n",
    "\n",
    "3. **Metrics Collection:**\n",
    "   - Real-time performance tracking\n",
    "   - Statistical aggregation\n",
    "   - Error pattern analysis\n",
    "   - Resource utilization monitoring\n",
    "\n",
    "### Best Practices:\n",
    "1. Consistent error formatting\n",
    "2. Comprehensive error context\n",
    "3. Type-safe configuration\n",
    "4. Detailed logging integration\n",
    "5. Performance metric tracking\n",
    "6. Resource usage monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotError(Exception):\n",
    "    \"\"\"Base exception class for chatbot errors\"\"\"\n",
    "    def __init__(self, message: str = None, error_code: str = None):\n",
    "        self.message = message or \"An unexpected error occurred in the chatbot\"\n",
    "        self.error_code = error_code or \"CHATBOT_ERROR\"\n",
    "        self.timestamp = datetime.now()\n",
    "        \n",
    "        formatted_message = f\"[{self.error_code}] {self.timestamp}: {self.message}\"\n",
    "        super().__init__(formatted_message)\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert error details to dictionary for logging or API responses\"\"\"\n",
    "        return {\n",
    "            \"error_code\": self.error_code,\n",
    "            \"message\": self.message,\n",
    "            \"timestamp\": self.timestamp.isoformat(),\n",
    "            \"error_type\": self.__class__.__name__\n",
    "        }\n",
    "    \n",
    "    def log_error(self, log_level: int = logging.ERROR):\n",
    "        \"\"\"Log the error with specified logging level\"\"\"\n",
    "        logger.log(log_level, json.dumps(self.to_dict(), indent=2))\n",
    "\n",
    "class ConfigurationError(ChatbotError):\n",
    "    \"\"\"Raised when there are configuration-related issues\"\"\"\n",
    "    def __init__(self, message: str = None, config_key: str = None):\n",
    "        self.config_key = config_key\n",
    "        error_message = f\"Configuration error{f' for {config_key}' if config_key else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"CONFIG_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add configuration-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict[\"config_key\"] = self.config_key\n",
    "        return error_dict\n",
    "\n",
    "class DocumentLoadError(ChatbotError):\n",
    "    \"\"\"Raised when there are issues loading documents\"\"\"\n",
    "    def __init__(self, message: str = None, file_path: str = None):\n",
    "        self.file_path = file_path\n",
    "        error_message = f\"Failed to load document{f' {file_path}' if file_path else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"DOC_LOAD_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add document-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict[\"file_path\"] = self.file_path\n",
    "        return error_dict\n",
    "\n",
    "class ModelError(ChatbotError):\n",
    "    \"\"\"Raised when there are issues with the AI model\"\"\"\n",
    "    def __init__(self, message: str = None, model_name: str = None, query: str = None):\n",
    "        self.model_name = model_name\n",
    "        self.query = query\n",
    "        error_message = f\"Model error{f' for {model_name}' if model_name else ''}: {message}\"\n",
    "        super().__init__(\n",
    "            message=error_message,\n",
    "            error_code=\"MODEL_ERROR\"\n",
    "        )\n",
    "        \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Add model-specific details to error dictionary\"\"\"\n",
    "        error_dict = super().to_dict()\n",
    "        error_dict.update({\n",
    "            \"model_name\": self.model_name,\n",
    "            \"query\": self.query if self.query else None\n",
    "        })\n",
    "        return error_dict\n",
    "class Config:\n",
    "    \"\"\"Configuration settings for the chatbot\"\"\"\n",
    "    CHUNK_SIZE = 1000\n",
    "    CHUNK_OVERLAP = 0\n",
    "    MODEL_TEMPERATURE = 0\n",
    "    EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "    \n",
    "    @classmethod\n",
    "    def get_settings(cls) -> Dict[str, Any]:\n",
    "        \"\"\"Returns all configuration settings as a dictionary\"\"\"\n",
    "        return {k: v for k, v in cls.__dict__.items() \n",
    "                if not k.startswith('_')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and API Key Management\n",
    "\n",
    "This section implements secure environment configuration and API key management with comprehensive validation and error handling. Let's examine the components in detail:\n",
    "\n",
    "### Configuration Class Implementation:\n",
    "\n",
    "1. **Default Configuration Management:**\n",
    "   - **Structure:**\n",
    "     * Dictionary-based storage\n",
    "     * Type-safe value handling\n",
    "     * Immutable defaults\n",
    "     * Runtime override support\n",
    "   \n",
    "   - **Default Values:**\n",
    "     * Text processing parameters\n",
    "     * Model configuration\n",
    "     * Performance limits\n",
    "     * Logging settings\n",
    "\n",
    "2. **Configuration Methods:**\n",
    "   - **Initialization:**\n",
    "     * Default value copying\n",
    "     * Load time tracking\n",
    "     * Validation setup\n",
    "     * Error handling initialization\n",
    "\n",
    "   - **Environment Loading:**\n",
    "     * Variable detection\n",
    "     * Type conversion\n",
    "     * Validation\n",
    "     * Error reporting\n",
    "\n",
    "   - **Value Access:**\n",
    "     * Type-safe retrieval\n",
    "     * Default handling\n",
    "     * Error checking\n",
    "     * Logging integration\n",
    "\n",
    "### Environment Variable Management:\n",
    "\n",
    "1. **Required Variables:**\n",
    "   - **OpenAI Configuration:**\n",
    "     * API_KEY: Authentication token\n",
    "     * Model selection\n",
    "     * Rate limits\n",
    "   \n",
    "   - **Document Management:**\n",
    "     * PDF_DOC_PATH: Document location\n",
    "     * Format requirements\n",
    "     * Access permissions\n",
    "\n",
    "2. **Validation Process:**\n",
    "   - **Security Checks:**\n",
    "     * API key format validation\n",
    "     * Path existence verification\n",
    "     * Permission checking\n",
    "     * Format validation\n",
    "   \n",
    "   - **Error Handling:**\n",
    "     * Detailed error messages\n",
    "     * Recovery suggestions\n",
    "     * Logging integration\n",
    "     * Security considerations\n",
    "\n",
    "### Metrics System:\n",
    "\n",
    "1. **Performance Tracking:**\n",
    "   - **Duration Metrics:**\n",
    "     * Operation timing\n",
    "     * Statistical analysis\n",
    "     * Threshold monitoring\n",
    "     * Performance logging\n",
    "   \n",
    "   - **Error Tracking:**\n",
    "     * Error categorization\n",
    "     * Pattern detection\n",
    "     * Impact analysis\n",
    "     * Resolution tracking\n",
    "\n",
    "2. **Statistical Analysis:**\n",
    "   - **Calculations:**\n",
    "     * Mean/min/max values\n",
    "     * Error frequencies\n",
    "     * Performance trends\n",
    "     * Resource utilization\n",
    "\n",
    "### Implementation Features:\n",
    "\n",
    "1. **Security:**\n",
    "   - Secure variable handling\n",
    "   - Path sanitization\n",
    "   - Access control\n",
    "   - Error masking\n",
    "\n",
    "2. **Validation:**\n",
    "   - Type checking\n",
    "   - Format verification\n",
    "   - Permission validation\n",
    "   - Path confirmation\n",
    "\n",
    "3. **Monitoring:**\n",
    "   - Performance tracking\n",
    "   - Error logging\n",
    "   - Resource monitoring\n",
    "   - Usage analytics\n",
    "\n",
    "### Error Recovery:\n",
    "\n",
    "1. **Handling Strategies:**\n",
    "   - Graceful degradation\n",
    "   - Retry mechanisms\n",
    "   - Alternative paths\n",
    "   - User notification\n",
    "\n",
    "2. **Logging Integration:**\n",
    "   - Detailed error context\n",
    "   - Stack trace capture\n",
    "   - System state recording\n",
    "   - Recovery attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Configuration settings for the chatbot\"\"\"\n",
    "    \n",
    "    # Default configuration values\n",
    "    DEFAULT_CONFIG = {\n",
    "        'CHUNK_SIZE': 1000,\n",
    "        'CHUNK_OVERLAP': 0,\n",
    "        'MODEL_TEMPERATURE': 0,\n",
    "        'EMBEDDING_MODEL': \"text-embedding-ada-002\",\n",
    "        'MAX_RETRIES': 3,\n",
    "        'RATE_LIMIT_PER_SECOND': 5,\n",
    "        'LOG_LEVEL': logging.INFO\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._config = self.DEFAULT_CONFIG.copy()\n",
    "        self._load_time = datetime.now()\n",
    "        \n",
    "    def load_from_env(self) -> None:\n",
    "        \"\"\"Load configuration from environment variables\"\"\"\n",
    "        try:\n",
    "            for key in self.DEFAULT_CONFIG:\n",
    "                env_value = os.getenv(f'CHATBOT_{key}')\n",
    "                if env_value:\n",
    "                    # Convert string values to appropriate types\n",
    "                    if isinstance(self.DEFAULT_CONFIG[key], int):\n",
    "                        self._config[key] = int(env_value)\n",
    "                    elif isinstance(self.DEFAULT_CONFIG[key], float):\n",
    "                        self._config[key] = float(env_value)\n",
    "                    else:\n",
    "                        self._config[key] = env_value\n",
    "                        \n",
    "            logger.info(\"Configuration loaded successfully\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Invalid configuration value: {str(e)}\",\n",
    "                config_key=key\n",
    "            )\n",
    "    \n",
    "    def get(self, key: str) -> Any:\n",
    "        \"\"\"Get configuration value\"\"\"\n",
    "        if key not in self._config:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Configuration key not found: {key}\",\n",
    "                config_key=key\n",
    "            )\n",
    "        return self._config[key]\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert configuration to dictionary\"\"\"\n",
    "        return {\n",
    "            'config': self._config,\n",
    "            'load_time': self._load_time.isoformat(),\n",
    "            'age_seconds': (datetime.now() - self._load_time).total_seconds()\n",
    "        }\n",
    "\n",
    "def load_env() -> tuple:\n",
    "    \"\"\"\n",
    "    Load and validate environment variables\n",
    "    Returns:\n",
    "        tuple: (api_key, pdf_doc_path)\n",
    "    Raises:\n",
    "        ConfigurationError: If required environment variables are missing\n",
    "    \"\"\"\n",
    "    try:\n",
    "        load_dotenv(verbose=True)\n",
    "        \n",
    "        required_vars = {\n",
    "            'OPENAI_API_KEY': 'OpenAI API key',\n",
    "            'PDF_DOC_PATH': 'PDF document path'\n",
    "        }\n",
    "        \n",
    "        env_vars = {}\n",
    "        for var, description in required_vars.items():\n",
    "            value = os.getenv(var)\n",
    "            if not value:\n",
    "                raise ConfigurationError(\n",
    "                    message=f\"Missing {description}\",\n",
    "                    config_key=var\n",
    "                )\n",
    "            env_vars[var] = value\n",
    "            \n",
    "        # Validate PDF path exists\n",
    "        pdf_path = Path(env_vars['PDF_DOC_PATH'])\n",
    "        if not pdf_path.exists():\n",
    "            raise ConfigurationError(\n",
    "                message=f\"PDF file does not exist: {pdf_path}\",\n",
    "                config_key='PDF_DOC_PATH'\n",
    "            )\n",
    "        \n",
    "        logger.info(\"Environment variables loaded successfully\")\n",
    "        return env_vars['OPENAI_API_KEY'], str(pdf_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ConfigurationError(\n",
    "                message=f\"Environment loading failed: {str(e)}\"\n",
    "            )\n",
    "        e.log_error()\n",
    "        raise\n",
    "\n",
    "class MetricsTracker:\n",
    "    \"\"\"Track performance metrics for the chatbot\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        \n",
    "    def record_duration(self, operation: str, duration: float):\n",
    "        \"\"\"Record duration of an operation\"\"\"\n",
    "        self.metrics[f\"{operation}_duration\"].append(duration)\n",
    "        logger.debug(f\"{operation} took {duration:.2f} seconds\")\n",
    "    \n",
    "    def record_error(self, error: ChatbotError):\n",
    "        \"\"\"Record an error\"\"\"\n",
    "        self.metrics[\"errors\"].append(error.to_dict())\n",
    "        error.log_error()\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get statistical summary of metrics\"\"\"\n",
    "        stats = {}\n",
    "        for metric, values in self.metrics.items():\n",
    "            if metric.endswith('_duration'):\n",
    "                if values:\n",
    "                    stats[metric] = {\n",
    "                        'mean': sum(values) / len(values),\n",
    "                        'min': min(values),\n",
    "                        'max': max(values),\n",
    "                        'count': len(values)\n",
    "                    }\n",
    "            elif metric == \"errors\":\n",
    "                stats[\"error_count\"] = len(values)\n",
    "                stats[\"error_types\"] = defaultdict(int)\n",
    "                for error in values:\n",
    "                    stats[\"error_types\"][error[\"error_type\"]] += 1\n",
    "                    \n",
    "        return stats\n",
    "\n",
    "# Global instances\n",
    "config = Config()\n",
    "metrics = MetricsTracker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing and Vector Store Creation System\n",
    "\n",
    "This section implements a sophisticated document processing pipeline that converts raw text into searchable vector embeddings. Let's examine the complete implementation:\n",
    "\n",
    "### Document Loading System:\n",
    "\n",
    "1. **File Processing Pipeline:**\n",
    "   - **Input Validation:**\n",
    "     * File existence verification\n",
    "     * Format checking\n",
    "     * Permission validation\n",
    "     * Size limitations\n",
    "   \n",
    "   - **Loading Strategy:**\n",
    "     * Format-specific loaders\n",
    "     * Chunk management\n",
    "     * Memory optimization\n",
    "     * Error handling\n",
    "\n",
    "2. **Retry Mechanism:**\n",
    "   - **Implementation:**\n",
    "     * Configurable retry count\n",
    "     * Exponential backoff\n",
    "     * Failure tracking\n",
    "     * Success verification\n",
    "   \n",
    "   - **Error Management:**\n",
    "     * Detailed error capture\n",
    "     * Recovery attempts\n",
    "     * Resource cleanup\n",
    "     * State restoration\n",
    "\n",
    "### TextProcessor Class Architecture:\n",
    "\n",
    "1. **Initialization Components:**\n",
    "   - **Text Splitter Setup:**\n",
    "     * Chunk size configuration\n",
    "     * Overlap management\n",
    "     * Token limits\n",
    "     * Format handling\n",
    "   \n",
    "   - **Embedding Configuration:**\n",
    "     * Model selection\n",
    "     * Parameter optimization\n",
    "     * Cache initialization\n",
    "     * Resource allocation\n",
    "\n",
    "2. **Document Processing:**\n",
    "   - **Splitting Logic:**\n",
    "     * Content segmentation\n",
    "     * Context preservation\n",
    "     * Token management\n",
    "     * Format maintenance\n",
    "   \n",
    "   - **Embedding Generation:**\n",
    "     * Vector creation\n",
    "     * Dimension management\n",
    "     * Quality verification\n",
    "     * Performance optimization\n",
    "\n",
    "### Vector Store Implementation:\n",
    "\n",
    "1. **Chroma Database Setup:**\n",
    "   - **Initialization:**\n",
    "     * Index creation\n",
    "     * Storage configuration\n",
    "     * Performance tuning\n",
    "     * Backup strategy\n",
    "   \n",
    "   - **Management:**\n",
    "     * Data persistence\n",
    "     * Index optimization\n",
    "     * Query preparation\n",
    "     * Resource efficiency\n",
    "\n",
    "2. **Performance Features:**\n",
    "   - **Caching System:**\n",
    "     * Result caching\n",
    "     * Cache invalidation\n",
    "     * Memory management\n",
    "     * Performance monitoring\n",
    "   \n",
    "   - **Batch Processing:**\n",
    "     * Chunk batching\n",
    "     * Parallel processing\n",
    "     * Resource allocation\n",
    "     * Error handling\n",
    "\n",
    "### Metrics and Monitoring:\n",
    "\n",
    "1. **Performance Tracking:**\n",
    "   - **Time Measurements:**\n",
    "     * Processing duration\n",
    "     * Operation latency\n",
    "     * System overhead\n",
    "     * Resource usage\n",
    "   \n",
    "   - **Quality Metrics:**\n",
    "     * Embedding accuracy\n",
    "     * Processing success rate\n",
    "     * Error frequency\n",
    "     * Resource efficiency\n",
    "\n",
    "2. **Statistical Analysis:**\n",
    "   - **Data Collection:**\n",
    "     * Operation timing\n",
    "     * Resource utilization\n",
    "     * Error patterns\n",
    "     * System health\n",
    "\n",
    "### Error Management:\n",
    "\n",
    "1. **Error Categories:**\n",
    "   - Document loading failures\n",
    "   - Processing errors\n",
    "   - Resource constraints\n",
    "   - System limitations\n",
    "\n",
    "2. **Recovery Mechanisms:**\n",
    "   - Retry strategies\n",
    "   - Alternative processing\n",
    "   - Resource reallocation\n",
    "   - State recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file_path: str) -> List:\n",
    "    \"\"\"\n",
    "    Load and process document based on file type with enhanced error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the document\n",
    "    Returns:\n",
    "        List: Loaded document pages\n",
    "    Raises:\n",
    "        DocumentLoadError: If document loading fails\n",
    "    \"\"\"\n",
    "    start_time = time()\n",
    "    try:\n",
    "        logger.info(f\"Starting document load: {file_path}\")\n",
    "        file_path = Path(file_path)\n",
    "        \n",
    "        # Validate file existence and permissions\n",
    "        if not file_path.exists():\n",
    "            raise DocumentLoadError(\n",
    "                message=\"File does not exist\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        if not file_path.is_file():\n",
    "            raise DocumentLoadError(\n",
    "                message=\"Path is not a file\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "            \n",
    "        # Select appropriate loader based on file extension\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            loader = PyPDFLoader(str(file_path))\n",
    "        else:\n",
    "            raise DocumentLoadError(\n",
    "                message=f\"Unsupported file type: {file_path.suffix}\",\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        \n",
    "        # Load document with retries\n",
    "        max_retries = config.get('MAX_RETRIES')\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                documents = loader.load()\n",
    "                \n",
    "                # Log success metrics\n",
    "                duration = time() - start_time\n",
    "                metrics.record_duration('document_load', duration)\n",
    "                logger.info(\n",
    "                    f\"Successfully loaded document with {len(documents)} pages in {duration:.2f} seconds\"\n",
    "                )\n",
    "                return documents\n",
    "                \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    logger.warning(f\"Retry {attempt + 1}/{max_retries} failed: {str(e)}\")\n",
    "                    continue\n",
    "                raise DocumentLoadError(\n",
    "                    message=f\"Failed after {max_retries} attempts: {str(e)}\",\n",
    "                    file_path=str(file_path)\n",
    "                )\n",
    "                \n",
    "    except Exception as e:\n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = DocumentLoadError(\n",
    "                message=str(e),\n",
    "                file_path=str(file_path)\n",
    "            )\n",
    "        metrics.record_error(e)\n",
    "        raise\n",
    "\n",
    "class TextProcessor:\n",
    "    \"\"\"Handles text splitting and embedding creation with enhanced error handling and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.text_splitter = CharacterTextSplitter(\n",
    "                chunk_size=config.get('CHUNK_SIZE'),\n",
    "                chunk_overlap=config.get('CHUNK_OVERLAP')\n",
    "            )\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            self._cache = {}\n",
    "            self._stats = defaultdict(int)\n",
    "            logger.info(\"TextProcessor initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ConfigurationError(\n",
    "                message=f\"Failed to initialize TextProcessor: {str(e)}\"\n",
    "            )\n",
    "    \n",
    "    def _split_documents(self, documents: List) -> List:\n",
    "        \"\"\"\n",
    "        Split documents into chunks with error handling\n",
    "        \n",
    "        Args:\n",
    "            documents (List): List of documents to split\n",
    "        Returns:\n",
    "            List: Split text chunks\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            texts = self.text_splitter.split_documents(documents)\n",
    "            duration = time() - start_time\n",
    "            metrics.record_duration('text_splitting', duration)\n",
    "            self._stats['total_chunks'] = len(texts)\n",
    "            logger.info(f\"Split documents into {len(texts)} chunks in {duration:.2f} seconds\")\n",
    "            return texts\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = ModelError(\n",
    "                message=f\"Failed to split documents: {str(e)}\",\n",
    "                model_name=\"text_splitter\"\n",
    "            )\n",
    "            metrics.record_error(error)\n",
    "            raise error\n",
    "    \n",
    "    def process_documents(self, documents: List) -> Chroma:\n",
    "        \"\"\"\n",
    "        Process documents into embeddings and store in Chroma\n",
    "        \n",
    "        Args:\n",
    "            documents (List): List of document pages\n",
    "        Returns:\n",
    "            Chroma: Vector store with processed documents\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            logger.info(\"Starting document processing\")\n",
    "            \n",
    "            # Split documents\n",
    "            texts = self._split_documents(documents)\n",
    "            \n",
    "            # Create vector store\n",
    "            logger.info(\"Creating vector store\")\n",
    "            db = Chroma.from_documents(texts, self.embeddings)\n",
    "            \n",
    "            # Record success metrics\n",
    "            duration = time() - start_time\n",
    "            metrics.record_duration('document_processing', duration)\n",
    "            self._stats['processing_time'] = duration\n",
    "            \n",
    "            logger.info(\n",
    "                f\"Successfully created vector store in {duration:.2f} seconds. \"\n",
    "                f\"Stats: {json.dumps(self._stats, indent=2)}\"\n",
    "            )\n",
    "            return db\n",
    "            \n",
    "        except Exception as e:\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                e = ModelError(\n",
    "                    message=f\"Failed to process documents: {str(e)}\",\n",
    "                    model_name=\"document_processor\"\n",
    "                )\n",
    "            metrics.record_error(e)\n",
    "            raise e\n",
    "            \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get processing statistics\"\"\"\n",
    "        return {\n",
    "            'stats': self._stats,\n",
    "            'cache_size': len(self._cache),\n",
    "            'embedding_model': config.get('EMBEDDING_MODEL')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Limiting and Question-Answering System\n",
    "\n",
    "This section implements the core QA functionality with sophisticated rate limiting protection. Let's examine each component:\n",
    "\n",
    "### Rate Limiting Decorator:\n",
    "1. **Architecture:**\n",
    "  - Rolling window implementation\n",
    "  - Function-specific tracking\n",
    "  - Automated cleanup\n",
    "  - Performance monitoring\n",
    "\n",
    "2. **Features:**\n",
    "  - Configurable rate limits\n",
    "  - Per-function restrictions\n",
    "  - Timestamp management\n",
    "  - Violation tracking\n",
    "\n",
    "3. **Error Management:**\n",
    "  - Custom error generation\n",
    "  - Metric recording\n",
    "  - Violation logging\n",
    "  - Recovery handling\n",
    "\n",
    "### QASystem Class Components:\n",
    "\n",
    "1. **Initialization Framework:**\n",
    "  - Model configuration\n",
    "  - Chain setup\n",
    "  - Retriever integration\n",
    "  - History management\n",
    "\n",
    "2. **Query Processing Pipeline:**\n",
    "  - Input validation\n",
    "  - Rate limiting\n",
    "  - Context retrieval\n",
    "  - Response generation\n",
    "\n",
    "3. **Response Management:**\n",
    "  - Format standardization\n",
    "  - History recording\n",
    "  - Performance tracking\n",
    "  - Error handling\n",
    "\n",
    "### Performance Monitoring:\n",
    "\n",
    "1. **Metric Collection:**\n",
    "  - Query timing\n",
    "  - Success rates\n",
    "  - Resource usage\n",
    "  - Error frequency\n",
    "\n",
    "2. **Statistical Analysis:**\n",
    "  - Average response time\n",
    "  - Error patterns\n",
    "  - Resource utilization\n",
    "  - System health\n",
    "\n",
    "### History Management:\n",
    "\n",
    "1. **Query Tracking:**\n",
    "  - Timestamp recording\n",
    "  - Duration monitoring\n",
    "  - Context preservation\n",
    "  - Pattern analysis\n",
    "\n",
    "2. **Historical Analysis:**\n",
    "  - Performance trends\n",
    "  - Usage patterns\n",
    "  - Error correlations\n",
    "  - System optimization\n",
    "\n",
    "### Implementation Features:\n",
    "\n",
    "1. **Rate Control:**\n",
    "  - Dynamic limiting\n",
    "  - Adaptive thresholds\n",
    "  - Load balancing\n",
    "  - Resource protection\n",
    "\n",
    "2. **Error Handling:**\n",
    "  - Custom error types\n",
    "  - Context preservation\n",
    "  - Recovery strategies\n",
    "  - User notification\n",
    "\n",
    "3. **Performance Optimization:**\n",
    "  - Response caching\n",
    "  - Resource management\n",
    "  - Load distribution\n",
    "  - System monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limit(max_per_second: int):\n",
    "    \"\"\"\n",
    "    Enhanced rate limiting decorator with metrics tracking\n",
    "    \n",
    "    Args:\n",
    "        max_per_second (int): Maximum number of calls allowed per second\n",
    "    \"\"\"\n",
    "    calls = defaultdict(list)\n",
    "    \n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            now = time()\n",
    "            func_name = func.__name__\n",
    "            \n",
    "            # Clean old calls\n",
    "            calls[func_name] = [c for c in calls[func_name] if c > now - 1]\n",
    "            \n",
    "            # Check rate limit\n",
    "            if len(calls[func_name]) >= max_per_second:\n",
    "                error = ChatbotError(\n",
    "                    message=f\"Rate limit of {max_per_second} calls per second exceeded for {func_name}\",\n",
    "                    error_code=\"RATE_LIMIT_ERROR\"\n",
    "                )\n",
    "                metrics.record_error(error)\n",
    "                raise error\n",
    "                \n",
    "            # Record call\n",
    "            calls[func_name].append(now)\n",
    "            \n",
    "            # Execute function\n",
    "            try:\n",
    "                result = func(*args, **kwargs)\n",
    "                metrics.record_duration(func_name, time() - now)\n",
    "                return result\n",
    "            except Exception as e:\n",
    "                if not isinstance(e, ChatbotError):\n",
    "                    e = ModelError(\n",
    "                        message=str(e),\n",
    "                        model_name=func_name\n",
    "                    )\n",
    "                metrics.record_error(e)\n",
    "                raise e\n",
    "                \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "class QASystem:\n",
    "    \"\"\"Enhanced question-answering system with metrics and error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: Chroma):\n",
    "        \"\"\"Initialize QA system with vector store\"\"\"\n",
    "        try:\n",
    "            self.qa_chain = RetrievalQA.from_chain_type(\n",
    "                llm=OpenAI(temperature=config.get('MODEL_TEMPERATURE')),\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=vector_store.as_retriever()\n",
    "            )\n",
    "            self.query_history = []\n",
    "            logger.info(\"QA system initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error = ModelError(\n",
    "                message=f\"Failed to initialize QA system: {str(e)}\",\n",
    "                model_name=\"QASystem\"\n",
    "            )\n",
    "            metrics.record_error(error)\n",
    "            raise error\n",
    "    \n",
    "    @rate_limit(max_per_second=5)\n",
    "    def get_response(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Get response for user query with enhanced error handling\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question\n",
    "        Returns:\n",
    "            str: Generated response\n",
    "        \"\"\"\n",
    "        start_time = time()\n",
    "        try:\n",
    "            # Validate query\n",
    "            if not query or not query.strip():\n",
    "                raise ModelError(\n",
    "                    message=\"Empty query provided\",\n",
    "                    query=query\n",
    "                )\n",
    "            \n",
    "            # Process query\n",
    "            response = self.qa_chain.invoke({\"query\": query})\n",
    "            \n",
    "            # Record successful query\n",
    "            duration = time() - start_time\n",
    "            self.query_history.append({\n",
    "                'query': query,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'duration': duration\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"Query processed successfully in {duration:.2f} seconds\")\n",
    "            return response['result'] if isinstance(response, dict) else response\n",
    "            \n",
    "        except Exception as e:\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                e = ModelError(\n",
    "                    message=f\"Failed to process query: {str(e)}\",\n",
    "                    model_name=\"QASystem\",\n",
    "                    query=query\n",
    "                )\n",
    "            metrics.record_error(e)\n",
    "            raise e\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get QA system statistics\"\"\"\n",
    "        return {\n",
    "            'total_queries': len(self.query_history),\n",
    "            'average_duration': sum(q['duration'] for q in self.query_history) / len(self.query_history) if self.query_history else 0,\n",
    "            'query_history': self.query_history[-10:]  # Last 10 queries\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Interface and User Interaction System\n",
    "\n",
    "This section implements a sophisticated conversational interface using Gradio, managing user interactions and system responses. Let's examine each component:\n",
    "\n",
    "### Chat Interface Architecture:\n",
    "\n",
    "1. **State Management:**\n",
    "   - Conversation history tracking\n",
    "   - Session metrics collection \n",
    "   - Interface statistics\n",
    "   - Performance monitoring\n",
    "   \n",
    "2. **Interaction Components:**\n",
    "   - Message processing pipeline\n",
    "   - Response generation\n",
    "   - History updates\n",
    "   - Error management\n",
    "\n",
    "3. **Metric Collection:**\n",
    "   - Interaction counting\n",
    "   - Error tracking\n",
    "   - Response timing\n",
    "   - Resource usage\n",
    "\n",
    "### User Interface Components:\n",
    "\n",
    "1. **Display Elements:**\n",
    "   - Chat history display\n",
    "   - Input field configuration\n",
    "   - Control button layout \n",
    "   - Statistics panel\n",
    "\n",
    "2. **Interactive Features:**\n",
    "   - Real-time message processing\n",
    "   - Dynamic history updates\n",
    "   - Error display handling\n",
    "   - System status indicators\n",
    "\n",
    "3. **Control Elements:**\n",
    "   - Conversation reset\n",
    "   - Statistics display\n",
    "   - System monitoring\n",
    "   - Error recovery\n",
    "\n",
    "### Response Processing:\n",
    "\n",
    "1. **Message Handling:**\n",
    "   - Input validation\n",
    "   - Context management\n",
    "   - Response generation\n",
    "   - History updates\n",
    "\n",
    "2. **Error Management:**\n",
    "   - Error detection\n",
    "   - User notification\n",
    "   - Recovery procedures\n",
    "   - State preservation\n",
    "\n",
    "3. **Performance Tracking:**\n",
    "   - Response timing\n",
    "   - Resource monitoring\n",
    "   - Error frequency\n",
    "   - System health\n",
    "\n",
    "### Interface Features:\n",
    "\n",
    "1. **Conversation Management:**\n",
    "   - History preservation\n",
    "   - Context maintenance\n",
    "   - State recovery\n",
    "   - Session tracking\n",
    "\n",
    "2. **User Experience:**\n",
    "   - Responsive updates\n",
    "   - Clear feedback\n",
    "   - Error notifications\n",
    "   - Status indicators\n",
    "\n",
    "3. **System Integration:**\n",
    "   - QA system connection\n",
    "   - Metric collection\n",
    "   - Error propagation\n",
    "   - Resource management\n",
    "\n",
    "### Data Management:\n",
    "\n",
    "1. **History Storage:**\n",
    "   - Message archiving\n",
    "   - Context preservation\n",
    "   - Timestamp tracking\n",
    "   - State management\n",
    "\n",
    "2. **Statistics Collection:**\n",
    "   - Usage metrics\n",
    "   - Error tracking\n",
    "   - Performance data\n",
    "   - System health\n",
    "\n",
    "### Interface Design:\n",
    "\n",
    "1. **Layout Organization:**\n",
    "   - Clear structure\n",
    "   - Intuitive controls\n",
    "   - Status indicators\n",
    "   - Error displays\n",
    "\n",
    "2. **User Interaction:**\n",
    "   - Immediate feedback\n",
    "   - Clear messaging\n",
    "   - Error handling\n",
    "   - Status updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatInterface:\n",
    "    \"\"\"Enhanced chat interface with error handling and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, qa_system: QASystem):\n",
    "        self.qa_system = qa_system\n",
    "        self.conversation_history = []\n",
    "        self.interface_metrics = {\n",
    "            'total_interactions': 0,\n",
    "            'error_count': 0,\n",
    "            'start_time': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def respond(self, message: str, history: List) -> tuple:\n",
    "        \"\"\"\n",
    "        Process user message with enhanced error handling\n",
    "        \n",
    "        Args:\n",
    "            message (str): User's message\n",
    "            history (List): Conversation history\n",
    "        Returns:\n",
    "            tuple: (message, updated_history)\n",
    "        \"\"\"\n",
    "        self.interface_metrics['total_interactions'] += 1\n",
    "        \n",
    "        try:\n",
    "            response = self.qa_system.get_response(message)\n",
    "            self.conversation_history.append({\n",
    "                'user_message': message,\n",
    "                'bot_response': response,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "            return response, history + [[message, response]]\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.interface_metrics['error_count'] += 1\n",
    "            error_message = f\"Error: {str(e)}\"\n",
    "            if not isinstance(e, ChatbotError):\n",
    "                metrics.record_error(ChatbotError(error_message))\n",
    "            return error_message, history + [[message, error_message]]\n",
    "    \n",
    "    def create_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Create enhanced Gradio interface\"\"\"\n",
    "        with gr.Blocks() as demo:\n",
    "            # Header\n",
    "            gr.Markdown(\"# Nestlé HR Chatbot\")\n",
    "            \n",
    "            # Chat interface\n",
    "            chatbot = gr.Chatbot()\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Ask about Nestle's HR policies\",\n",
    "                placeholder=\"Type your question here...\",\n",
    "                show_label=True\n",
    "            )\n",
    "            \n",
    "            # Control buttons\n",
    "            with gr.Row():\n",
    "                clear = gr.Button(\"Clear Conversation\")\n",
    "                show_stats = gr.Button(\"Show Statistics\")\n",
    "            \n",
    "            # Statistics display\n",
    "            stats_output = gr.JSON(label=\"System Statistics\", visible=False)\n",
    "            \n",
    "            # Event handlers\n",
    "            msg.submit(\n",
    "                self.respond,\n",
    "                [msg, chatbot],\n",
    "                [msg, chatbot]\n",
    "            )\n",
    "            \n",
    "            clear.click(\n",
    "                lambda: (None, None),\n",
    "                None,\n",
    "                [msg, chatbot],\n",
    "                queue=False\n",
    "            )\n",
    "            \n",
    "            show_stats.click(\n",
    "                lambda: {\n",
    "                    'interface': self.interface_metrics,\n",
    "                    'qa_system': self.qa_system.get_stats(),\n",
    "                    'overall_metrics': metrics.get_statistics()\n",
    "                },\n",
    "                None,\n",
    "                stats_output\n",
    "            )\n",
    "            \n",
    "        return demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application State Management and Initialization\n",
    "\n",
    "This section orchestrates the overall application lifecycle, managing state, initialization, and system health. Let's examine each component:\n",
    "\n",
    "### Application State Manager:\n",
    "\n",
    "1. **State Tracking:**\n",
    "   - Component initialization status\n",
    "   - System health monitoring\n",
    "   - Resource allocation\n",
    "   - Runtime metrics\n",
    "   \n",
    "2. **Health Management:**\n",
    "   - Status updates\n",
    "   - Health checks\n",
    "   - Error detection\n",
    "   - Recovery procedures\n",
    "\n",
    "3. **Component Registry:**\n",
    "   - Initialization tracking\n",
    "   - Dependency management\n",
    "   - Resource allocation\n",
    "   - State preservation\n",
    "\n",
    "### Initialization Process:\n",
    "\n",
    "1. **Environment Setup:**\n",
    "   - Configuration loading\n",
    "   - Variable validation\n",
    "   - Resource allocation\n",
    "   - System preparation\n",
    "\n",
    "2. **Component Initialization:**\n",
    "   - Sequential startup\n",
    "   - Dependency resolution\n",
    "   - State verification\n",
    "   - Error handling\n",
    "\n",
    "3. **Resource Management:**\n",
    "   - Memory allocation\n",
    "   - Connection establishment\n",
    "   - Cache initialization\n",
    "   - System optimization\n",
    "\n",
    "### Health Monitoring:\n",
    "\n",
    "1. **Status Tracking:**\n",
    "   - Component health\n",
    "   - System performance\n",
    "   - Resource usage\n",
    "   - Error conditions\n",
    "\n",
    "2. **Metrics Collection:**\n",
    "   - Performance data\n",
    "   - Resource utilization\n",
    "   - Error frequency\n",
    "   - System statistics\n",
    "\n",
    "### Error Management:\n",
    "\n",
    "1. **Error Detection:**\n",
    "   - Component failures\n",
    "   - Resource issues\n",
    "   - System problems\n",
    "   - Performance degradation\n",
    "\n",
    "2. **Recovery Procedures:**\n",
    "   - Error handling\n",
    "   - State recovery\n",
    "   - Resource cleanup\n",
    "   - System restoration\n",
    "\n",
    "### System Features:\n",
    "\n",
    "1. **State Management:**\n",
    "   - Comprehensive tracking\n",
    "   - Status monitoring\n",
    "   - Resource oversight\n",
    "   - Performance analysis\n",
    "\n",
    "2. **Health Reporting:**\n",
    "   - Status updates\n",
    "   - Error notifications\n",
    "   - Performance metrics\n",
    "   - System analytics\n",
    "\n",
    "3. **Resource Control:**\n",
    "   - Allocation management\n",
    "   - Usage monitoring\n",
    "   - Optimization\n",
    "   - Cleanup procedures\n",
    "\n",
    "### Implementation Benefits:\n",
    "\n",
    "1. **System Reliability:**\n",
    "   - Consistent state tracking\n",
    "   - Error detection\n",
    "   - Recovery mechanisms\n",
    "   - Health monitoring\n",
    "\n",
    "2. **Performance Optimization:**\n",
    "   - Resource management\n",
    "   - Usage tracking\n",
    "   - System tuning\n",
    "   - Performance metrics\n",
    "\n",
    "3. **Maintenance Support:**\n",
    "   - Status reporting\n",
    "   - Error tracking\n",
    "   - System analytics\n",
    "   - Resource monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:57:54,867 - __main__ - INFO - [2334330675.py:43] - Starting application initialization\n",
      "2024-12-29 18:57:54,867 - __main__ - INFO - [2334330675.py:46] - Loading environment and configuration\n",
      "2024-12-29 18:57:54,869 - __main__ - INFO - [465277660.py:92] - Environment variables loaded successfully\n",
      "2024-12-29 18:57:54,869 - __main__ - INFO - [465277660.py:33] - Configuration loaded successfully\n",
      "2024-12-29 18:57:54,869 - __main__ - INFO - [2334330675.py:52] - Loading documents\n",
      "2024-12-29 18:57:54,869 - __main__ - INFO - [5827190.py:14] - Starting document load: dataset/the_nestle_hr_policy_pdf_2012.pdf\n",
      "2024-12-29 18:57:55,133 - __main__ - INFO - [5827190.py:47] - Successfully loaded document with 8 pages in 0.26 seconds\n",
      "2024-12-29 18:57:55,133 - __main__ - INFO - [2334330675.py:57] - Initializing text processor\n",
      "2024-12-29 18:57:55,144 - __main__ - INFO - [5827190.py:82] - TextProcessor initialized successfully\n",
      "2024-12-29 18:57:55,145 - __main__ - INFO - [5827190.py:126] - Starting document processing\n",
      "2024-12-29 18:57:55,145 - __main__ - INFO - [5827190.py:104] - Split documents into 7 chunks in 0.00 seconds\n",
      "2024-12-29 18:57:55,145 - __main__ - INFO - [5827190.py:132] - Creating vector store\n",
      "2024-12-29 18:57:55,551 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:57:55,674 - __main__ - INFO - [5827190.py:140] - Successfully created vector store in 0.53 seconds. Stats: {\n",
      "  \"total_chunks\": 7,\n",
      "  \"processing_time\": 0.5295591354370117\n",
      "}\n",
      "2024-12-29 18:57:55,675 - __main__ - INFO - [2334330675.py:64] - Initializing QA system\n",
      "2024-12-29 18:57:55,693 - __main__ - INFO - [3138667305.py:60] - QA system initialized successfully\n",
      "2024-12-29 18:57:55,693 - __main__ - INFO - [2334330675.py:69] - Setting up chat interface\n",
      "2024-12-29 18:57:55,693 - __main__ - INFO - [2334330675.py:20] - Health status updated: healthy - Application initialized successfully\n",
      "2024-12-29 18:57:55,694 - __main__ - INFO - [2334330675.py:77] - Application initialization completed successfully\n",
      "/opt/homebrew/anaconda3/envs/1720690009_20241228_v2/lib/python3.10/site-packages/gradio/components/chatbot.py:242: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n",
      "2024-12-29 18:57:55,812 - httpx - INFO - [_client.py:1025] - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:57:55,816 - httpx - INFO - [_client.py:1025] - HTTP Request: HEAD http://127.0.0.1:7861/ \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:57:55,819 - __main__ - INFO - [2334330675.py:125] - Application started successfully\n",
      "{\n",
      "  \"health\": {\n",
      "    \"status\": \"healthy\",\n",
      "    \"last_check\": \"2024-12-29T18:57:55.693949\",\n",
      "    \"message\": \"Application initialized successfully\"\n",
      "  },\n",
      "  \"uptime_seconds\": 0.95167,\n",
      "  \"initialized\": true,\n",
      "  \"components\": [\n",
      "    \"config\",\n",
      "    \"documents\",\n",
      "    \"text_processor\",\n",
      "    \"vector_store\",\n",
      "    \"qa_system\",\n",
      "    \"interface\"\n",
      "  ],\n",
      "  \"config\": {\n",
      "    \"config\": {\n",
      "      \"CHUNK_SIZE\": 1000,\n",
      "      \"CHUNK_OVERLAP\": 0,\n",
      "      \"MODEL_TEMPERATURE\": 0,\n",
      "      \"EMBEDDING_MODEL\": \"text-embedding-ada-002\",\n",
      "      \"MAX_RETRIES\": 3,\n",
      "      \"RATE_LIMIT_PER_SECOND\": 5,\n",
      "      \"LOG_LEVEL\": 20\n",
      "    },\n",
      "    \"load_time\": \"2024-12-29T18:57:54.789042\",\n",
      "    \"age_seconds\": 1.029908\n",
      "  },\n",
      "  \"metrics\": {\n",
      "    \"document_load_duration\": {\n",
      "      \"mean\": 0.2634608745574951,\n",
      "      \"min\": 0.2634608745574951,\n",
      "      \"max\": 0.2634608745574951,\n",
      "      \"count\": 1\n",
      "    },\n",
      "    \"text_splitting_duration\": {\n",
      "      \"mean\": 8.893013000488281e-05,\n",
      "      \"min\": 8.893013000488281e-05,\n",
      "      \"max\": 8.893013000488281e-05,\n",
      "      \"count\": 1\n",
      "    },\n",
      "    \"document_processing_duration\": {\n",
      "      \"mean\": 0.5295591354370117,\n",
      "      \"min\": 0.5295591354370117,\n",
      "      \"max\": 0.5295591354370117,\n",
      "      \"count\": 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "2024-12-29 18:57:55,819 - __main__ - INFO - [2334330675.py:93] - Cleaning up application resources\n",
      "2024-12-29 18:57:55,819 - __main__ - INFO - [2334330675.py:100] - Application shutdown complete\n"
     ]
    }
   ],
   "source": [
    "class ApplicationState:\n",
    "    \"\"\"Manages global application state and initialization\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.startup_time = datetime.now()\n",
    "        self.initialized = False\n",
    "        self.components = {}\n",
    "        self.health_status = {\n",
    "            'status': 'initializing',\n",
    "            'last_check': self.startup_time.isoformat()\n",
    "        }\n",
    "    \n",
    "    def update_health(self, status: str, message: str = None):\n",
    "        \"\"\"Update application health status\"\"\"\n",
    "        self.health_status.update({\n",
    "            'status': status,\n",
    "            'last_check': datetime.now().isoformat(),\n",
    "            'message': message\n",
    "        })\n",
    "        logger.info(f\"Health status updated: {status} - {message}\")\n",
    "    \n",
    "    def get_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get complete application status\"\"\"\n",
    "        return {\n",
    "            'health': self.health_status,\n",
    "            'uptime_seconds': (datetime.now() - self.startup_time).total_seconds(),\n",
    "            'initialized': self.initialized,\n",
    "            'components': list(self.components.keys()),\n",
    "            'config': config.to_dict(),\n",
    "            'metrics': metrics.get_statistics()\n",
    "        }\n",
    "\n",
    "def initialize_application() -> ApplicationState:\n",
    "    \"\"\"\n",
    "    Initialize all application components with comprehensive error handling\n",
    "    \n",
    "    Returns:\n",
    "        ApplicationState: Application state manager\n",
    "    \"\"\"\n",
    "    app_state = ApplicationState()\n",
    "    \n",
    "    try:\n",
    "        logger.info(\"Starting application initialization\")\n",
    "        \n",
    "        # Load environment variables and configuration\n",
    "        logger.info(\"Loading environment and configuration\")\n",
    "        api_key, pdf_path = load_env()\n",
    "        config.load_from_env()\n",
    "        app_state.components['config'] = config\n",
    "        \n",
    "        # Load and process documents\n",
    "        logger.info(\"Loading documents\")\n",
    "        documents = load_document(pdf_path)\n",
    "        app_state.components['documents'] = f\"Loaded {len(documents)} pages\"\n",
    "        \n",
    "        # Initialize text processor\n",
    "        logger.info(\"Initializing text processor\")\n",
    "        processor = TextProcessor()\n",
    "        vector_store = processor.process_documents(documents)\n",
    "        app_state.components['text_processor'] = processor\n",
    "        app_state.components['vector_store'] = vector_store\n",
    "        \n",
    "        # Initialize QA system\n",
    "        logger.info(\"Initializing QA system\")\n",
    "        qa_system = QASystem(vector_store)\n",
    "        app_state.components['qa_system'] = qa_system\n",
    "        \n",
    "        # Initialize chat interface\n",
    "        logger.info(\"Setting up chat interface\")\n",
    "        interface = ChatInterface(qa_system)\n",
    "        app_state.components['interface'] = interface\n",
    "        \n",
    "        # Mark initialization as complete\n",
    "        app_state.initialized = True\n",
    "        app_state.update_health('healthy', 'Application initialized successfully')\n",
    "        \n",
    "        logger.info(\"Application initialization completed successfully\")\n",
    "        return app_state\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Application initialization failed: {str(e)}\"\n",
    "        logger.error(error_message)\n",
    "        app_state.update_health('error', error_message)\n",
    "        \n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ChatbotError(error_message)\n",
    "        metrics.record_error(e)\n",
    "        raise\n",
    "\n",
    "def cleanup_resources():\n",
    "    \"\"\"Cleanup application resources\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Cleaning up application resources\")\n",
    "        # Add cleanup logic here (e.g., closing file handles, database connections)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during cleanup: {str(e)}\")\n",
    "        \n",
    "    finally:\n",
    "        logger.info(\"Application shutdown complete\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main application entry point with enhanced error handling and monitoring\"\"\"\n",
    "    app_state = None\n",
    "    \n",
    "    try:\n",
    "        # Initialize application\n",
    "        app_state = initialize_application()\n",
    "        \n",
    "        # Create and launch interface\n",
    "        demo = app_state.components['interface'].create_interface()\n",
    "        \n",
    "        # Configure shutdown handling\n",
    "        def handle_shutdown():\n",
    "            logger.info(\"Shutdown initiated\")\n",
    "            cleanup_resources()\n",
    "        \n",
    "        # Launch interface with shutdown handling\n",
    "        demo.launch(\n",
    "            share=False,  # Set to True to create a public URL\n",
    "            prevent_thread_lock=True\n",
    "        )\n",
    "        \n",
    "        # Log successful startup\n",
    "        logger.info(\n",
    "            \"Application started successfully\\n\" +\n",
    "            json.dumps(app_state.get_status(), indent=2)\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_message = f\"Application startup failed: {str(e)}\"\n",
    "        logger.error(error_message)\n",
    "        \n",
    "        if app_state:\n",
    "            app_state.update_health('error', error_message)\n",
    "        \n",
    "        if not isinstance(e, ChatbotError):\n",
    "            e = ChatbotError(error_message)\n",
    "        metrics.record_error(e)\n",
    "        \n",
    "        raise\n",
    "        \n",
    "    finally:\n",
    "        cleanup_resources()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Fatal error: {str(e)}\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and System Reporting Utilities\n",
    "\n",
    "This section implements comprehensive testing procedures and system reporting capabilities. Let's examine each component of the testing and reporting framework:\n",
    "\n",
    "### Test Runner System:\n",
    "\n",
    "1. **Environment Testing:**\n",
    "   - API key validation\n",
    "   - Path verification\n",
    "   - Configuration testing\n",
    "   - Permission checking\n",
    "\n",
    "2. **Document Processing Tests:**\n",
    "   - Loading verification\n",
    "   - Document processing\n",
    "   - Chunking validation\n",
    "   - Format handling\n",
    "\n",
    "3. **QA System Testing:**\n",
    "   - Query processing\n",
    "   - Response validation\n",
    "   - Rate limiting checks\n",
    "   - Error handling\n",
    "\n",
    "### System Reporting Framework:\n",
    "\n",
    "1. **Configuration Reporting:**\n",
    "   - Current settings\n",
    "   - Environment state\n",
    "   - System parameters\n",
    "   - Runtime configuration\n",
    "\n",
    "2. **Performance Analytics:**\n",
    "   - Operation timing\n",
    "   - Resource utilization\n",
    "   - Response latency\n",
    "   - System load\n",
    "\n",
    "3. **Error Analysis:**\n",
    "   - Error frequency\n",
    "   - Error patterns\n",
    "   - Impact assessment\n",
    "   - Resolution tracking\n",
    "\n",
    "### System Metrics:\n",
    "\n",
    "1. **Resource Monitoring:**\n",
    "   - Memory usage\n",
    "   - CPU utilization\n",
    "   - Storage metrics\n",
    "   - Network status\n",
    "\n",
    "2. **Performance Statistics:**\n",
    "   - Response times\n",
    "   - Processing speed\n",
    "   - Resource efficiency\n",
    "   - System throughput\n",
    "\n",
    "### Implementation Features:\n",
    "\n",
    "1. **Test Execution:**\n",
    "   - Sequential testing\n",
    "   - Dependency handling\n",
    "   - State verification\n",
    "   - Error capture\n",
    "\n",
    "2. **Report Generation:**\n",
    "   - Metrics compilation\n",
    "   - Statistics calculation\n",
    "   - Status summaries\n",
    "   - Performance analysis\n",
    "\n",
    "3. **System Analysis:**\n",
    "   - Trend identification\n",
    "   - Pattern recognition\n",
    "   - Performance evaluation\n",
    "   - Resource assessment\n",
    "\n",
    "### Testing Components:\n",
    "\n",
    "1. **Functionality Testing:**\n",
    "   - Core operations\n",
    "   - Error handling\n",
    "   - Recovery procedures\n",
    "   - State management\n",
    "\n",
    "2. **Performance Testing:**\n",
    "   - Response timing\n",
    "   - Resource usage\n",
    "   - System limits\n",
    "   - Load handling\n",
    "\n",
    "### Reporting Features:\n",
    "\n",
    "1. **System Status:**\n",
    "   - Component health\n",
    "   - Resource state\n",
    "   - Error conditions\n",
    "   - Performance metrics\n",
    "\n",
    "2. **Analysis Tools:**\n",
    "   - Statistical processing\n",
    "   - Trend analysis\n",
    "   - Pattern detection\n",
    "   - Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    \"\"\"Basic application tests\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting application tests\")\n",
    "        \n",
    "        # Test environment loading\n",
    "        api_key, pdf_path = load_env()\n",
    "        assert api_key, \"API key not loaded\"\n",
    "        assert pdf_path, \"PDF path not loaded\"\n",
    "        \n",
    "        # Test document loading\n",
    "        documents = load_document(pdf_path)\n",
    "        assert documents, \"Documents not loaded\"\n",
    "        \n",
    "        # Test text processing\n",
    "        processor = TextProcessor()\n",
    "        vector_store = processor.process_documents(documents)\n",
    "        assert vector_store, \"Vector store not created\"\n",
    "        \n",
    "        # Test QA system\n",
    "        qa_system = QASystem(vector_store)\n",
    "        test_query = \"What are the working hours?\"\n",
    "        response = qa_system.get_response(test_query)\n",
    "        assert response, \"No response received\"\n",
    "        \n",
    "        logger.info(\"All tests passed successfully\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def generate_system_report() -> Dict[str, Any]:\n",
    "    \"\"\"Generate comprehensive system report\"\"\"\n",
    "    return {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'configuration': config.to_dict(),\n",
    "        'metrics': metrics.get_statistics(),\n",
    "        'environment': {\n",
    "            'python_version': sys.version,\n",
    "            'platform': platform.platform(),\n",
    "            'memory_usage': psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        },\n",
    "        'log_summary': {\n",
    "            'error_count': len([r for r in metrics.metrics['errors']]),\n",
    "            'last_errors': metrics.metrics['errors'][-5:]  # Last 5 errors\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:57:55,832 - __main__ - INFO - [2109332838.py:4] - Starting application tests\n",
      "2024-12-29 18:57:55,834 - __main__ - INFO - [465277660.py:92] - Environment variables loaded successfully\n",
      "2024-12-29 18:57:55,834 - __main__ - INFO - [5827190.py:14] - Starting document load: dataset/the_nestle_hr_policy_pdf_2012.pdf\n",
      "2024-12-29 18:57:56,001 - __main__ - INFO - [5827190.py:47] - Successfully loaded document with 8 pages in 0.17 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "Performing run tests\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:57:56,012 - __main__ - INFO - [5827190.py:82] - TextProcessor initialized successfully\n",
      "2024-12-29 18:57:56,012 - __main__ - INFO - [5827190.py:126] - Starting document processing\n",
      "2024-12-29 18:57:56,012 - __main__ - INFO - [5827190.py:104] - Split documents into 7 chunks in 0.00 seconds\n",
      "2024-12-29 18:57:56,013 - __main__ - INFO - [5827190.py:132] - Creating vector store\n",
      "2024-12-29 18:57:56,270 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:57:56,341 - __main__ - INFO - [5827190.py:140] - Successfully created vector store in 0.33 seconds. Stats: {\n",
      "  \"total_chunks\": 7,\n",
      "  \"processing_time\": 0.3287811279296875\n",
      "}\n",
      "2024-12-29 18:57:56,358 - __main__ - INFO - [3138667305.py:60] - QA system initialized successfully\n",
      "2024-12-29 18:57:56,741 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:57:57,164 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:57:57,166 - __main__ - INFO - [3138667305.py:100] - Query processed successfully in 0.81 seconds\n",
      "2024-12-29 18:57:57,166 - __main__ - INFO - [2109332838.py:26] - All tests passed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:57:56,117 - httpx - INFO - [_client.py:1025] - HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "Generating System Report\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'timestamp': '2024-12-29T18:57:57.168198',\n",
       " 'configuration': {'config': {'CHUNK_SIZE': 1000,\n",
       "   'CHUNK_OVERLAP': 0,\n",
       "   'MODEL_TEMPERATURE': 0,\n",
       "   'EMBEDDING_MODEL': 'text-embedding-ada-002',\n",
       "   'MAX_RETRIES': 3,\n",
       "   'RATE_LIMIT_PER_SECOND': 5,\n",
       "   'LOG_LEVEL': 20},\n",
       "  'load_time': '2024-12-29T18:57:54.789042',\n",
       "  'age_seconds': 2.379163},\n",
       " 'metrics': {'document_load_duration': {'mean': 0.21483445167541504,\n",
       "   'min': 0.16620802879333496,\n",
       "   'max': 0.2634608745574951,\n",
       "   'count': 2},\n",
       "  'text_splitting_duration': {'mean': 8.64267349243164e-05,\n",
       "   'min': 8.392333984375e-05,\n",
       "   'max': 8.893013000488281e-05,\n",
       "   'count': 2},\n",
       "  'document_processing_duration': {'mean': 0.4291701316833496,\n",
       "   'min': 0.3287811279296875,\n",
       "   'max': 0.5295591354370117,\n",
       "   'count': 2},\n",
       "  'get_response_duration': {'mean': 0.8077270984649658,\n",
       "   'min': 0.8077270984649658,\n",
       "   'max': 0.8077270984649658,\n",
       "   'count': 1}},\n",
       " 'environment': {'python_version': '3.10.12 | packaged by conda-forge | (main, Jun 23 2023, 22:41:52) [Clang 15.0.7 ]',\n",
       "  'platform': 'macOS-15.2-arm64-arm-64bit',\n",
       "  'memory_usage': 288.796875},\n",
       " 'log_summary': {'error_count': 0, 'last_errors': []}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 18:59:46,850 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:59:49,246 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 18:59:49,253 - __main__ - INFO - [3138667305.py:100] - Query processed successfully in 2.66 seconds\n",
      "2024-12-29 19:00:28,601 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:00:30,805 - httpx - INFO - [_client.py:1025] - HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-29 19:00:30,809 - __main__ - INFO - [3138667305.py:100] - Query processed successfully in 2.39 seconds\n"
     ]
    }
   ],
   "source": [
    "print('\\n========================')\n",
    "print(f'Performing run tests')\n",
    "print('========================\\n')\n",
    "\n",
    "run_tests()\n",
    "\n",
    "print('\\n========================')\n",
    "print(f'Generating System Report')\n",
    "print('========================\\n')\n",
    "generate_system_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1720690009_20241228_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
