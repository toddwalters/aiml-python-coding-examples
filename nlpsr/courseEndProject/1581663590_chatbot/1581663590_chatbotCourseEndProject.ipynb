{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[**NLP and Speech Recognition Chatbot**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [**NLP and Speech Recognition Chatbot**](#toc1_)    \n",
    "  - 1.1. [**Problem Statement**](#toc1_1_)    \n",
    "  - 1.2. [**Objectives**](#toc1_2_)    \n",
    "  - 1.3. [**Analysis To Be Done**](#toc1_3_)    \n",
    "    - 1.3.1. [**Ensure Necessary Modules are Installed**](#toc1_3_1_)    \n",
    "    - 1.3.2. [**Import Modules and Set Default Environment Variables and Load Data**](#toc1_3_2_)    \n",
    "    - 1.3.3. [**Preprocess Data**](#toc1_3_3_)    \n",
    "    - 1.3.4. [**Create Pickle Files**](#toc1_3_4_)    \n",
    "    - 1.3.5. [**Create Training And Testing Datasets**](#toc1_3_5_)    \n",
    "    - 1.3.6. [**Build the Model**](#toc1_3_6_)    \n",
    "    - 1.3.7. [**Predict The Responses**](#toc1_3_7_)    \n",
    "      - 1.3.7.1. [**Load Required Python Modules**](#toc1_3_7_1_)    \n",
    "      - 1.3.7.2. [**Establish Environment Variables and Load Data**](#toc1_3_7_2_)    \n",
    "      - 1.3.7.3. [**Creat Prediction Functions**](#toc1_3_7_3_)    \n",
    "      - 1.3.7.4. [**Interactive loop for testing**](#toc1_3_7_4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "    numbering=true\n",
    "    anchor=true\n",
    "    flat=false\n",
    "    minLevel=1\n",
    "    maxLevel=6\n",
    "    /vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## 1.1. <a id='toc1_1_'></a>[**Problem Statement**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A company holds an event that has been given the deserved promotion through marketing in \n",
    "hopes of attracting as big an audience as possible. Now, itâ€™s up to the customer support team to \n",
    "guide the audience and answer any queries. Providing high-quality support and guidance is the \n",
    "challenge. The chatbot is very helpful for its 24/7 presence and ability to reply instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## 1.2. <a id='toc1_2_'></a>[**Objectives**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop a real-time chatbot to engage with the customers in order to boost their \n",
    "business growth by using NLP and Speech Recognition.\n",
    "\n",
    "**Domain:** Customer Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## 1.3. <a id='toc1_3_'></a>[**Analysis To Be Done**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of prebuilt commands or inputs as a dataset. Here, we use \n",
    "command .json as Dataset that contains the patterns we need to find and the responses we \n",
    "want to return to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### 1.3.1. <a id='toc1_3_1_'></a>[**Ensure Necessary Modules are Installed**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install nltk \n",
    "%pip install keras \n",
    "%pip install SpeechRecognition  \n",
    "%pip install tensorflow \n",
    "# %pip install pickle - standard library in python does not need to be installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### 1.3.2. <a id='toc1_3_2_'></a>[**Import Modules and Set Default Environment Variables and Load Data**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary NLTK data if not already downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Directory for dataset and dependency files\n",
    "dataset_dir = '1581663590_datasetsanddependencyfiles'\n",
    "\n",
    "# Initialize lists\n",
    "words = [] \n",
    "classes = [] \n",
    "documents = [] \n",
    "ignore_words = ['?', '!']\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load intents file\n",
    "with open(f'{dataset_dir}/commands.json') as data_file:\n",
    "    intents = json.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "### 1.3.3. <a id='toc1_3_3_'></a>[**Preprocess Data**](#toc0_)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each intent in the intents dataset\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        \n",
    "        # Add to documents in our corpus\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        \n",
    "        # Add to our classes if it's not already there\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize, lower each word, and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words] \n",
    "words = sorted(list(set(words))) \n",
    "\n",
    "# Sort classes \n",
    "classes = sorted(list(set(classes))) \n",
    "\n",
    "# Display basic stats\n",
    "print(len(documents), \"documents\") \n",
    "print(len(classes), \"classes\", classes) \n",
    "print(len(words), \"unique lemmatized words\", words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. <a id='toc1_3_4_'></a>[**Create Pickle Files**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words,open(f'{dataset_dir}/words.pkl','wb'))\n",
    "pickle.dump(classes,open(f'{dataset_dir}/classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. <a id='toc1_3_5_'></a>[**Create Training And Testing Datasets**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output_empty based on the number of classes\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Create separate lists for training data inputs (X) and outputs (Y)\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "# Create bag of words for each sentence and corresponding output\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word to create the base form\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create the bag of words array with 1 if word match found in current pattern\n",
    "    bag = [1 if w in pattern_words else 0 for w in words]\n",
    "    \n",
    "    # Output is '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    # Append the bag of words and output row to their respective lists\n",
    "    train_x.append(bag)\n",
    "    train_y.append(output_row)\n",
    "\n",
    "# Initial deep verification of train_x and train_y\n",
    "def verify_array(array, name):\n",
    "    print(f\"Verifying {name}...\")\n",
    "    for i, row in enumerate(array):\n",
    "        if not isinstance(row, np.ndarray):\n",
    "            print(f\"Row {i} in {name} is not a numpy array. Found type: {type(row)}\")\n",
    "        elif row.shape[0] != array.shape[1]:\n",
    "            print(f\"Row {i} in {name} has inconsistent shape. Expected {array.shape[1]}, found {row.shape[0]}\")\n",
    "        elif row.dtype != np.float32:\n",
    "            print(f\"Row {i} in {name} has incorrect dtype. Expected float32, found {row.dtype}\")\n",
    "\n",
    "# Convert train_x and train_y to numpy arrays\n",
    "train_x = np.array(train_x, dtype=np.float32)\n",
    "train_y = np.array(train_y, dtype=np.float32)\n",
    "\n",
    "# Deep verification before proceeding\n",
    "verify_array(train_x, \"train_x\")\n",
    "verify_array(train_y, \"train_y\")\n",
    "\n",
    "# Ensure consistency by stacking rows\n",
    "try:\n",
    "    train_x = np.vstack(train_x)\n",
    "    train_y = np.vstack(train_y)\n",
    "    print(\"train_x and train_y successfully stacked.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error in stacking train_x or train_y: {e}\")\n",
    "    raise\n",
    "\n",
    "# Print shapes and types for debugging\n",
    "print(\"Shape of train_x:\", train_x.shape)\n",
    "print(\"Shape of train_y:\", train_y.shape)\n",
    "print(\"Data type of train_x:\", train_x.dtype)\n",
    "print(\"Data type of train_y:\", train_y.dtype)\n",
    "\n",
    "# Double-check by printing sample values if needed\n",
    "print(f\"Sample values from train_x:\\n\", train_x[:5])\n",
    "print(f\"Sample values from train_y:\\n\", train_y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep copy train_x and train_y to ensure there are no lingering references\n",
    "train_x = copy.deepcopy(train_x)\n",
    "train_y = copy.deepcopy(train_y)\n",
    "\n",
    "# Convert train_x and train_y to strict numpy arrays and enforce dtype\n",
    "train_x = np.array(train_x, dtype=np.float32)\n",
    "train_y = np.array(train_y, dtype=np.float32)\n",
    "\n",
    "# Print shapes and sample values to verify\n",
    "print(\"Final check - Shape of train_x:\", train_x.shape)\n",
    "print(\"Final check - Shape of train_y:\", train_y.shape)\n",
    "print(\"Sample values from train_x:\", train_x[:5])\n",
    "print(\"Sample values from train_y:\", train_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6. <a id='toc1_3_6_'></a>[**Build the Model**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model structure\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(train_x.shape[1],)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(train_y.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile model with updated learning rate parameter\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True) \n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy']) \n",
    "\n",
    "# Train and save the model\n",
    "hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
    "model.save(f'{dataset_dir}/chatbot_model.h5', hist) \n",
    "\n",
    "print(\"Model created and saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7. <a id='toc1_3_7_'></a>[**Predict The Responses**](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7.1. <a id='toc1_3_7_1_'></a>[**Load Required Python Modules**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7.2. <a id='toc1_3_7_2_'></a>[**Establish Environment Variables and Load Data**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset directory\n",
    "dataset_dir = '1581663590_datasetsanddependencyfiles'\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(f'{dataset_dir}/chatbot_model.h5')\n",
    "\n",
    "# Load words and classes\n",
    "with open(f'{dataset_dir}/words.pkl', 'rb') as f:\n",
    "    words = pickle.load(f)\n",
    "with open(f'{dataset_dir}/classes.pkl', 'rb') as f:\n",
    "    classes = pickle.load(f)\n",
    "\n",
    "# Load intents\n",
    "with open(f'{dataset_dir}/commands.json', 'r') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7.3. <a id='toc1_3_7_3_'></a>[**Creat Prediction Functions**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up the sentence\n",
    "def clean_up_sentence(sentence):\n",
    "    # Tokenize the sentence\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # Lemmatize each word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# Function to create a bag of words from the sentence\n",
    "def bow(sentence, words, show_details=True):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(f'found in bag: {w}')\n",
    "    return np.array(bag)\n",
    "\n",
    "# Function to predict the class\n",
    "def predict_class(sentence, model):\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "# Function to get the response\n",
    "def get_response(intents_list, intents_json):\n",
    "    if intents_list:\n",
    "        tag = intents_list[0]['intent']\n",
    "        list_of_intents = intents_json['intents']\n",
    "        for i in list_of_intents:\n",
    "            if i['tag'] == tag:\n",
    "                result = random.choice(i['responses'])\n",
    "                break\n",
    "    else:\n",
    "        result = \"I don't understand, please try again.\"\n",
    "    return result\n",
    "\n",
    "# Function to chat with the model\n",
    "def chatbot_response(text):\n",
    "    intents = predict_class(text, model)\n",
    "    response = get_response(intents, intents)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.7.4. <a id='toc1_3_7_4_'></a>[**Interactive loop for testing**](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chatbot is ready to chat! (Type 'exit' to stop)\")\n",
    "while True:\n",
    "    message = input(\"You: \")\n",
    "    if message.lower() == \"exit\":\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    response = chatbot_response(message)\n",
    "    print(\"Bot:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
