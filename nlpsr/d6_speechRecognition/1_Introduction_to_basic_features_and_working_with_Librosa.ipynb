{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <font size = 24 color = 'steelblue'>**Introduction to Audio Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <center> <font size = 12> ðŸ”Š  **Working with Audio in Python**\n",
    "<center> <img src=\"images/intro-to-audio-analytics-title.jpeg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<font size = 4>\n",
    "\n",
    "**By the end of this notebook you will be able to:**\n",
    "\n",
    "- Learn about some important concepts in speech recognition    \n",
    "- Understand how to load audio files in python environment\n",
    "- Create visualizations\n",
    "- Exploring features\n",
    "- Prepare the data for machine learning\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id= 'p0'>\n",
    "<font size = 4>\n",
    "    \n",
    "**Table of Contents:**<br>\n",
    "[1. Python packages for audio analysis](#p1)<br>\n",
    "[2. Important concepts](#p2)<br>\n",
    "[3. Load the data](#p3)<br>\n",
    "[4. Visualization of data](#p4)<br>\n",
    "[5. Spectogram](#p5)<br>\n",
    "[6. Mel spectogram](#p6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id = 'p1'>\n",
    "<font size = 10 color = 'midnightblue'> **Python Packages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Numerous Python libraries, such as **Librosa**, **Pydub**, **pyAudioAnalysis**, **Playsound**, and **Mingus**, offer a wide range of capabilities for handling various tasks with audio files.\n",
    "- Librosa is leveraged in this notebook.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Librosa:**\n",
    "\n",
    "- Librosa, a Python package, is crafted for the analysis of music and audio.\n",
    "- It provides crucial components required for constructing systems dedicated to music information retrieval.\n",
    "- https://librosa.org/doc/main/generated/librosa.feature.mfcc.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda install -c conda-forge librosa seaborn python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')\n",
    "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "    DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
    "    print(\"Running in Colab environment\")\n",
    "except ModuleNotFoundError:\n",
    "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
    "    DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
    "    print(\"Running in local environment\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5 color = seagreen> **Load the packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "#itertools is a built-in module in Python for handling iterables.\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "color_cycle = cycle(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 'p2'>\n",
    "<font size = 10 color = 'midnightblue'> **Important concepts:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6 color = pwdrblue> **Audio in digital form:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "**Frequency**\n",
    "- Frequency refers to the number of cycles of a sound wave that occur in one second, measured in Hertz (Hz).\n",
    "- In speech recognition, the frequency of an audio signal is crucial for understanding the pitch and tone of the speaker's voice. Different phonemes and characteristics of speech are associated with specific frequency ranges.\n",
    "\n",
    "<center><img src=\"images/frequency.png\" width=\"400\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "<b>Amplitude</b>\n",
    "- Amplitude represents the maximum displacement of a wave from its equilibrium position and is related to the loudness of a sound.\n",
    "- Amplitude is essential for understanding the volume and intensity of speech. It helps in identifying variations in loudness, which can carry important information, especially in conversational contexts.\n",
    "\n",
    "<center><img src=\"images/amplitude.png\" width=\"400\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Intensity**\n",
    "- Intensity, in the context of sound, is the amount of energy carried by a sound wave, often perceived as loudness and measured in decibels (dB).\n",
    "- Intensity is significant for recognizing the emphasis, stress, and emotional content in speech. It plays a role in distinguishing between different phonetic elements and conveying the speaker's intention.\n",
    "\n",
    "<center><img src=\"images/intensity.gif\" width=\"400\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Length of Audio File:**\n",
    "- The length of an audio file refers to the duration of the recording, typically measured in seconds or minutes.\n",
    "- The length of an audio file is a practical consideration in speech recognition systems.\n",
    "- Longer audio files may require more computational resources, and the duration can impact the effectiveness of certain algorithms.\n",
    "- It's also crucial for managing real-time processing requirements.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Sample Rate**\n",
    "- Sample rate refers to the number of samples of audio taken per second and is measured in Hertz (Hz).\n",
    "- Sample rate is specific to how the computer reads in the audio file.\n",
    "- Think of it as the \"resolution\" of the audio.\n",
    "- The sample rate is crucial for accurately representing the analog audio signal in a digital form. Higher sample rates can capture more details, enhancing the precision of speech recognition systems.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"images/sample-rate.png\" width=\"400\"/>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p3'>\n",
    "<font size = 10 color = 'midnightblue'> **Loading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 5 colro = seagreen>\n",
    "\n",
    "Audio files are available in multiple formats like: `mp3`, `wav`, `m4a`, `flac`, `ogg`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Note about the data used:**\n",
    "- The data used here is the `Ryerson Audio-Visual Database of Emotional Speech and Song or RAVDESS`.\n",
    "- The section of data used here comprises **1440 files**, resulting from **60 trials per actor multiplied by 24 actors**.\n",
    "- The RAVDESS features 24 professional actors, evenly divided between 12 females and 12 males.\n",
    "- These actors vocalize two lexically-matched statements in a neutral North American accent.\n",
    "- Speech emotions included in the dataset encompass calm, happy, sad, angry, fearful, surprise, and disgust expressions.\n",
    "- Each expression is generated at two levels of emotional intensity, namely normal and strong, with an additional neutral expression.\n",
    "\n",
    "Citation: Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(f'{DATASET_PATH}/ravdess-emotional-speech-audio.zip', 'r') as f:\n",
    "    f.extractall(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all .wav files from the extracted folder structure\n",
    "audio_files = glob('ravdess-emotional-speech-audio/Actor_*/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(audio_files) > 0:\n",
    "    display(ipd.Audio(audio_files[0]))\n",
    "else:\n",
    "    print(\"No audio files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = seagreen size = 5> <center><b>Looking at first few (10) records and printing shape of the array and sample rate.</b></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "Here,\n",
    "- `y` represents the audio time series (amplitude values) of the loaded audio file.\n",
    "- `sr` represents the sample rate, which is the number of samples of audio per second.\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_files[0])\n",
    "print(f'y: {y[:10]}')\n",
    "print(f'shape y: {y.shape}')\n",
    "print(f'sr: {sr}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id = 'p3'>\n",
    "<font size = 10 color = 'midnightblue'><b>Visualization</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>     \n",
    "\n",
    "- Generate a line plot of the raw audio time series using Pandas and Matplotlib.\n",
    "- The resulting plot visualizes the amplitude values of the audio over time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Example',\n",
    "                  color=color_pal[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "**Observations:**\n",
    "\n",
    "- Use `librosa.effects.trim` function to trim silence from the audio time series y.\n",
    "- `top_db=20` sets a threshold (in decibels) below which segments of the audio are considered silent and will be trimmed.\n",
    "- The trimmed audio is stored in the variable `y_trimmed`, and the second variable `_` is used to discard the second output of the function, which represents the indices of the trimmed segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming leading/lagging silence\n",
    "y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "pd.Series(y_trimmed).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Trimmed Example',\n",
    "                  color=color_pal[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- Select a specific range of values from the original audio time series (y) and creates a line plot of that zoomed-in portion.\n",
    "- The resulting plot provides a detailed view of the amplitude values within the specified index range, allowing for a closer examination of the audio waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y[30000:30500]).plot(figsize=(10, 5),\n",
    "                  lw=1,\n",
    "                  title='Raw Audio Zoomed In Example',\n",
    "                  color=color_pal[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 'p5'>\n",
    "<font size = 10 color = 'midnightblue'> **Spectrogram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "- A spectrogram is a visual representation of the spectrum of frequencies in a signal as they vary with time.\n",
    "- It provides a way to analyze the frequency content of a signal over different time intervals.\n",
    "- Perform the Short-Time Fourier Transform (STFT) on the audio time series y using Librosa\n",
    "- STFT and spectrogram conversion are critical steps in audio analytics, providing a rich representation of the frequency content over time. This representation is valuable for a wide range of applications\n",
    "- The STFT is a powerful tool for extracting time-varying frequency information from an audio signal.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = librosa.stft(y)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "S_db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the transformed audio data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "img = librosa.display.specshow(S_db,\n",
    "                              x_axis='time',\n",
    "                              y_axis='log',\n",
    "                              ax=ax)\n",
    "ax.set_title('Spectogram Example', fontsize=20)\n",
    "fig.colorbar(img, ax=ax, format=f'%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "**Here are the key components of a spectrogram:**\n",
    "\n",
    "- **X-Axis (Horizontal Axis):**\n",
    "    - Represents time.\n",
    "    - The signal is divided into short time intervals, and each column in the spectrogram corresponds to the frequency content of the signal during that specific time interval.\n",
    "\n",
    "- **Y-Axis (Vertical Axis)**:\n",
    "    - Represents frequency.\n",
    "    - The vertical axis shows the frequency range covered by the analysis, usually from low to high frequencies.\n",
    "\n",
    "- **Color or Intensity:**\n",
    "    - The color or intensity of each point in the spectrogram represents the amplitude or power of the corresponding frequency component at a specific time.\n",
    "    - Brighter or more intense colors usually indicate higher amplitudes.\n",
    "\n",
    "- **Frequency Resolution:**\n",
    "    - The width of the frequency bins in the spectrogram determines the frequency resolution.\n",
    "    - Narrower bins provide better frequency resolution but may result in a loss of time resolution.\n",
    "\n",
    "- **Time Resolution:**\n",
    "    - The length of the time intervals, or windows, used in the analysis determines the time resolution.\n",
    "    - Shorter windows provide better time resolution but may result in a loss of frequency resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### <a id = 'p6'>\n",
    "<font size = 10 color = 'midnightblue'> **Mel Spectrogram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "    \n",
    "- Mel spectrograms play a crucial role in audio analytics, particularly in tasks related to human auditory perception.\n",
    "- Use `librosa.feature.melspectrogram` to compute the mel spectrogram S.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=y,\n",
    "                                   sr=sr,\n",
    "                                   n_mels=128 * 2,)\n",
    "S_db_mel = librosa.amplitude_to_db(S, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the mel spectogram\n",
    "img = librosa.display.specshow(S_db_mel,\n",
    "                              x_axis='time',\n",
    "                              y_axis='log',\n",
    "                              ax=ax)\n",
    "ax.set_title('Mel Spectogram Example', fontsize=20)\n",
    "fig.colorbar(img, ax=ax, format=f'%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_db_mel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size = 4>\n",
    "\n",
    "<center>    <b>We can run the above for the whole dataset and create features for any ML algorithm ðŸ˜ƒ</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-play",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
