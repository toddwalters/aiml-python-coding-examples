{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toddwalters/aiml-python-coding-examples/blob/main/capstone/projects/historicalStructureClassification/1703138137_ToddWalters_project_historicalStructureClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- 1. [**Historical Structure Classification Project**](#toc1_)    \n",
        "  - 1.1. [**Project Context**](#toc1_1_)    \n",
        "  - 1.2. [**Project Objectives**](#toc1_2_)    \n",
        "  - 1.3. [**Project Dataset Description**](#toc1_3_)    \n",
        "  - 1.4. [**Project Analysis Steps To Perform**](#toc1_4_)    \n",
        "    - 1.4.1. [**Part 1 - Deep Learning**](#toc1_4_1_)    \n",
        "      - 1.4.1.1. [**Import Modules and Set Default Environment Variables**](#toc1_4_1_1_)    \n",
        "      - 1.4.1.2. [**[OPTIONAL] Analyze Example File Shape** ](#toc1_4_1_2_)    \n",
        "      - 1.4.1.3. [**[OPTIONAL] Look for corrupt files** ](#toc1_4_1_3_)    \n",
        "      - 1.4.1.4. [**[OPTIONAL] Rename Files** ](#toc1_4_1_4_)    \n",
        "      - 1.4.1.5. [**Plot The Sample Images**](#toc1_4_1_5_)    \n",
        "      - 1.4.1.6. [**Define Utility Functions and Classes**](#toc1_4_1_6_)    \n",
        "      - 1.4.1.7. [**Select an CNN Pre-Trained Model for Transfer Learning**](#toc1_4_1_7_)    \n",
        "      - 1.4.1.8. [**Compile The Model**](#toc1_4_1_8_)    \n",
        "      - 1.4.1.9. [**Define Callback Class**](#toc1_4_1_9_)    \n",
        "      - 1.4.1.10. [**Set Up Dataset Directories And Review Sample Numbers**](#toc1_4_1_10_)    \n",
        "      - 1.4.1.11. [**Train The Model Without Augmentation**](#toc1_4_1_11_)    \n",
        "      - 1.4.1.12. [**Train The Model Without Augmentation All Together**](#toc1_4_1_12_)    \n",
        "      - 1.4.1.13. [**Test Trained Model**](#toc1_4_1_13_)    \n",
        "      - 1.4.1.14. [**Train The Model With Augmentation**](#toc1_4_1_14_)    \n",
        "      - 1.4.1.15. [**Visualize Training And Validation Accuracy**](#toc1_4_1_15_)    \n",
        "      - 1.4.1.16. [**Plot Hyperparameter Search Results**](#toc1_4_1_16_)    \n",
        "      - 1.4.1.17. [**Test Trained Model**](#toc1_4_1_17_)    \n",
        "    - 1.4.2. [**Part 2 - Data Science**](#toc1_4_2_)    \n",
        "      - 1.4.2.1. [**Data Analysis**](#toc1_4_2_1_)    \n",
        "      - 1.4.2.2. [**Data Analysis v2**](#toc1_4_2_2_)    \n",
        "        - 1.4.2.2.1. [**Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc1_4_2_2_1_)    \n",
        "        - 1.4.2.2.2. [**Use the data from the last six months as the testing data**](#toc1_4_2_2_2_)    \n",
        "        - 1.4.2.2.3. [**Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc1_4_2_2_3_)    \n",
        "        - 1.4.2.2.4. [**Use the best-performing models to make a forecast for the next year**](#toc1_4_2_2_4_)    \n",
        "    - 1.4.3. [**4.4. Forecasting using deep learning algorithms**](#toc1_4_3_)    \n",
        "      - 1.4.3.1. [**4.4.1. Use sales amount for predictions instead of item count**](#toc1_4_3_1_)    \n",
        "      - 1.4.3.2. [**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc1_4_3_2_)    \n",
        "        - 1.4.3.2.1. [**4.4.2.1. Define the train and test series**](#toc1_4_3_2_1_)    \n",
        "        - 1.4.3.2.2. [**4.4.2.2. Generate synthetic data for the last 12 months**](#toc1_4_3_2_2_)    \n",
        "        - 1.4.3.2.3. [**4.4.2.3. Build and train an LSTM model**](#toc1_4_3_2_3_)    \n",
        "        - 1.4.3.2.4. [**4.4.2.4. Use the model to make predictions for the test data**](#toc1_4_3_2_4_)    \n",
        "      - 1.4.3.3. [**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc1_4_3_3_)    \n",
        "      - 1.4.3.4. [**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc1_4_3_4_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=true\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Table of Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----\n",
        "\n",
        "# 1. <a id='toc1_'></a>[**Historical Structure Classification Project**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------------------------\n",
        "## 1.1. <a id='toc1_1_'></a>[**Project Context**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "This project is part of an AIML Capstone and focuses on two main aspects:\n",
        "1. Classifying historical structures using deep learning techniques\n",
        "2. Developing a recommendation engine for tourism\n",
        "\n",
        "The project aims to assist the travel and tourism industries by monitoring the condition of historical structures and providing personalized recommendations to tourists.\n",
        "\n",
        "-----------------------------\n",
        "## 1.2. <a id='toc1_2_'></a>[**Project Objectives**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "1. Develop a CNN model to classify historical structures from images\n",
        "2. Perform exploratory data analysis on tourism data\n",
        "3. Create a recommendation engine for tourist attractions\n",
        "\n",
        "-----------------------------\n",
        "## 1.3. <a id='toc1_3_'></a>[**Project Dataset Description**](#toc0_)\n",
        "-----------------------------\n",
        "\n",
        "**Part 1: Image Classification**\n",
        "\n",
        "1. **Structures_dataset.zip**: Training set of images of historical structures\n",
        "2. **dataset_test**: Test set of images of historical structures\n",
        "\n",
        "**Part 2: Tourism Recommendation**\n",
        "\n",
        "1. **user.csv**: User demographic data\n",
        "   - Columns: User_id, location, age\n",
        "2. **tourism_with_id.csv**: Details on tourist attractions in Indonesia's five largest cities\n",
        "   - Columns: Place_id, Place_name, Description, Category, City, Price, Rating, Time_minute, Coordinate, Lat, Long\n",
        "3. **tourism_rating.csv**: User ratings for tourist attractions\n",
        "   - Columns: user_id, place_id, place_rating\n",
        "\n",
        "-----------------------------------\n",
        "## 1.4. <a id='toc1_4_'></a>[**Project Analysis Steps To Perform**](#toc0_)\n",
        "-----------------------------------\n",
        "\n",
        "\n",
        "**Part 1 - Deep Learning**\n",
        "\n",
        "1. Plot the sample images (8–10) from each class or category to gain a better understanding\n",
        "of each class\n",
        "   > Hint: You can use the OpenCV open-source library for this task.\n",
        "2. Select an CNN ptm_name of your choice to train the CV model. Configure the\n",
        "ptm_name for transfer learning, set up a TensorFlow environment for the selected\n",
        "backbone ptm_name, and load pre-trained weights\n",
        "   > Note: Algorithm or ptm_name selection is an important step in the training of ML models,\n",
        "so select the one that performs the best on your dataset.\n",
        "3. Deep learning models tend to work well with large amounts (millions of images) of data, but\n",
        "we may not always have enough data to train a deep learning model from scratch. Transfer\n",
        "learning techniques allow us to train and fine-tune large deep learning ptm_names using\n",
        "limited data.\n",
        "   > Hint: For transfer learning, use pre-trained CNN weights and freeze all convolutional\n",
        "layers' weights.\n",
        "4. As of now, CNN ptm_name has been configured for our model. Modify the top of this\n",
        "ptm_name to work with our dataset by:\n",
        "• Adding an appropriate number of dense layers with an activation function.\n",
        "• Using dropout for regularization.\n",
        "   > Note: It is important to understand that these parameters are hyperparameters that\n",
        "must be tuned.\n",
        "5. Compile the model with the right set of parameters like optimizer, loss function, and metric\n",
        "6. Define your callback class to stop the training once validation accuracy reaches a certain\n",
        "number of your choice\n",
        "7. Setup the train or test dataset directories and review the number of image samples for the train\n",
        "and test datasets for each class\n",
        "8. Train the model without augmentation while continuously monitoring the validation accuracy\n",
        "9. Train the model with augmentation and keep monitoring validation accuracy\n",
        "   > Note: Choose carefully the number of epochs, steps per epoch, and validation steps based on\n",
        "your computer configuration\n",
        "10. Visualize training and validation accuracy on the y-axis against each epoch on the x-axis to\n",
        "see if the model overfits after a certain epoch\n",
        "Deep learning\n",
        "\n",
        "**Part 2 - Data Science**\n",
        "\n",
        "1. Import all the datasets and perform preliminary inspections, such as:\n",
        "   1. Check for missing values and duplicates\n",
        "   2. Remove any anomalies found in the data\n",
        "2. To understand the tourism highlights better, we should explore the data in depth.\n",
        "   1. Explore the user group that provides the tourism ratings by:\n",
        "      - Analyzing the age distribution of users visiting the places and rating them\n",
        "      - Identifying the places where most of these users (tourists) are coming from\n",
        "3. Next, let's explore the locations and categories of tourist spots.\n",
        "   1. What are the different categories of tourist spots?\n",
        "   2. What kind of tourism each location is most famous or suitable for?\n",
        "   3. Which city would be best for a nature enthusiast to visit?\n",
        "4. To better understand tourism, we need to create a combined data with places and their user ratings.\n",
        "   1. Use this data to figure out the spots that are most loved by the tourists. Also, which city\n",
        "has the most loved tourist spots?\n",
        "   2. Indonesia provides a wide range of tourist spots ranging from historical and cultural\n",
        "beauties to advanced amusement parks. Among these, which category of places are users\n",
        "liking the most?\n",
        "5. Build a recommender model for the system\n",
        "   1. Use the above data to develop a collaborative filtering model for recommendation and\n",
        "use that to recommend other places to visit using the current tourist location(place name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.1. <a id='toc1_4_1_'></a>[**Part 1 - Deep Learning**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.1. <a id='toc1_4_1_1_'></a>[**Import Modules and Set Default Environment Variables**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from keras import backend as K\n",
        "from keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.utils import Sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.2. <a id='toc1_4_1_2_'></a>[**[OPTIONAL] Analyze Example File Shape**](#toc0_)  [&#8593;](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MF Running in local environment\n",
            "DATASET_PATH: /Users/toddwalters/Development/data/1703138137_dataset/part_1/dataset_hist_structures\n",
            "Width: 128, Height: 128\n",
            "Image mode: RGB\n",
            "The image is color.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # drive.mount('/content/drive')\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "    print(\"MF Running in Colab environment\")\n",
        "except ModuleNotFoundError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "    print(\"MF Running in local environment\")\n",
        "    print(f\"DATASET_PATH: {DATASET_PATH}\")\n",
        "\n",
        "# Open an image file\n",
        "image = Image.open(f'{DATASET_PATH}/train_dataset/bell_tower/7720646_09270baef1.jpg')\n",
        "\n",
        "# Get dimensions\n",
        "width, height = image.size\n",
        "print(f\"Width: {width}, Height: {height}\")\n",
        "\n",
        "# Check the image mode\n",
        "mode = image.mode\n",
        "print(f\"Image mode: {mode}\")\n",
        "\n",
        "# Determine if the image is color or black and white\n",
        "if mode == 'L' or mode == '1':\n",
        "    print(\"The image is black and white.\")\n",
        "elif mode == 'RGB':\n",
        "    # Convert image to numpy array\n",
        "    image_array = np.array(image)\n",
        "    \n",
        "    # Check if all color channels are the same\n",
        "    if np.array_equal(image_array[:, :, 0], image_array[:, :, 1]) and np.array_equal(image_array[:, :, 1], image_array[:, :, 2]):\n",
        "        print(\"The color channels are all the same.  The image is grayscale.\")\n",
        "    else:\n",
        "        print(\"The image is color.\")\n",
        "else:\n",
        "    print(\"The image mode is not recognized for color or black and white determination.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.3. <a id='toc1_4_1_3_'></a>[**[OPTIONAL] Look for corrupt files**](#toc0_)  [&#8593;](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def verify_images(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    img = Image.open(file_path)\n",
        "                    img.verify()\n",
        "                except (IOError, SyntaxError) as e:\n",
        "                    print(f'Bad file: {file_path}')\n",
        "                    os.remove(file_path)\n",
        "                    print(f'Deleted bad file: {file_path}')\n",
        "\n",
        "# verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "# verify_images(f'{DATASET_PATH}/structure_dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.4. <a id='toc1_4_1_4_'></a>[**[OPTIONAL] Rename Files**](#toc0_)  [&#8593;](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def random_string(length):\n",
        "    \"\"\"Generate a random string of fixed length\"\"\"\n",
        "    letters = string.ascii_lowercase + string.digits\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def rename_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            # Generate a unique name\n",
        "            new_name = str(uuid.uuid4())\n",
        "\n",
        "            # Get the file extension\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "            # Create the new filename\n",
        "            new_filename = f\"{new_name}.jpg\"\n",
        "\n",
        "            # Full paths\n",
        "            old_file = os.path.join(root, filename)\n",
        "            new_file = os.path.join(root, new_filename)\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(old_file, new_file)\n",
        "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
        "\n",
        "# Uncomment if you want to rename all of the files in the dataset\n",
        "# directories = [f'{DATASET_PATH}/structures_dataset', f'{DATASET_PATH}/dataset_test']\n",
        "# for start_directory in directories:\n",
        "#     rename_files(start_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.5. <a id='toc1_4_1_5_'></a>[**Plot The Sample Images**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2knLkSoXxTJ"
      },
      "outputs": [],
      "source": [
        "def plot_sample_images(dataset_path, num_samples=8):\n",
        "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
        "\n",
        "        for j, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "        # Set the class name as the title for the first image in the row\n",
        "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('COLAB_DATASET_PATH', default='/default/dataset/path')\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "\n",
        "plot_sample_images(f'{DATASET_PATH}/structures_dataset/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_sample_images` that takes a dataset path and plots a specified number of sample images from each class. It uses OpenCV to read the images and matplotlib to display them in a grid.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing sample images from each class helps us understand the nature of the data we're working with. It allows us to identify any potential issues with the images, such as inconsistent sizes, color schemes, or quality. This step is crucial for determining if any preprocessing steps are needed before training the model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.6. <a id='toc1_4_1_6_'></a>[**Define Utility Functions and Classes**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define utility functions and classes\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, generator, steps_per_epoch):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.reset()\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_accuracy = logs.get('accuracy', 'N/A')\n",
        "        val_accuracy = logs.get('val_accuracy', 'N/A')\n",
        "\n",
        "        train_accuracy_str = f\"{train_accuracy:.4f}\" if isinstance(train_accuracy, float) else str(train_accuracy)\n",
        "        val_accuracy_str = f\"{val_accuracy:.4f}\" if isinstance(val_accuracy, float) else str(val_accuracy)\n",
        "\n",
        "        print(f\" Epoch {epoch + 1} - Train accuracy: {train_accuracy_str}\")\n",
        "        print(f\" Epoch {epoch + 1} - Val accuracy: {val_accuracy_str}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if batch % 100 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            loss_str = f\"{loss:.4f}\" if isinstance(loss, float) else str(loss)\n",
        "            print(f\" Batch {batch} - Loss: {loss_str}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "        print(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset).numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1} - Test samples: {tf.data.experimental.cardinality(self.test_dataset).numpy()}\")\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        evaluation = self.model.evaluate(self.validation_data, steps=self.validation_steps, verbose=0)\n",
        "        print(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "        print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_data_generators(train_path, test_path, batch_size=32):\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        test_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get class names before applying normalization\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Repeat datasets\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    test_dataset = test_dataset.repeat()\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    total_train_samples = sum([len(files) for r, d, files in os.walk(train_path) if files])\n",
        "    total_test_samples = sum([len(files) for r, d, files in os.walk(test_path) if files])\n",
        "    steps_per_epoch = total_train_samples // batch_size\n",
        "    validation_steps = total_test_samples // batch_size\n",
        "\n",
        "    # Get classes\n",
        "    classes = np.concatenate([y for x, y in train_dataset.take(steps_per_epoch)], axis=0)\n",
        "    classes = np.argmax(classes, axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names\n",
        "\n",
        "def verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch):\n",
        "    print(\"Verifying dataset...\")\n",
        "\n",
        "    # 1. Print dataset information\n",
        "    print(f\"Train dataset cardinality: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "    print(f\"Test dataset cardinality: {tf.data.experimental.cardinality(test_dataset)}\")\n",
        "\n",
        "    # 2. Inspect a few batches\n",
        "    print(\"\\nInspecting batches:\")\n",
        "    for i, (images, labels) in enumerate(train_dataset.take(10)):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Image min-max values: {tf.reduce_min(images).numpy()} - {tf.reduce_max(images).numpy()}\")\n",
        "        print(f\"  Unique labels: {np.unique(labels.numpy())}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 3. Check dataset configuration\n",
        "    print(\"\\nDataset configuration:\")\n",
        "    print(f\"Train dataset: {train_dataset}\")\n",
        "    print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "    # 4. Verify steps per epoch\n",
        "    total_train_samples = tf.data.experimental.cardinality(train_dataset).numpy() * batch_size\n",
        "    calculated_steps = total_train_samples // batch_size\n",
        "    print(f\"\\nCalculated steps per epoch: {calculated_steps}\")\n",
        "    print(f\"Provided steps per epoch: {steps_per_epoch}\")\n",
        "    assert calculated_steps == steps_per_epoch, \"Steps per epoch mismatch\"\n",
        "\n",
        "    # 5. Test complete iteration\n",
        "    print(\"\\nTesting complete iteration:\")\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for step, (images, labels) in enumerate(train_dataset):\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: images shape {images.shape}, labels shape {labels.shape}\")\n",
        "            if step >= steps_per_epoch:\n",
        "                break\n",
        "\n",
        "    # 6. Check for data leakage\n",
        "    print(\"\\nChecking for data leakage...\")\n",
        "    train_sample = next(iter(train_dataset.take(100)))[0][0]\n",
        "    for test_batch in test_dataset.take(100):\n",
        "        assert not np.array_equal(train_sample, test_batch[0][0]), \"Data leakage detected\"\n",
        "    print(\"No data leakage detected\")\n",
        "\n",
        "    # 7. Verify data augmentation (if applicable)\n",
        "    # Uncomment and modify this section if you're using data augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\nVerifying data augmentation:\")\n",
        "    for original, augmented in zip(train_dataset.take(1), train_dataset.map(augmentation_function).take(1)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(original[0][0])\n",
        "        plt.title(\"Original\")\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(augmented[0][0])\n",
        "        plt.title(\"Augmented\")\n",
        "        plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDataset verification complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.7. <a id='toc1_4_1_7_'></a>[**Select an CNN Pre-Trained Model for Transfer Learning**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "35yzkV_1XxTK"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes):\n",
        "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `create_model` that creates a CNN using transfer learning. It uses ResNet50 as the base model with pre-trained ImageNet weights. The base model layers are frozen, and custom dense layers are added on top for fine-tuning.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Transfer learning allows us to leverage pre-trained models on large datasets, which is particularly useful when we have limited data. By using a pre-trained model as a feature extractor and adding custom layers, we can adapt the model to our specific classification task while benefiting from the general features learned by the base model.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.8. <a id='toc1_4_1_8_'></a>[**Compile The Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hJqh425GXxTK"
      },
      "outputs": [],
      "source": [
        "def compile_model(model):\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        # decay_steps=1000,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `compile_model` that compiles the model with the Adam optimizer, categorical crossentropy loss function, and accuracy metric.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Compiling the model is a crucial step that defines how the model will be trained. The choice of optimizer, loss function, and metrics affects the training process and the model's performance. Categorical crossentropy is appropriate for multi-class classification tasks, and accuracy provides a clear measure of the model's performance.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.9. <a id='toc1_4_1_9_'></a>[**Define Callback Class**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6EEh6fvpXxTK"
      },
      "outputs": [],
      "source": [
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        accuracy = logs.get('accuracy', 'N/A')\n",
        "        accuracy_str = f\"{accuracy:.4f}\" if isinstance(accuracy, float) else str(accuracy)\n",
        "        print(f\" Epoch {epoch + 1} - Accuracy: {accuracy_str}\")\n",
        "        if isinstance(accuracy, float) and accuracy >= self.target_accuracy:\n",
        "            print(f\"Reached target accuracy of {self.target_accuracy}\")\n",
        "            self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a custom callback class `AccuracyCallback` that stops the training when a target validation accuracy is reached.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Callbacks allow us to customize the training process. In this case, we're using a callback to prevent overfitting by stopping the training once we've reached a satisfactory level of validation accuracy. This helps us avoid wasting computational resources and reduces the risk of the model memorizing the training data.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.10. <a id='toc1_4_1_10_'></a>[**Set Up Dataset Directories And Review Sample Numbers**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNIxwiL7XxTK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('COLAB_DATASET_PATH', default='/default/dataset/path')\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "\n",
        "# Check TensorFlow Verision\n",
        "print()\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "print()\n",
        "\n",
        "# Setup Dataset Directories\n",
        "train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "# Set up data generators first to get class information\n",
        "batch_size = 32\n",
        "train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "# Now we can use class_names for our existing count_samples function\n",
        "train_counts = count_samples(train_path)\n",
        "test_counts = count_samples(test_path)\n",
        "\n",
        "print(\"\\nTraining samples per class:\")\n",
        "for class_name, count in sorted(train_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print(\"\\nTest samples per class:\")\n",
        "for class_name, count in sorted(test_counts.items()):\n",
        "    print(f\"{class_name}: {count}\")\n",
        "print()\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Print class weights\n",
        "print(\"Class weights:\")\n",
        "for i, weight in enumerate(class_weights):\n",
        "    print(f\"Class {i} ({class_names[i]}): {weight}\")\n",
        "print()\n",
        "\n",
        "# Create and compile the model\n",
        "num_classes = len(class_names)\n",
        "model = create_model(num_classes)\n",
        "# print(model.summary())\n",
        "compiled_model = compile_model(model)\n",
        "\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Validation steps: {validation_steps}\")\n",
        "print()\n",
        "\n",
        "# Setup Dataset Directories\n",
        "train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "test_path = f'{DATASET_PATH}/dataset_test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `count_samples` that counts the number of samples in each class for a given dataset. It then applies this function to both the training and test datasets and prints the results.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Understanding the distribution of samples across classes is crucial for several reasons:\n",
        "\n",
        "    1. It helps identify any class imbalance issues that may need to be addressed.\n",
        "    2. It ensures we have enough samples in each class for both training and testing.\n",
        "    3. It helps in setting appropriate batch sizes and steps per epoch during training.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.11. <a id='toc1_4_1_11_'></a>[**Train The Model Without Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "N4YBG86DXxTL"
      },
      "outputs": [],
      "source": [
        "# Main execution\n",
        "def main(class_names, verify_images, rename_dataset, plot_samples, verify_data, plot_history):\n",
        "    if verify_images:\n",
        "        verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "        verify_images(f'{DATASET_PATH}/structure_dataset')\n",
        "\n",
        "    if rename_dataset:\n",
        "        start_directory = f'{DATASET_PATH}/structures_dataset'\n",
        "        rename_files(start_directory)\n",
        "        start_directory = f'{DATASET_PATH}/dataset_test'\n",
        "        rename_files(start_directory)\n",
        "\n",
        "    if plot_samples:\n",
        "        plot_sample_images(f'{DATASET_PATH}/structures_dataset/')\n",
        "    \n",
        "    if verify_data:\n",
        "        verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch)\n",
        "    \n",
        "    # Set up callbacks\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=0.98)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_dataset, test_dataset)\n",
        "    custom_validation = CustomValidationCallback(test_dataset, validation_steps)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    try:\n",
        "        # Separate evaluation before training\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print(f\"Initial evaluation: {evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Training\n",
        "        history = compiled_model.fit(\n",
        "            train_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=25,\n",
        "            validation_data=test_dataset,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation, accuracy_callback],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print()\n",
        "        print(\"Final evaluation:\")\n",
        "        print(\"-----------------\")\n",
        "        final_evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print()\n",
        "        print(f\"Final evaluation: {final_evaluation}\")\n",
        "        print()\n",
        "\n",
        "        if plot_history:\n",
        "            # Plot training history\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['loss'], label='Training Loss')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print()\n",
        "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        print()\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    return compiled_model, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compiled_model, class_names = main(class_names, verify_images=False, rename_dataset=False, plot_samples=False, verify_data=False, plot_history=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.12. <a id='toc1_4_1_12_'></a>[**Train The Model Without Augmentation All Together**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import math\n",
        "import multiprocessing\n",
        "import os\n",
        "import time\n",
        "import uuid\n",
        "import random\n",
        "import string\n",
        "import warnings\n",
        "from dotenv import load_dotenv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.applications import (EfficientNetB0, InceptionV3, MobileNetV2, ResNet50V2, VGG16)\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (Callback, EarlyStopping, LambdaCallback, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau)\n",
        "from keras.layers import (BatchNormalization, Dense, Dropout, GlobalAveragePooling2D, Input)\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.utils import Sequence\n",
        "\n",
        "def verify_images(directory): # Look for corrupt files and delete them\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    img = Image.open(file_path) # open the image file\n",
        "                    img.verify() # verify that it is, in fact an image\n",
        "                except (IOError, SyntaxError) as e:\n",
        "                    print(f'Bad file: {file_path}') # print out the names of corrupt files\n",
        "                    os.remove(file_path)\n",
        "                    print(f'Deleted bad file: {file_path}')\n",
        "\n",
        "def random_string(length):\n",
        "    \"\"\"Generate a random string of fixed length\"\"\"\n",
        "    letters = string.ascii_lowercase + string.digits\n",
        "    return ''.join(random.choice(letters) for i in range(length))\n",
        "\n",
        "def rename_files(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for filename in files:\n",
        "            # Generate a unique name\n",
        "            new_name = str(uuid.uuid4())\n",
        "\n",
        "            # Get the file extension\n",
        "            file_extension = os.path.splitext(filename)[1]\n",
        "\n",
        "            # Create the new filename\n",
        "            new_filename = f\"{new_name}.jpg\"\n",
        "\n",
        "            # Full paths\n",
        "            old_file = os.path.join(root, filename)\n",
        "            new_file = os.path.join(root, new_filename)\n",
        "\n",
        "            # Rename the file\n",
        "            os.rename(old_file, new_file)\n",
        "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
        "\n",
        "def plot_sample_images(dataset_path, num_samples=8):\n",
        "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
        "    fig, axes = plt.subplots(len(classes), num_samples, figsize=(20, 4*len(classes)))\n",
        "\n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if os.path.isfile(os.path.join(class_path, f))][:num_samples]\n",
        "\n",
        "        for j, image_name in enumerate(images):\n",
        "            image_path = os.path.join(class_path, image_name)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            axes[i, j].imshow(img)\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "        # Set the class name as the title for the first image in the row\n",
        "        axes[i, 0].set_title(class_name, fontsize=16, pad=20)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Define utility functions and classes\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, generator, steps_per_epoch):\n",
        "        self.generator = generator\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.steps_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return next(self.generator)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.generator.reset()\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        train_accuracy = logs.get('accuracy', 'N/A')\n",
        "        val_accuracy = logs.get('val_accuracy', 'N/A')\n",
        "\n",
        "        train_accuracy_str = f\"{train_accuracy:.4f}\" if isinstance(train_accuracy, float) else str(train_accuracy)\n",
        "        val_accuracy_str = f\"{val_accuracy:.4f}\" if isinstance(val_accuracy, float) else str(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1} - Train accuracy: {train_accuracy_str}\")\n",
        "        print(f\"Epoch {epoch + 1} - Val accuracy: {val_accuracy_str}\")\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        if batch % 100 == 0:\n",
        "            loss = logs.get('loss', 'N/A')\n",
        "            loss_str = f\"{loss:.4f}\" if isinstance(loss, float) else str(loss)\n",
        "            print(f\" Batch {batch} - Loss: {loss_str}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, test_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        print(f\"\\nStarting epoch {epoch + 1}\")\n",
        "        print(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset).numpy()}\")\n",
        "        print(f\"Epoch {epoch + 1} - Test samples: {tf.data.experimental.cardinality(self.test_dataset).numpy()}\")\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        evaluation = self.model.evaluate(self.validation_data, steps=self.validation_steps, verbose=0)\n",
        "        print(f\"\\nEpoch {epoch + 1} - Custom validation:\")\n",
        "        print(f\"Loss: {evaluation[0]:.4f}\")\n",
        "        print(f\"Accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "def count_samples(dataset_path):\n",
        "    class_counts = {}\n",
        "    for class_name in os.listdir(dataset_path):\n",
        "        class_path = os.path.join(dataset_path, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            class_counts[class_name] = len(os.listdir(class_path))\n",
        "    return class_counts\n",
        "\n",
        "def create_data_generators(train_path, test_path, batch_size=32):\n",
        "    train_dataset = image_dataset_from_directory(\n",
        "        train_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_dataset = image_dataset_from_directory(\n",
        "        test_path,\n",
        "        image_size=(128, 128),\n",
        "        batch_size=batch_size,\n",
        "        label_mode='categorical',\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Get class names before applying normalization\n",
        "    class_names = train_dataset.class_names\n",
        "\n",
        "    # Rescale the pixel values\n",
        "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "    train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "    test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "    # Repeat datasets\n",
        "    train_dataset = train_dataset.repeat()\n",
        "    test_dataset = test_dataset.repeat()\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    total_train_samples = sum([len(files) for r, d, files in os.walk(train_path) if files])\n",
        "    total_test_samples = sum([len(files) for r, d, files in os.walk(test_path) if files])\n",
        "    steps_per_epoch = total_train_samples // batch_size\n",
        "    validation_steps = total_test_samples // batch_size\n",
        "\n",
        "    # Get classes\n",
        "    classes = np.concatenate([y for x, y in train_dataset.take(steps_per_epoch)], axis=0)\n",
        "    classes = np.argmax(classes, axis=1)\n",
        "\n",
        "    return train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names\n",
        "\n",
        "def verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch):\n",
        "    print(\"Verifying dataset...\")\n",
        "\n",
        "    # 1. Print dataset information\n",
        "    print(f\"Train dataset cardinality: {tf.data.experimental.cardinality(train_dataset)}\")\n",
        "    print(f\"Test dataset cardinality: {tf.data.experimental.cardinality(test_dataset)}\")\n",
        "\n",
        "    # 2. Inspect a few batches\n",
        "    print(\"\\nInspecting batches:\")\n",
        "    for i, (images, labels) in enumerate(train_dataset.take(10)):\n",
        "        print(f\"Batch {i+1}:\")\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Image min-max values: {tf.reduce_min(images).numpy()} - {tf.reduce_max(images).numpy()}\")\n",
        "        print(f\"  Unique labels: {np.unique(labels.numpy())}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 3. Check dataset configuration\n",
        "    print(\"\\nDataset configuration:\")\n",
        "    print(f\"Train dataset: {train_dataset}\")\n",
        "    print(f\"Test dataset: {test_dataset}\")\n",
        "\n",
        "    # 4. Verify steps per epoch\n",
        "    total_train_samples = tf.data.experimental.cardinality(train_dataset).numpy() * batch_size\n",
        "    calculated_steps = total_train_samples // batch_size\n",
        "    print(f\"\\nCalculated steps per epoch: {calculated_steps}\")\n",
        "    print(f\"Provided steps per epoch: {steps_per_epoch}\")\n",
        "    assert calculated_steps == steps_per_epoch, \"Steps per epoch mismatch\"\n",
        "\n",
        "    # 5. Test complete iteration\n",
        "    print(\"\\nTesting complete iteration:\")\n",
        "    for epoch in range(5):\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        for step, (images, labels) in enumerate(train_dataset):\n",
        "            if step % 100 == 0:\n",
        "                print(f\"  Step {step}: images shape {images.shape}, labels shape {labels.shape}\")\n",
        "            if step >= steps_per_epoch:\n",
        "                break\n",
        "\n",
        "    # 6. Check for data leakage\n",
        "    print(\"\\nChecking for data leakage...\")\n",
        "    train_sample = next(iter(train_dataset.take(100)))[0][0]\n",
        "    for test_batch in test_dataset.take(100):\n",
        "        assert not np.array_equal(train_sample, test_batch[0][0]), \"Data leakage detected\"\n",
        "    print(\"No data leakage detected\")\n",
        "\n",
        "    # 7. Verify data augmentation (if applicable)\n",
        "    # Uncomment and modify this section if you're using data augmentation\n",
        "    \"\"\"\n",
        "    print(\"\\nVerifying data augmentation:\")\n",
        "    for original, augmented in zip(train_dataset.take(1), train_dataset.map(augmentation_function).take(1)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(original[0][0])\n",
        "        plt.title(\"Original\")\n",
        "        plt.subplot(122)\n",
        "        plt.imshow(augmented[0][0])\n",
        "        plt.title(\"Augmented\")\n",
        "        plt.show()\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDataset verification complete.\")\n",
        "\n",
        "def create_model(num_classes):\n",
        "    base_model = ResNet50V2(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "\n",
        "    # Freeze the base model layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "def compile_model(model):\n",
        "    initial_learning_rate = 0.001\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate,\n",
        "        decay_steps=100,\n",
        "        decay_rate=0.96,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        accuracy = logs.get('accuracy', 'N/A')\n",
        "        accuracy_str = f\"{accuracy:.4f}\" if isinstance(accuracy, float) else str(accuracy)\n",
        "        print(f\" Epoch {epoch + 1} - Accuracy: {accuracy_str}\")\n",
        "        if isinstance(accuracy, float) and accuracy >= self.target_accuracy:\n",
        "            print(f\"Reached target accuracy of {self.target_accuracy}\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "# Main execution\n",
        "def main(verify_images, rename_dataset, plot_samples, verify_data, plot_history):\n",
        "\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "    # Mount Google Drive if using Colab\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH', default='/default/dataset/path')\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "\n",
        "    # Check TensorFlow Verision\n",
        "    print()\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    print(f\"Eager execution: {tf.executing_eagerly()}\")\n",
        "    print()\n",
        "    \n",
        "    if verify_images:\n",
        "        verify_images(f'{DATASET_PATH}/dataset_test')\n",
        "        verify_images(f'{DATASET_PATH}/structure_dataset')\n",
        "    \n",
        "    if rename_dataset:\n",
        "        start_directory = f'{DATASET_PATH}/structures_dataset'\n",
        "        rename_files(start_directory)\n",
        "        start_directory = f'{DATASET_PATH}/dataset_test'\n",
        "        rename_files(start_directory)\n",
        "    \n",
        "    if plot_samples:\n",
        "        plot_sample_images(f'{DATASET_PATH}/structures_dataset/')\n",
        "    \n",
        "    # Setup Dataset Directories\n",
        "    train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "    test_path = f'{DATASET_PATH}/dataset_test'\n",
        "    \n",
        "    # Set up data generators first to get class information\n",
        "    batch_size = 32\n",
        "    train_dataset, test_dataset, steps_per_epoch, validation_steps, classes, class_names = create_data_generators(train_path, test_path, batch_size)\n",
        "\n",
        "    if verify_data:\n",
        "        verify_dataset(train_dataset, test_dataset, batch_size, steps_per_epoch)\n",
        "\n",
        "    # Now we can use class_names for our existing count_samples function\n",
        "    train_counts = count_samples(train_path)\n",
        "    test_counts = count_samples(test_path)\n",
        "    \n",
        "    print(\"\\nTraining samples per class:\")\n",
        "    for class_name, count in sorted(train_counts.items()):\n",
        "        print(f\"{class_name}: {count}\")\n",
        "    print(\"\\nTest samples per class:\")\n",
        "    for class_name, count in sorted(test_counts.items()):\n",
        "        print(f\"{class_name}: {count}\")\n",
        "    print()\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "    \n",
        "    # Print class weights\n",
        "    print(\"Class weights:\")\n",
        "    for i, weight in enumerate(class_weights):\n",
        "        print(f\"Class {i} ({class_names[i]}): {weight}\")\n",
        "    print()\n",
        "    \n",
        "    # Create and compile the model\n",
        "    num_classes = len(class_names)\n",
        "    model = create_model(num_classes)\n",
        "    # print(model.summary())\n",
        "    compiled_model = compile_model(model)\n",
        "    \n",
        "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "    print(f\"Validation steps: {validation_steps}\")\n",
        "    print()\n",
        "    \n",
        "    # Setup Dataset Directories\n",
        "    train_path = f'{DATASET_PATH}/structures_dataset'\n",
        "    test_path = f'{DATASET_PATH}/dataset_test'\n",
        "\n",
        "    # Set up callbacks\n",
        "    accuracy_callback = AccuracyCallback(target_accuracy=0.98)\n",
        "    debug_callback = DebugCallback()\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    dataset_logger = DatasetLogger(train_dataset, test_dataset)\n",
        "    custom_validation = CustomValidationCallback(test_dataset, validation_steps)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(classes), y=classes)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    try:\n",
        "        # Separate evaluation before training\n",
        "        model.summary()\n",
        "        print()\n",
        "        print(\"Evaluating model before training:\")\n",
        "        evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print(f\"Initial evaluation: {evaluation}\")\n",
        "        print()\n",
        "\n",
        "        # Training\n",
        "        history = compiled_model.fit(\n",
        "            train_dataset,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            epochs=25,\n",
        "            validation_data=test_dataset,\n",
        "            validation_steps=validation_steps,\n",
        "            callbacks=[early_stopping, debug_callback, dataset_logger, custom_validation, accuracy_callback],\n",
        "            class_weight=class_weight_dict,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print()\n",
        "        print(\"Final evaluation:\")\n",
        "        print(\"-----------------\")\n",
        "        final_evaluation = compiled_model.evaluate(test_dataset, steps=validation_steps)\n",
        "        print()\n",
        "        print(f\"Final evaluation: {final_evaluation}\")\n",
        "        print()\n",
        "\n",
        "        if plot_history:\n",
        "            # Plot training history\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "            plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy')\n",
        "            plt.legend()\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(history.history['loss'], label='Training Loss')\n",
        "            plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "    except tf.errors.ResourceExhaustedError:\n",
        "        print()\n",
        "        print(\"Memory error occurred. Try reducing batch size or image dimensions.\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print()\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        print()\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "    return compiled_model, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compiled_model, class_names = main(verify_images=False, rename_dataset=False, plot_samples=False, verify_data=False, plot_history=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.13. <a id='toc1_4_1_13_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from dotenv import load_dotenv\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "def test_model_on_sample_images(model, class_names):\n",
        "\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    sample_images_dir = os.getenv('SAMPLE_IMAGES_DIR', default='/default/dataset/path')\n",
        "\n",
        "    # Collect image file paths\n",
        "    image_paths = [\n",
        "        os.path.join(sample_images_dir, fname)\n",
        "        for fname in os.listdir(sample_images_dir)\n",
        "        if fname.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "    ]\n",
        "\n",
        "    if not image_paths:\n",
        "        print(\"No images found in the directory.\")\n",
        "        return\n",
        "\n",
        "    for img_path in image_paths:\n",
        "        # Load and preprocess the image\n",
        "        img = load_img(img_path, target_size=(128, 128))\n",
        "        img_array = img_to_array(img)\n",
        "\n",
        "        # Rescale pixel values (assuming the model expects inputs in [0, 1])\n",
        "        img_array = img_array / 255.0\n",
        "\n",
        "        # Expand dimensions to match the model's input shape\n",
        "        img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "        # Predict the class\n",
        "        predictions = model.predict(img_array)\n",
        "        predicted_class = np.argmax(predictions[0])\n",
        "        predicted_class_name = class_names[predicted_class]\n",
        "        confidence = np.max(predictions[0]) * 100\n",
        "\n",
        "        # Display the image with its predicted class name and confidence\n",
        "        plt.figure()\n",
        "        plt.imshow(img)\n",
        "        plt.title(f\"Predicted: {predicted_class_name} ({confidence:.2f}% confidence)\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model_on_sample_images(compiled_model, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code sets up data generators for the training and test datasets, then trains the model using these generators. The `ImageDataGenerator` is used to load and preprocess images in batches, which is memory-efficient for large datasets.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Training the model without augmentation provides a baseline performance. It allows us to see how well the model performs with the original data before applying any data augmentation techniques. This step is crucial for understanding if the model has enough capacity to learn from the data and if there are any immediate issues like overfitting or underfitting.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.14. <a id='toc1_4_1_14_'></a>[**Train The Model With Augmentation**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHszyF2ednn5",
        "outputId": "edb3f833-7435-4d84-a818-16ed128a5ae6"
      },
      "outputs": [],
      "source": [
        "# !pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "import sys\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import traceback\n",
        "import warnings\n",
        "import yaml\n",
        "from collections import Counter\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import sklearn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.applications import (\n",
        "    EfficientNetB0,\n",
        "    InceptionV3,\n",
        "    MobileNetV2,\n",
        "    ResNet50V2,\n",
        "    VGG16\n",
        ")\n",
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from keras.callbacks import (\n",
        "    Callback,\n",
        "    EarlyStopping,\n",
        "    LambdaCallback,\n",
        "    LearningRateScheduler,\n",
        "    ModelCheckpoint,\n",
        "    ReduceLROnPlateau\n",
        ")\n",
        "from keras.layers import (\n",
        "    BatchNormalization,\n",
        "    Conv2D,\n",
        "    Dense,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    GlobalAveragePooling2D,\n",
        "    Input,\n",
        "    MaxPooling2D,\n",
        "    RandomRotation,\n",
        "    RandomFlip,\n",
        "    RandomZoom,\n",
        "    RandomContrast,\n",
        "    RandomBrightness,\n",
        "    RandomTranslation,\n",
        "    Rescaling\n",
        ")\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras.utils import Sequence\n",
        "from keras.metrics import Precision, Recall, AUC, Metric\n",
        "from keras.preprocessing import image\n",
        "\n",
        "# Import Keras Tuner modules\n",
        "from keras_tuner import (\n",
        "    Hyperband, \n",
        "    HyperModel, \n",
        "    HyperParameters, \n",
        "    BayesianOptimization, \n",
        "    RandomSearch\n",
        ")\n",
        "\n",
        "class F1Score(Metric):\n",
        "    def __init__(self, name='f1_score', **kwargs):\n",
        "        super(F1Score, self).__init__(name=name, **kwargs)\n",
        "        self.precision = Precision(name='precision')\n",
        "        self.recall = Recall(name='recall')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        # Update the precision and recall variables\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def result(self):\n",
        "        # Compute the F1 score\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "    def reset_states(self):\n",
        "        # Reset the state of the metrics\n",
        "        self.precision.reset_states()\n",
        "        self.recall.reset_states()\n",
        "\n",
        "# Define mappings for architectures and preprocessing functions\n",
        "ARCHITECTURES = {\n",
        "    'ResNet50V2': ResNet50V2,\n",
        "    'VGG16': VGG16,\n",
        "    'InceptionV3': InceptionV3,\n",
        "    'MobileNetV2': MobileNetV2,\n",
        "    'EfficientNetB0': EfficientNetB0\n",
        "}\n",
        "\n",
        "PREPROCESSING_FUNCTIONS = {\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'efficientnet_preprocess': efficientnet_preprocess\n",
        "}\n",
        "\n",
        "# Define metrics mapping\n",
        "METRICS = {\n",
        "    'Precision': Precision(name='precision'),\n",
        "    'Recall': Recall(name='recall'),\n",
        "    'AUC': AUC(name='auc'),\n",
        "    'F1Score': F1Score(name='f1_score')\n",
        "}\n",
        "\n",
        "class AccuracyCallback(Callback):\n",
        "    def __init__(self, target_accuracy):\n",
        "        super().__init__()\n",
        "        self.target_accuracy = target_accuracy\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('val_accuracy') >= self.target_accuracy:\n",
        "            print(f\"\\nReached {self.target_accuracy*100}% validation accuracy. Stopping training.\")\n",
        "            print()\n",
        "            print(\"--------------------\")\n",
        "            print()\n",
        "            self.model.stop_training = True\n",
        "\n",
        "class CustomValidationCallback(Callback):\n",
        "    def __init__(self, validation_data, validation_steps):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.validation_steps = validation_steps\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_loss = 0\n",
        "        val_accuracy = 0\n",
        "        for x, y in self.validation_data.take(self.validation_steps):\n",
        "            val_metrics = self.model.test_on_batch(x, y)\n",
        "            val_loss += val_metrics[0]\n",
        "            val_accuracy += val_metrics[1]\n",
        "\n",
        "        val_loss /= self.validation_steps\n",
        "        val_accuracy /= self.validation_steps\n",
        "\n",
        "        logs['val_loss'] = val_loss\n",
        "        logs['val_accuracy'] = val_accuracy\n",
        "        logger.debug(f\"Epoch {epoch + 1} - Custom validation:\")\n",
        "        logger.debug(f\"Loss: {val_loss:.4f}\")\n",
        "        logger.debug(f\"Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "class DatasetLogger(Callback):\n",
        "    def __init__(self, train_dataset, val_dataset):\n",
        "        super().__init__()\n",
        "        self.train_dataset = train_dataset\n",
        "        self.val_dataset = val_dataset\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logger.debug(f\"Epoch {epoch + 1} - Train samples: {tf.data.experimental.cardinality(self.train_dataset)}\")\n",
        "        logger.debug(f\"Epoch {epoch + 1} - Val samples: {tf.data.experimental.cardinality(self.val_dataset)}\")\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs is not None:\n",
        "            logger.debug(f\"Epoch {epoch + 1} - Train accuracy: {logs.get('accuracy', 'N/A'):.4f}\")\n",
        "            logger.debug(f\"Epoch {epoch + 1} - Val accuracy: {logs.get('val_accuracy', 'N/A'):.4f}\")\n",
        "\n",
        "class DebugCallback(Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        logger.debug(f\"Starting epoch {epoch + 1}\\n\")\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        if batch % 100 == 0:\n",
        "            # logger.debug(f\"Starting batch {batch}\\n\")\n",
        "            pass\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logger.debug(f\"End of epoch {epoch + 1}\\n\")\n",
        "        if logs:\n",
        "            for key, value in logs.items():\n",
        "                logger.debug(f\"{key}: {value}\")\n",
        "        logger.debug(\"--------------------\\n\")\n",
        "\n",
        "class DataGenerator:\n",
        "    def __init__(self, config):\n",
        "        logger.debug(f\"DataGenerator initialization starting.\")\n",
        "        self.config = config\n",
        "        self.batch_size = config['data']['batch_size']\n",
        "        self.input_shape = tuple(config['data']['input_shape'])\n",
        "        self.target_size = tuple(config['data']['target_size'])\n",
        "        self.color_mode = config['data'].get('color_mode', 'rgb')\n",
        "        self.preprocessing_function_name = config['data']['preprocessing_function']\n",
        "        \n",
        "        # Determine preprocessing function\n",
        "        use_pretrained_weights = config['model'].get('use_pretrained_weights', True)\n",
        "        if use_pretrained_weights:\n",
        "            self.preprocessing_function = PREPROCESSING_FUNCTIONS[config['data']['preprocessing_function']]\n",
        "        else:\n",
        "            # When training from scratch you may choose to use a simple rescaling\n",
        "            self.preprocessing_function = lambda x: x / 255.0  # Or define a custom function\n",
        "            \n",
        "        self.augmentation_params = config['augmentation']\n",
        "        self.pre_split = config['data'].get('pre_split', True)\n",
        "\n",
        "        logger.debug(f'DG batch_size = {self.batch_size}')\n",
        "        logger.debug(f'DG target_zie = {self.target_size}')\n",
        "        logger.debug(f'DG preprocessing_function = {self.preprocessing_function_name}')\n",
        "        logger.debug(f'DG augmentation_params = {self.augmentation_params}')\n",
        "        logger.debug(f'DG pre_split = {self.pre_split}')\n",
        "\n",
        "        # Create data augmentation and rescaling layers\n",
        "        self.data_augmentation = self.create_data_augmentation()\n",
        "        self.rescale_layer = self.create_rescale_layer()\n",
        "\n",
        "        if self.pre_split:\n",
        "            logger.debug(\"DG Calling load_pre_split_data function.\")\n",
        "            self.load_pre_split_data()\n",
        "        else:\n",
        "            self.load_and_split_data()\n",
        "\n",
        "    def load_pre_split_data(self):\n",
        "        # Paths for pre-split data\n",
        "        logger.debug(\"DG LPSD Starting load_pre_split_data function.\")\n",
        "        self.train_path = self.config['data']['train_path']\n",
        "        logger.debug(f\"DG LPSD Train path: {self.train_path}\")\n",
        "        self.test_path = self.config['data']['test_path']\n",
        "        logger.debug(f\"DG LPSD Test path: {self.test_path}\")\n",
        "\n",
        "        # Validate paths\n",
        "        if not os.path.exists(self.train_path):\n",
        "            raise FileNotFoundError(f\"DG LPSD Training path not found: {self.train_path}\")\n",
        "        if not os.path.exists(self.test_path):\n",
        "            raise FileNotFoundError(f\"DG LPSD Testing path not found: {self.test_path}\")\n",
        "\n",
        "        # Load datasets\n",
        "        logger.debug(\"DG LPSD Loading train_dataset datasets\")\n",
        "        self.train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.train_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=None,  # Load as individual samples\n",
        "            image_size=self.target_size,\n",
        "            color_mode=self.color_mode,\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        logger.debug(\"DG LPSD Loading test_dataset datasets\")  \n",
        "        self.test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            self.test_path,\n",
        "            label_mode='categorical',\n",
        "            batch_size=None,\n",
        "            image_size=self.target_size,\n",
        "            color_mode=self.color_mode,\n",
        "            shuffle=False\n",
        "        )\n",
        "\n",
        "        train_dataset_unbatched = self.train_dataset\n",
        "        test_dataset_unbatched = self.test_dataset\n",
        "\n",
        "        self.class_names = self.train_dataset.class_names\n",
        "\n",
        "        logger.debug(f\"DG LPSD Class names: {self.class_names}\")\n",
        "\n",
        "        # Prepare datasets\n",
        "        logger.debug(\"DG LPSD Preparing datasets\")\n",
        "        self.train_dataset = self.prepare_dataset(self.train_dataset, augment=True)\n",
        "        logger.debug(\"DG LPSD Train dataset prepared.\")\n",
        "        self.val_dataset = self.prepare_dataset(self.test_dataset, augment=False)\n",
        "        logger.debug(\"DG LPSD Val dataset prepared.\")\n",
        "        self.test_dataset = self.val_dataset  # Use the validation dataset for testing if appropriate\n",
        "        logger.debug(\"DG LPSD Test dataset prepared.\")\n",
        "\n",
        "        # Compute sample counts\n",
        "        self.train_sample_count = tf.data.experimental.cardinality(train_dataset_unbatched).numpy()\n",
        "        self.val_sample_count = tf.data.experimental.cardinality(test_dataset_unbatched).numpy()\n",
        "        self.steps_per_epoch = math.ceil(self.train_sample_count / self.batch_size)\n",
        "        self.validation_steps = math.ceil(self.val_sample_count / self.batch_size)\n",
        "        \n",
        "        # Compute class counts directly from the dataset's file paths\n",
        "        self.class_counts = self.count_samples_from_directories(self.train_path, self.class_names)\n",
        "        \n",
        "        # Compute class weights\n",
        "        self.class_weight_dict = compute_class_weights_from_counts(self.class_counts, self.class_names)\n",
        "        \n",
        "        print(f'\\nDG LPSD Train path: {self.train_path}')\n",
        "        print(f'DG LPSD Test path: {self.test_path}')\n",
        "        print(f'DG LPSD Batch size: {self.batch_size}')\n",
        "        print(f'DG LPSD Input shape: {self.input_shape}')\n",
        "        print(f'DG LPSD color_mode: {self.color_mode}')\n",
        "        print(f'DG LPSD Target size: {self.target_size}')\n",
        "        print(f'DG LPSD steps_per_epoch: {self.steps_per_epoch}')\n",
        "        print(f'DG LPSD validation_steps: {self.validation_steps}')\n",
        "        print(f'DG LPSD Preprocessing function: {self.preprocessing_function_name}')\n",
        "        print(f'DG LPSD Augmentation params: {self.augmentation_params}')\n",
        "        print(f'DG LPSD Class names: {self.class_names}')\n",
        "        print(f'DG LPSD Class counts: {self.class_counts}')\n",
        "        print(f'DG LPSD Class weights: {self.class_weight_dict}')\n",
        "        print(f'DG LPSD Training set size: {self.train_sample_count}')\n",
        "        print(f'DG LPSD Validation set size: {self.val_sample_count}')\n",
        "        print(f'DG LPSD Testing set size: {self.val_sample_count}\\n')\n",
        "\n",
        "    def count_samples_from_directories(self, dataset_path, class_names):\n",
        "        import os\n",
        "        counts = {}\n",
        "        for class_name in class_names:\n",
        "            class_dir = os.path.join(dataset_path, class_name)\n",
        "            if os.path.isdir(class_dir):\n",
        "                counts[class_name] = len([\n",
        "                    fname for fname in os.listdir(class_dir)\n",
        "                    if os.path.isfile(os.path.join(class_dir, fname))\n",
        "                ])\n",
        "            else:\n",
        "                counts[class_name] = 0\n",
        "        return counts\n",
        "\n",
        "    def load_and_split_data(self):\n",
        "        logger.debug(\"DG LASD Getting Class Names.\")\n",
        "        self.class_names = [\n",
        "            d for d in sorted(os.listdir(self.config['data']['dataset_path']))\n",
        "            if os.path.isdir(os.path.join(self.config['data']['dataset_path'], d))\n",
        "        ]\n",
        "        logger.debug(f\"DG LASD Class Names: {self.class_names}\")\n",
        "        class_indices = {name: index for index, name in enumerate(self.class_names)}\n",
        "        logger.debug(f\"DG LASD Class Indices: {class_indices}\")\n",
        "\n",
        "        # Collect file paths and labels\n",
        "        file_paths = []\n",
        "        labels = []\n",
        "        for class_name in self.class_names:\n",
        "            class_dir = os.path.join(self.config['data']['dataset_path'], class_name)\n",
        "            class_files = glob.glob(os.path.join(class_dir, '*'))\n",
        "            file_paths.extend(class_files)\n",
        "            labels.extend([class_indices[class_name]] * len(class_files))\n",
        "\n",
        "        file_paths = np.array(file_paths)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # First split: train and temp (val + test)\n",
        "        logger.debug(\"DG LASD Splitting data into train and temp sets.\")\n",
        "        train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "            file_paths, labels,\n",
        "            test_size=0.3,\n",
        "            stratify=labels,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Second split: validation and test\n",
        "        logger.debug(\"DG LASD Splitting temp data into val and test sets.\")\n",
        "        val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "            temp_paths, temp_labels,\n",
        "            test_size=0.5,\n",
        "            stratify=temp_labels,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Mapping indices back to class names for readability\n",
        "        index_to_class = {index: name for name, index in class_indices.items()}\n",
        "\n",
        "        # Training set class distribution\n",
        "        train_class_counts = Counter(train_labels)\n",
        "        train_class_counts_named = {index_to_class[k]: v for k, v in train_class_counts.items()}\n",
        "        logger.debug(f\"DG LASD Training class distribution: {train_class_counts_named}\")\n",
        "\n",
        "        # Validation set class distribution\n",
        "        val_class_counts = Counter(val_labels)\n",
        "        val_class_counts_named = {index_to_class[k]: v for k, v in val_class_counts.items()}\n",
        "        logger.debug(f\"DG LASD Validation class distribution: {val_class_counts_named}\")\n",
        "\n",
        "        # Test set class distribution\n",
        "        test_class_counts = Counter(test_labels)\n",
        "        test_class_counts_named = {index_to_class[k]: v for k, v in test_class_counts.items()}\n",
        "        logger.debug(f\"DG LASD Test class distribution: {test_class_counts_named}\")\n",
        "\n",
        "        # Store the training class counts as an attribute\n",
        "        self.class_counts = train_class_counts_named\n",
        "\n",
        "        # Create datasets from file paths and labels\n",
        "        logger.debug(\"DG LASD Creating datasets from file paths and labels.\")\n",
        "        train_dataset = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "        val_dataset = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "        test_dataset = tf.data.Dataset.from_tensor_slices((test_paths, test_labels))\n",
        "\n",
        "        # Get color_mode from config (default to 'rgb')\n",
        "        color_mode = self.config['data'].get('color_mode', 'rgb').lower()\n",
        "\n",
        "        # Set the number of channels based on color mode\n",
        "        channels = 3 if color_mode == 'rgb' else 1\n",
        "\n",
        "        # Define the load_image function inside the method\n",
        "        def load_image(file_path, label):\n",
        "            # Read the image from file\n",
        "            image = tf.io.read_file(file_path)\n",
        "            # Decode the image data (supports JPEG, PNG, BMP, and GIF)\n",
        "            image = tf.image.decode_image(image, channels=channels)\n",
        "            # Set static shape if possible\n",
        "            if channels == 3:\n",
        "                image.set_shape([None, None, 3])\n",
        "            else:\n",
        "                image.set_shape([None, None, 1])\n",
        "            # Convert image to float32 and resize\n",
        "            image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "            # Resize image to target size\n",
        "            image = tf.image.resize(image, self.target_size)\n",
        "            # Apply preprocessing function if specified\n",
        "            if self.preprocessing_function:\n",
        "                image = self.preprocessing_function(image)\n",
        "            else: \n",
        "                # Default normalization if no preprocessing function is specified\n",
        "                image = image / 255.0\n",
        "            \n",
        "            # One-hot encode the label\n",
        "            label = tf.one_hot(label, depth=len(self.class_names))\n",
        "            return image, label\n",
        "\n",
        "        # Map function to load images from file paths\n",
        "        logger.debug(\"DG LASD Mapping load_image function to datasets.\")\n",
        "        train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Prepare datasets\n",
        "        logger.debug(\"DG LASD Preparing datasets.\")\n",
        "        self.train_dataset = self.prepare_dataset(train_dataset, augment=True)\n",
        "        self.val_dataset = self.prepare_dataset(val_dataset, augment=False)\n",
        "        self.test_dataset = self.prepare_dataset(test_dataset, augment=False)\n",
        "\n",
        "        # Compute sample counts\n",
        "        logger.debug(\"DG LASD Computing sample counts.\")\n",
        "        self.train_sample_count = len(train_paths)\n",
        "        self.val_sample_count = len(val_paths)\n",
        "        self.test_sample_count = len(test_paths)\n",
        "\n",
        "        self.steps_per_epoch = math.ceil(self.train_sample_count / self.batch_size)\n",
        "        self.validation_steps = math.ceil(self.val_sample_count / self.batch_size)\n",
        "        self.test_steps = math.ceil(self.test_sample_count / self.batch_size)\n",
        "        \n",
        "        # Compute class counts directly from the dataset's file paths\n",
        "        self.class_counts = self.count_samples_from_directories(self.config['data']['dataset_path'], self.class_names)\n",
        "        \n",
        "        print(f'\\nDG LASD Batch size: {self.batch_size}')\n",
        "        print(f'DG LASD Input shape: {self.input_shape}')\n",
        "        print(f'DG LASD color_mode: {self.color_mode}')\n",
        "        print(f'DG LASD Target size: {self.target_size}')\n",
        "        print(f'DG LASD Steps per epoch: {self.steps_per_epoch}')\n",
        "        print(f'DG LASD Validation steps: {self.validation_steps}')\n",
        "        print(f'DG LASD Test steps: {self.test_steps}')\n",
        "        print(f'DG LASD Augmentation params: {self.augmentation_params}')\n",
        "        print(f'DG LASD Class names: {self.class_names}')\n",
        "        print(f'DG LASD Class counts: {self.class_counts}')\n",
        "        print(f'DG LASD Training sample size: {self.train_sample_count}')\n",
        "        print(f'DG LASD Training class distribution: {train_class_counts_named}')\n",
        "        print(f'DG LASD Validation sample size: {self.val_sample_count}')\n",
        "        print(f'DG LASD Validation class distribution: {val_class_counts_named}')\n",
        "        print(f'DG LASD Test sample size: {self.test_sample_count}')\n",
        "        print(f'DG LASD Test class distribution: {test_class_counts_named}')\n",
        "        \n",
        "    def split_dataset(self):\n",
        "        # Calculate dataset size\n",
        "        dataset_size = tf.data.experimental.cardinality(self.dataset).numpy()\n",
        "\n",
        "        # Define split sizes\n",
        "        train_size = int(0.7 * dataset_size)\n",
        "        val_size = int(0.15 * dataset_size)\n",
        "        test_size = dataset_size - train_size - val_size\n",
        "\n",
        "        # Shuffle and split\n",
        "        self.dataset = self.dataset.shuffle(buffer_size=dataset_size, seed=42)\n",
        "        train_dataset = self.dataset.take(train_size)\n",
        "        val_test_dataset = self.dataset.skip(train_size)\n",
        "        val_dataset = val_test_dataset.take(val_size)\n",
        "        test_dataset = val_test_dataset.skip(val_size)\n",
        "\n",
        "        logger.debug(f\"DG SD Dataset size: {dataset_size}\")\n",
        "        logger.debug(f\"DG SD Training size: {train_size}\")\n",
        "        logger.debug(f\"DG SD Validation size: {val_size}\")\n",
        "        logger.debug(f\"DG SD Test size: {test_size}\")\n",
        "\n",
        "        return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "    def prepare_dataset(self, dataset, augment):\n",
        "        if augment:\n",
        "            try:\n",
        "                dataset = dataset.map(self.augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                dataset = dataset.shuffle(1000).repeat()\n",
        "            except Exception as e:\n",
        "                logger.error(f\"DG SD An error occurred trying to prepare dataset with augment true: {e}\")\n",
        "                logger.debug(traceback.format_exc())\n",
        "        else:\n",
        "            dataset = dataset.map(self.normalize_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            dataset = dataset.cache()\n",
        "        dataset = dataset.batch(self.batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    def get_dataset_size(self, dataset):\n",
        "        return tf.data.experimental.cardinality(dataset).numpy() * self.batch_size\n",
        "\n",
        "    def create_data_augmentation(self):\n",
        "        layers = []\n",
        "        augmentation_params = self.augmentation_params\n",
        "\n",
        "        # Apply augmentations based on parameters\n",
        "        if augmentation_params.get('rotation_range'):\n",
        "            rotation_range = augmentation_params['rotation_range']\n",
        "            factor = rotation_range / 360.0  # Convert degrees to fraction of full circle\n",
        "            # Ensure factor is within [-1.0, 1.0]\n",
        "            factor = max(min(factor, 1.0), -1.0)\n",
        "            layers.append(RandomRotation(factor=(-factor, factor)))\n",
        "\n",
        "        if augmentation_params.get('horizontal_flip'):\n",
        "            layers.append(RandomFlip(mode='horizontal'))\n",
        "\n",
        "        if augmentation_params.get('vertical_flip'):\n",
        "            layers.append(RandomFlip(mode='vertical'))\n",
        "\n",
        "        if augmentation_params.get('zoom_range'):\n",
        "            zoom = augmentation_params['zoom_range']\n",
        "            # RandomZoom expects height_factor and width_factor in [-1.0, 1.0]\n",
        "            # Ensure zoom is within [0.0, 1.0] to avoid invalid factors\n",
        "            zoom = max(min(zoom, 1.0), 0.0)\n",
        "            layers.append(RandomZoom(height_factor=(-zoom, zoom), width_factor=(-zoom, zoom)))\n",
        "\n",
        "        if augmentation_params.get('width_shift_range') or augmentation_params.get('height_shift_range'):\n",
        "            width_shift = augmentation_params.get('width_shift_range', 0.0)\n",
        "            height_shift = augmentation_params.get('height_shift_range', 0.0)\n",
        "            # RandomTranslation expects height_factor and width_factor in [-1.0, 1.0]\n",
        "            width_shift = max(min(width_shift, 1.0), -1.0)\n",
        "            height_shift = max(min(height_shift, 1.0), -1.0)\n",
        "            layers.append(RandomTranslation(height_factor=height_shift, width_factor=width_shift))\n",
        "\n",
        "        if augmentation_params.get('brightness_range'):\n",
        "            brightness = augmentation_params['brightness_range']\n",
        "            # RandomBrightness expects factor in [0.0, inf), but to avoid extreme brightness, cap it\n",
        "            brightness = max(brightness, 0.0)\n",
        "            layers.append(RandomBrightness(factor=brightness))\n",
        "\n",
        "        if augmentation_params.get('contrast_range'):\n",
        "            contrast = augmentation_params['contrast_range']\n",
        "            # RandomContrast expects factor in [0.0, inf), but to avoid extreme contrast, cap it\n",
        "            contrast = max(contrast, 0.0)\n",
        "            layers.append(RandomContrast(factor=contrast))\n",
        "            \n",
        "        if not layers:\n",
        "            layers.append(tf.keras.layers.Lambda(lambda x: x))\n",
        "\n",
        "        data_augmentation = tf.keras.Sequential(layers)\n",
        "        return data_augmentation\n",
        "\n",
        "    def create_rescale_layer(self):\n",
        "        # Define which preprocessing functions expect which input ranges\n",
        "        preprocess_0_255 = [resnet_preprocess, vgg_preprocess]\n",
        "        preprocess_0_1 = [efficientnet_preprocess]\n",
        "        preprocess_minus1_1 = [mobilenet_preprocess, inception_preprocess]\n",
        "\n",
        "        if self.preprocessing_function in preprocess_0_255:\n",
        "            # No rescaling needed; images are already in [0, 255]\n",
        "            return None\n",
        "        elif self.preprocessing_function in preprocess_0_1:\n",
        "            # Rescaling needed to bring images to [0, 1]\n",
        "            return Rescaling(1./255)\n",
        "        elif self.preprocessing_function in preprocess_minus1_1:\n",
        "            # Rescaling needed to bring images to [0, 1]; preprocessing function will scale to [-1, 1]\n",
        "            return Rescaling(1./255)\n",
        "        else:\n",
        "            # Default to rescaling to [0, 1]\n",
        "            return Rescaling(1./255)\n",
        "\n",
        "    def augment(self, images, labels):\n",
        "        # Different preprocessing functions expect different ranges of images after augmentation\n",
        "        # ResNet50V2 and VGG16: Expect images in the range [0, 255] with mean subtraction.\n",
        "        # InceptionV3 and MobileNetV2: Expect images scaled to [-1, 1].\n",
        "        # EfficientNetB0: Expects images scaled to [0, 1].\n",
        "        \n",
        "        images = tf.cast(images, tf.float32)\n",
        "\n",
        "        # Apply rescaling if necessary\n",
        "        if self.rescale_layer:\n",
        "            images = self.rescale_layer(images)\n",
        "\n",
        "        # Apply data augmentation\n",
        "        images = self.data_augmentation(images)\n",
        "\n",
        "        # Apply preprocessing function if it exists\n",
        "        if self.preprocessing_function is not None:\n",
        "            images = self.preprocessing_function(images)\n",
        "            \n",
        "        return images, labels\n",
        "\n",
        "    def normalize_and_preprocess(self, images, labels):\n",
        "        images = tf.cast(images, tf.float32)\n",
        "\n",
        "        # Apply rescaling if necessary\n",
        "        if self.rescale_layer:\n",
        "            images = self.rescale_layer(images)\n",
        "\n",
        "        # Apply preprocessing function if it exists\n",
        "        if self.preprocessing_function is not None:\n",
        "            images = self.preprocessing_function(images)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def create_datasets(self):\n",
        "        return None\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "    def __init__(self, config, num_classes, best_hyperparameters=None):\n",
        "        \"\"\"\n",
        "        Initializes the HyperModel.\n",
        "\n",
        "        Args:\n",
        "            config (dict): Configuration dictionary.\n",
        "            num_classes (int): Number of output classes.\n",
        "            best_hyperparameters (HyperParameters, optional): Best hyperparameters from tuning.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.num_classes = num_classes\n",
        "        self.best_hyperparameters = best_hyperparameters\n",
        "\n",
        "    def build(self, hp):\n",
        "        \"\"\"\n",
        "        Builds the model based on whether to use pre-trained weights or not.\n",
        "\n",
        "        Args:\n",
        "            hp (HyperParameters): Hyperparameters for tuning.\n",
        "\n",
        "        Returns:\n",
        "            keras.Model: Compiled Keras model.\n",
        "        \"\"\"\n",
        "        # Determine if we're using a pre-trained model\n",
        "        model_config = self.config['model']\n",
        "        use_pretrained_weights = model_config.get('use_pretrained_weights', True)\n",
        "        \n",
        "        # If hp is None, use best_hyperparameters or default fixed hyperparameters\n",
        "        if hp is None:\n",
        "            if self.best_hyperparameters is not None:\n",
        "                hp = self.best_hyperparameters\n",
        "            else:\n",
        "                # Create a default HyperParameters object with fixed values from config\n",
        "                hp = HyperParameters()\n",
        "                \n",
        "                print(f\"\\nMHM-b no best hyperparameters found setting values to defaults from config file\")\n",
        "                \n",
        "                if use_pretrained_weights:\n",
        "                    # Pre-trained model hyperparameters\n",
        "                    hp.Fixed('num_dense_layers', self.config['hyperparameters']['pretrained_model']['num_dense_layers']['default'])\n",
        "                    hp.Fixed('dense_units', self.config['hyperparameters']['pretrained_model']['dense_units']['default'])\n",
        "                    hp.Fixed('dropout_rate', self.config['hyperparameters']['pretrained_model']['dropout_rate']['default'])\n",
        "                    hp.Fixed('use_batch_norm', self.config['hyperparameters']['pretrained_model']['use_batch_norm']['default'])\n",
        "                    hp.Fixed('optimizer', self.config['hyperparameters']['pretrained_model']['optimizer']['default'])\n",
        "                    hp.Fixed('learning_rate', float(self.config['hyperparameters']['pretrained_model']['learning_rate']['default']))\n",
        "                    print(f\"\\nMHM-b setting num_dense_layers to default value: {hp.get('num_dense_layers')}\")\n",
        "                    print(f\"MHM-b setting dense_units to default value: {hp.get('dense_units')}\")\n",
        "                    print(f\"MHM-b setting dropout_rate to default value: {hp.get('dropout_rate')}\")\n",
        "                    print(f\"MHM-b setting use_batch_norm to default value: {hp.get('use_batch_norm')}\")\n",
        "                    print(f\"MHM-b setting optimizer to default value: {hp.get('optimizer')}\")\n",
        "                    print(f\"MHM-b setting learning_rate to default value: {hp.get('learning_rate')}\")\n",
        "                else:\n",
        "                    # Scratch model hyperparameters\n",
        "                    hp.Fixed('num_conv_layers', self.config['hyperparameters']['scratch_model']['num_conv_layers']['default'])\n",
        "                    hp.Fixed('conv_filters_scratch', self.config['hyperparameters']['scratch_model']['conv_filters_scratch']['default'])\n",
        "                    hp.Fixed('conv_kernel_size_scratch', self.config['hyperparameters']['scratch_model']['conv_kernel_size_scratch']['default'])\n",
        "                    hp.Fixed('use_conv_batch_norm_scratch', self.config['hyperparameters']['scratch_model']['use_conv_batch_norm_scratch']['default'])\n",
        "                    hp.Fixed('conv_dropout_rate_scratch', self.config['hyperparameters']['scratch_model']['conv_dropout_rate_scratch']['default'])\n",
        "                    hp.Fixed('num_dense_layers_scratch', self.config['hyperparameters']['scratch_model']['num_dense_layers_scratch']['default'])\n",
        "                    hp.Fixed('dense_units_scratch', self.config['hyperparameters']['scratch_model']['dense_units_scratch']['default'])\n",
        "                    hp.Fixed('use_dense_batch_norm_scratch', self.config['hyperparameters']['scratch_model']['use_dense_batch_norm_scratch']['default'])\n",
        "                    hp.Fixed('dropout_rate_scratch', self.config['hyperparameters']['scratch_model']['dropout_rate_scratch']['default'])\n",
        "                    hp.Fixed('optimizer_scratch', self.config['hyperparameters']['scratch_model']['optimizer_scratch']['default'])\n",
        "                    hp.Fixed('learning_rate_scratch', float(self.config['hyperparameters']['scratch_model']['learning_rate_scratch']['default']))\n",
        "                    print(f\"\\nMHM-b setting num_conv_layers to default value: {hp.get('num_conv_layers')}\")\n",
        "                    print(f\"MHM-b setting conv_filters_scratch to default value: {hp.get('conv_filters_scratch')}\")\n",
        "                    print(f\"MHM-b setting conv_kernel_size_scratch to default value: {hp.get('conv_kernel_size_scratch')}\")\n",
        "                    print(f\"MHM-b setting use_conv_batch_norm_scratch to default value: {hp.get('use_conv_batch_norm_scratch')}\")\n",
        "                    print(f\"MHM-b setting conv_dropout_rate_scratch to default value: {hp.get('conv_dropout_rate_scratch')}\")\n",
        "                    print(f\"MHM-b setting num_dense_layers_scratch to default value: {hp.get('num_dense_layers_scratch')}\")\n",
        "                    print(f\"MHM-b setting dense_units_scratch to default value: {hp.get('dense_units_scratch')}\")\n",
        "                    print(f\"MHM-b setting use_dense_batch_norm_scratch to default value: {hp.get('use_dense_batch_norm_scratch')}\")\n",
        "                    print(f\"MHM-b setting dropout_rate_scratch to default value: {hp.get('dropout_rate_scratch')}\")\n",
        "                    print(f\"MHM-b setting optimizer_scratch to default value: {hp.get('optimizer_scratch')}\")\n",
        "                    print(f\"MHM-b setting learning_rate_scratch to default value: {hp.get('learning_rate_scratch')}\")\n",
        "        \n",
        "        if use_pretrained_weights:\n",
        "            # **Using Pre-trained Architecture**\n",
        "            architecture_name = model_config['name']\n",
        "            architecture = ARCHITECTURES[architecture_name]\n",
        "            input_shape = tuple(model_config['input_shape'])\n",
        "            base_model_weights = 'imagenet' if use_pretrained_weights else None\n",
        "\n",
        "            print(f\"\\nMHM-b Building model with pre-trained model {architecture_name} with input shape {input_shape} and using the {base_model_weights} base model weights\")\n",
        "\n",
        "            # Load the base model with or without pre-trained weights\n",
        "            base_model = architecture(\n",
        "                weights=base_model_weights,\n",
        "                include_top=False,\n",
        "                input_shape=input_shape,\n",
        "                pooling='avg',  # Use global average pooling to reduce the need for Flatten\n",
        "                name='base_model'  # Assign a name for easy access\n",
        "            )\n",
        "\n",
        "            # Freeze the base model initially\n",
        "            base_model.trainable = False\n",
        "\n",
        "            # Input layer\n",
        "            inputs = Input(shape=input_shape)\n",
        "            x = base_model(inputs, training=False)\n",
        "\n",
        "            # Hyperparameter for the number of dense layers\n",
        "            num_dense_layers = hp.Int(\n",
        "                'num_dense_layers',\n",
        "                min_value=self.config['hyperparameters']['pretrained_model']['num_dense_layers']['min'],\n",
        "                max_value=self.config['hyperparameters']['pretrained_model']['num_dense_layers']['max'],\n",
        "                default=self.config['hyperparameters']['pretrained_model']['num_dense_layers']['default']\n",
        "            )\n",
        "\n",
        "            # Integrate hyperparameters for dense units and dropout rate\n",
        "            dense_units = hp.Int(\n",
        "                'dense_units',\n",
        "                min_value=self.config['hyperparameters']['pretrained_model']['dense_units']['min'],\n",
        "                max_value=self.config['hyperparameters']['pretrained_model']['dense_units']['max'],\n",
        "                step=self.config['hyperparameters']['pretrained_model']['dense_units']['step'],\n",
        "                default=self.config['hyperparameters']['pretrained_model']['dense_units']['default']\n",
        "            )\n",
        "            dropout_rate = hp.Float(\n",
        "                'dropout_rate',\n",
        "                min_value=self.config['hyperparameters']['pretrained_model']['dropout_rate']['min'],\n",
        "                max_value=self.config['hyperparameters']['pretrained_model']['dropout_rate']['max'],\n",
        "                step=self.config['hyperparameters']['pretrained_model']['dropout_rate']['step'],\n",
        "                default=self.config['hyperparameters']['pretrained_model']['dropout_rate']['default']\n",
        "            )\n",
        "            use_batch_norm = hp.Boolean(\n",
        "                'use_batch_norm',\n",
        "                default=self.config['hyperparameters']['pretrained_model']['use_batch_norm']['default']\n",
        "            )\n",
        "\n",
        "            # Build the classification head using global hyperparameters\n",
        "            for _ in range(num_dense_layers):\n",
        "                x = Dense(dense_units, activation='relu')(x)\n",
        "                if use_batch_norm:\n",
        "                    x = BatchNormalization()(x)\n",
        "                if dropout_rate > 0.0:\n",
        "                    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "            # Output layer\n",
        "            output = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "            # Create the model\n",
        "            model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "            # Compile the model with hyperparameter-defined optimizer\n",
        "            learning_rate = hp.Float(\n",
        "                'learning_rate',\n",
        "                min_value=float(self.config['hyperparameters']['pretrained_model']['learning_rate']['min']),\n",
        "                max_value=float(self.config['hyperparameters']['pretrained_model']['learning_rate']['max']),\n",
        "                sampling='log',\n",
        "                default=float(self.config['hyperparameters']['pretrained_model']['learning_rate']['default'])\n",
        "            )\n",
        "            optimizer_choice = hp.Choice(\n",
        "                'optimizer',\n",
        "                values=self.config['hyperparameters']['pretrained_model']['optimizer']['choices'],\n",
        "                default=self.config['hyperparameters']['pretrained_model']['optimizer']['default']\n",
        "            )\n",
        "\n",
        "            if optimizer_choice == 'adam':\n",
        "                optimizer = Adam(learning_rate=learning_rate)\n",
        "            elif optimizer_choice == 'sgd':\n",
        "                optimizer = SGD(learning_rate=learning_rate)\n",
        "            else:\n",
        "                optimizer = Adam(learning_rate=learning_rate)  # Default to Adam\n",
        "\n",
        "            metrics = ['accuracy'] + [METRICS[metric] for metric in self.config['model']['additional_metrics']]\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=metrics\n",
        "            )\n",
        "\n",
        "            # Print the model summary\n",
        "            print(\"\\nModel Summary:\")\n",
        "            model.summary()\n",
        "            print(\"\\n\")\n",
        "\n",
        "            return model\n",
        "        \n",
        "        else:\n",
        "            # **Building Custom Model from Scratch**\n",
        "            print(f'\\nMhM-b Building model from scratch.')\n",
        "            print()\n",
        "\n",
        "            # Input layer\n",
        "            inputs = Input(shape=tuple(self.config['data']['input_shape']))\n",
        "            x = inputs\n",
        "\n",
        "            # Hyperparameter for the number of convolutional layers\n",
        "            num_conv_layers = hp.Int(\n",
        "                'num_conv_layers',\n",
        "                min_value=self.config['hyperparameters']['scratch_model']['num_conv_layers']['min'],\n",
        "                max_value=self.config['hyperparameters']['scratch_model']['num_conv_layers']['max'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['num_conv_layers']['default']\n",
        "            )\n",
        "            \n",
        "            # Global hyperparameters for convolutional layers\n",
        "            # Hyperparameters for number of convolutional filters\n",
        "            filters = hp.Int(\n",
        "                'conv_filters_scratch',\n",
        "                min_value=self.config['hyperparameters']['scratch_model']['conv_filters_scratch']['min'],\n",
        "                max_value=self.config['hyperparameters']['scratch_model']['conv_filters_scratch']['max'],\n",
        "                step=self.config['hyperparameters']['scratch_model']['conv_filters_scratch']['step'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['conv_filters_scratch']['default']\n",
        "            )\n",
        "            # Hyperparameters for kernel size\n",
        "            kernel_size = hp.Choice(\n",
        "                'conv_kernel_size_scratch',\n",
        "                values=self.config['hyperparameters']['scratch_model']['conv_kernel_size_scratch']['choices'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['conv_kernel_size_scratch']['default']\n",
        "            )\n",
        "            # Hyperparameters for convolutional batch normalization\n",
        "            use_conv_batch_norm = hp.Boolean(\n",
        "                'use_conv_batch_norm_scratch',\n",
        "                default=self.config['hyperparameters']['scratch_model']['use_conv_batch_norm_scratch']['default']\n",
        "            )\n",
        "            # Hyperparameters for convolutional dropout rate\n",
        "            conv_dropout_rate = hp.Float(\n",
        "                'conv_dropout_rate_scratch',\n",
        "                min_value=float(self.config['hyperparameters']['scratch_model']['conv_dropout_rate_scratch']['min']),\n",
        "                max_value=float(self.config['hyperparameters']['scratch_model']['conv_dropout_rate_scratch']['max']),\n",
        "                step=float(self.config['hyperparameters']['scratch_model']['conv_dropout_rate_scratch']['step']),\n",
        "                default=float(self.config['hyperparameters']['scratch_model']['conv_dropout_rate_scratch']['default'])\n",
        "            )\n",
        "\n",
        "            # Build convolutional layers\n",
        "            for i in range(num_conv_layers):\n",
        "                x = Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', padding='same')(x)\n",
        "                if use_conv_batch_norm:\n",
        "                    x = BatchNormalization()(x)\n",
        "                x = MaxPooling2D((2, 2))(x)\n",
        "                if conv_dropout_rate > 0.0:\n",
        "                    x = Dropout(conv_dropout_rate)(x)\n",
        "\n",
        "            x = Flatten()(x)\n",
        "\n",
        "            # Hyperparameter for the number of dense layers\n",
        "            num_dense_layers = hp.Int(\n",
        "                'num_dense_layers_scratch',\n",
        "                min_value=self.config['hyperparameters']['scratch_model']['num_dense_layers_scratch']['min'],\n",
        "                max_value=self.config['hyperparameters']['scratch_model']['num_dense_layers_scratch']['max'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['num_dense_layers_scratch']['default']\n",
        "            )\n",
        "\n",
        "            # Global hyperparameters for dense layers\n",
        "            dense_units = hp.Int(\n",
        "                'dense_units_scratch',\n",
        "                min_value=self.config['hyperparameters']['scratch_model']['dense_units_scratch']['min'],\n",
        "                max_value=self.config['hyperparameters']['scratch_model']['dense_units_scratch']['max'],\n",
        "                step=self.config['hyperparameters']['scratch_model']['dense_units_scratch']['step'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['dense_units_scratch']['default']\n",
        "            )\n",
        "            dropout_rate = hp.Float(\n",
        "                'dropout_rate_scratch',\n",
        "                min_value=self.config['hyperparameters']['scratch_model']['dropout_rate_scratch']['min'],\n",
        "                max_value=self.config['hyperparameters']['scratch_model']['dropout_rate_scratch']['max'],\n",
        "                step=self.config['hyperparameters']['scratch_model']['dropout_rate_scratch']['step'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['dropout_rate_scratch']['default']\n",
        "            )\n",
        "            use_dense_batch_norm = hp.Boolean(\n",
        "                'use_dense_batch_norm_scratch',\n",
        "                default=False\n",
        "            )\n",
        "\n",
        "            # Build dense layers\n",
        "            for i in range(num_dense_layers):\n",
        "                x = Dense(dense_units, activation='relu')(x)\n",
        "                if use_dense_batch_norm:\n",
        "                    x = BatchNormalization()(x)\n",
        "                if dropout_rate > 0.0:\n",
        "                    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "            # Output layer\n",
        "            output = Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "            # Create the model\n",
        "            model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "            # Compile the model with hyperparameter-defined optimizer\n",
        "            learning_rate = hp.Float(\n",
        "                'learning_rate_scratch',\n",
        "                min_value=float(self.config['hyperparameters']['scratch_model']['learning_rate_scratch']['min']),\n",
        "                max_value=float(self.config['hyperparameters']['scratch_model']['learning_rate_scratch']['max']),\n",
        "                sampling='log',\n",
        "                default=float(self.config['hyperparameters']['scratch_model']['learning_rate_scratch']['default'])\n",
        "            )\n",
        "            optimizer_choice = hp.Choice(\n",
        "                'optimizer_scratch',\n",
        "                values=self.config['hyperparameters']['scratch_model']['optimizer_scratch']['choices'],\n",
        "                default=self.config['hyperparameters']['scratch_model']['optimizer_scratch']['default']\n",
        "            )\n",
        "\n",
        "            if optimizer_choice == 'adam':\n",
        "                optimizer = Adam(learning_rate=learning_rate)\n",
        "            elif optimizer_choice == 'sgd':\n",
        "                optimizer = SGD(learning_rate=learning_rate)\n",
        "            else:\n",
        "                optimizer = Adam(learning_rate=learning_rate)  # Default to Adam\n",
        "\n",
        "            metrics = ['accuracy'] + [METRICS[metric] for metric in self.config['model']['additional_metrics']]\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=metrics\n",
        "            )\n",
        "\n",
        "            # Print the model summary\n",
        "            print(\"\\nModel Summary:\")\n",
        "            model.summary()\n",
        "            print(\"\\n\")\n",
        "\n",
        "            return model\n",
        "\n",
        "def configure_logger(log_level=logging.CRITICAL):\n",
        "    \"\"\"Configures the logger with the given log level.\"\"\"\n",
        "    # Clear any existing TensorFlow session\n",
        "    tf.keras.backend.clear_session()\n",
        "    # To ignore warnings\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    logger = logging.getLogger()\n",
        "    if not logger.hasHandlers():\n",
        "        handler = logging.StreamHandler()\n",
        "        handler.setLevel(log_level)\n",
        "        formatter = logging.Formatter('%(levelname)s:%(message)s')\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "        logger.setLevel(log_level)\n",
        "    return logger\n",
        "\n",
        "def setup_random_seed(seed=42):\n",
        "    \"\"\"Configures random seed for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "def print_system_info():\n",
        "    \"\"\"Prints version info of the system and key libraries.\"\"\"\n",
        "    print(f\"\\nPython: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
        "    print(f\"TensorFlow: {tf.__version__}\")\n",
        "    print(f\"Keras: {tf.keras.__version__}\")\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    print(f'GPUs: {gpus if gpus else \"None\"}')\n",
        "    print()\n",
        "\n",
        "def setup_gpu(gpu_config):\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Log the number of GPUs available\n",
        "            logger.debug(f\"SG GPU setup complete. Found {len(gpus)} GPU(s).\")\n",
        "\n",
        "            # Optionally, you can log more details about each GPU\n",
        "            for i, gpu in enumerate(gpus):\n",
        "                logger.debug(f\"GPU {i}: {gpu}\")\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            logger.error(f\"GPU setup failed: {e}\")\n",
        "    else:\n",
        "        logger.warning(\"No GPUs found. The model will run on CPU.\")\n",
        "\n",
        "def load_config(config_path):\n",
        "    with open(config_path, 'r') as file:\n",
        "        return yaml.safe_load(file)\n",
        "\n",
        "def setup_datasets(config):\n",
        "    try:\n",
        "        data_generator = DataGenerator(config)\n",
        "        logger.debug(\"SD DataGenerator initialized successfully.\")\n",
        "        train_dataset, test_dataset, steps_per_epoch, validation_steps = data_generator.get_data_generators()\n",
        "        class_names = data_generator.class_names  # Use class names from data generator\n",
        "        logger.debug(f\"SD Class names: {class_names}\")\n",
        "\n",
        "        return train_dataset, test_dataset, steps_per_epoch, validation_steps, class_names\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Dataset setup failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def get_callbacks(config, train_dataset, test_dataset, validation_steps, for_tuning=False):\n",
        "    \"\"\"\n",
        "    Returns a list of callbacks based on the configuration.\n",
        "\n",
        "    Args:\n",
        "        config (dict): Configuration dictionary.\n",
        "        train_dataset (tf.data.Dataset): Training dataset.\n",
        "        test_dataset (tf.data.Dataset): Testing dataset.\n",
        "        validation_steps (int): Number of validation steps.\n",
        "        for_tuning (bool): Flag indicating if it's for hyperparameter tuning.\n",
        "\n",
        "    Returns:\n",
        "        list: List of Keras callbacks.\n",
        "    \"\"\"\n",
        "    callbacks = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=config['training']['patience'],\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            filepath=config['training']['model_checkpoint_path'],\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Incorporate ReduceLROnPlateau if specified in config\n",
        "    reduce_lr_config = config.get('reduce_lr_on_plateau', None)\n",
        "    if reduce_lr_config:\n",
        "        reduce_lr = ReduceLROnPlateau(\n",
        "            monitor=reduce_lr_config.get('monitor', 'val_loss'),\n",
        "            factor=float(reduce_lr_config.get('factor', 0.5)),\n",
        "            patience=reduce_lr_config.get('patience', 5),\n",
        "            min_lr=float(reduce_lr_config.get('min_lr', 1e-6)),\n",
        "            verbose=reduce_lr_config.get('verbose', 1)\n",
        "        )\n",
        "        callbacks.append(reduce_lr)\n",
        "    \n",
        "    if not for_tuning:\n",
        "        # Include custom callbacks only when not tuning\n",
        "        callbacks.extend([\n",
        "            AccuracyCallback(target_accuracy=config['training']['target_accuracy']),\n",
        "            CustomValidationCallback(test_dataset, validation_steps),\n",
        "            DebugCallback(),\n",
        "            DatasetLogger(train_dataset, test_dataset)\n",
        "        ])\n",
        "    \n",
        "    return callbacks\n",
        "\n",
        "def compute_class_weights_from_counts(class_counts, class_names):\n",
        "    total_samples = sum(class_counts.values())\n",
        "    class_weight_dict = {}\n",
        "    for idx, class_name in enumerate(class_names):\n",
        "        count = class_counts.get(class_name, 0)\n",
        "        if count > 0:\n",
        "            class_weight_dict[idx] = total_samples / (len(class_counts) * count)\n",
        "        else:\n",
        "            class_weight_dict[idx] = 0.0  # Handle classes with zero samples\n",
        "    return class_weight_dict\n",
        "\n",
        "def save_best_hyperparameters(best_hps, filepath='best_hyperparameters.json'):\n",
        "    # Assuming best_hps is an instance of HyperParameters\n",
        "    # Extracting hyperparameters values as a dictionary\n",
        "    hyperparameters_dict = {key: best_hps.get(key) for key in best_hps.values.keys()}\n",
        "    \n",
        "    # Save the hyperparameters to a JSON file\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(hyperparameters_dict, f, indent=4)\n",
        "    \n",
        "    print(f\"\\nSBHP Saved Best Hyperparameters to {filepath}.\")\n",
        "\n",
        "    # Log or print the loaded hyperparameters\n",
        "    print(\"\\nSBHP List of Best hyperparameters:\")\n",
        "    for key, value in hyperparameters_dict.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "def load_best_hyperparameters(filepath='best_hyperparameters.json'):\n",
        "    \"\"\"\n",
        "    Loads the best hyperparameters from a JSON file and logs them.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the JSON file containing the best hyperparameters.\n",
        "\n",
        "    Returns:\n",
        "        HyperParameters: A Keras Tuner HyperParameters object with the loaded values.\n",
        "    \"\"\"\n",
        "    # Load the hyperparameters from the JSON file\n",
        "    with open(filepath, 'r') as f:\n",
        "        hps_dict = json.load(f)\n",
        "    \n",
        "    # Log or print the loaded hyperparameters\n",
        "    print(f\"\\nLoaded hyperparameters from {filepath}:\")\n",
        "    for key, value in hps_dict.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    \n",
        "    # Create a HyperParameters object\n",
        "    hp = HyperParameters()\n",
        "    \n",
        "    # Set the hyperparameters from the loaded JSON\n",
        "    for key, value in hps_dict.items():\n",
        "        hp.Fixed(key, value)\n",
        "    \n",
        "    return hp\n",
        "\n",
        "def convert_pgm_to_png(input_dir, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        # Create corresponding subdirectories in output_dir\n",
        "        rel_path = os.path.relpath(root, input_dir)\n",
        "        output_subdir = os.path.join(output_dir, rel_path)\n",
        "        os.makedirs(output_subdir, exist_ok=True)\n",
        "        for filename in files:\n",
        "            if filename.lower().endswith('.pgm'):\n",
        "                filepath = os.path.join(root, filename)\n",
        "                try:\n",
        "                    with Image.open(filepath) as img:\n",
        "                        img = img.convert('L')  # Convert to RGB, use L for grayscale\n",
        "                        new_filename = os.path.splitext(filename)[0] + '.png'\n",
        "                        output_path = os.path.join(output_subdir, new_filename)\n",
        "                        img.save(output_path, 'PNG')\n",
        "                except Exception as e:\n",
        "                    print(f\"Error converting {filepath}: {e}\")\n",
        "\n",
        "def create_custom_model(config, num_classes, input_shape):\n",
        "\n",
        "    print(f\"\\nCreating custom model.\\n\")\n",
        "    \n",
        "    model_config = config['model']\n",
        "    use_pretrained_weights = model_config.get('use_pretrained_weights', True)\n",
        "    \n",
        "    if use_pretrained_weights:\n",
        "        optimizer_choice = config['hyperparameters']['pretrained_model']['optimizer']['default']\n",
        "        learning_rate = config['hyperparameters']['pretrained_model']['learning_rate']['default']\n",
        "    else:\n",
        "        optimizer_choice = config['hyperparameters']['scratch_model']['optimizer_scratch']['default']\n",
        "        learning_rate = config['hyperparameters']['scratch_model']['learning_rate_scratch']['default']\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Conv2D(128, (3,3), activation='relu')(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "    outputs = Dropout(0.2)(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    \n",
        "    if optimizer_choice == 'adam':\n",
        "        optimizer = Adam(learning_rate=learning_rate)\n",
        "    elif optimizer_choice == 'sgd':\n",
        "        optimizer = SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = Adam(learning_rate=learning_rate)  # Default to Adam\n",
        "        \n",
        "    metrics = ['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']]\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=metrics\n",
        "    )\n",
        "\n",
        "    # Print the model summary\n",
        "    print(\"\\nModel Summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def main(config_path):\n",
        "    \"\"\"\n",
        "    The main function to orchestrate data loading, model building, training, and evaluation.\n",
        "\n",
        "    Args:\n",
        "        config_path (str): Path to the configuration YAML file.\n",
        "    \"\"\"\n",
        "    global logger  # Use the global logger\n",
        "    logger = configure_logger(log_level=logging.ERROR)\n",
        "\n",
        "    setup_random_seed()\n",
        "    # print_system_info()\n",
        "    \n",
        "    # Load the configuration\n",
        "    config = load_config(config_path)\n",
        "    logger.debug(f\"MF Loaded configuration: {config}\")\n",
        "    performance_tuning = config.get('tuning', {}).get('perform_tuning', True)\n",
        "    logger.debug(f\"MF Perform tuning: {performance_tuning}\")\n",
        "    override = config.get('training', {}).get('override', False)\n",
        "\n",
        "    # Set up GPU if available\n",
        "    setup_gpu(config.get('gpu', {}))\n",
        "    logger.debug(\"MF Completed GPU setup.\")\n",
        "\n",
        "    # Load environment variables\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        # drive.mount('/content/drive')\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('COLAB_DATASET_PATH')\n",
        "        logger.debug(\"MF Running in Colab environment\")\n",
        "    except ImportError:\n",
        "        load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "        DATASET_PATH = os.getenv('DATASET_PATH', default='/default/dataset/path')\n",
        "        logger.debug(\"MF Running in local environment\")\n",
        "\n",
        "    # Update the dataset paths based on whether the data is pre-split or not\n",
        "    pre_split = config['data'].get('pre_split', True)\n",
        "    if pre_split:\n",
        "        # For pre-split data\n",
        "        train_path = os.path.join(DATASET_PATH, config['data']['train_dir'])\n",
        "        test_path = os.path.join(DATASET_PATH, config['data']['test_dir'])\n",
        "        config['data']['train_path'] = train_path\n",
        "        config['data']['test_path'] = test_path\n",
        "        logger.debug(f\"MF Train path: {train_path}\")\n",
        "        logger.debug(f\"MF Test path: {test_path}\")\n",
        "    else:\n",
        "        # For single directory data\n",
        "        dataset_path = os.path.join(DATASET_PATH, config['data']['dataset_dir'])\n",
        "        config['data']['dataset_path'] = dataset_path\n",
        "        logger.debug(f\"MF Dataset path: {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        # Initialize DataGenerator\n",
        "        data_generator = DataGenerator(config)\n",
        "        logger.debug(\"DataGenerator initialized successfully.\")\n",
        "        train_dataset = data_generator.train_dataset\n",
        "        val_dataset = data_generator.val_dataset\n",
        "        test_dataset = data_generator.test_dataset\n",
        "        class_names = data_generator.class_names\n",
        "        logger.debug(f\"MF Class names: {class_names}\")\n",
        "        steps_per_epoch = data_generator.steps_per_epoch\n",
        "        validation_steps = data_generator.validation_steps\n",
        "\n",
        "        num_classes = len(class_names)\n",
        "\n",
        "        # Compute class weights using training data\n",
        "        logger.debug(\"MF Counting Classes and Computing Weights\")\n",
        "        class_counts = data_generator.class_counts\n",
        "        logger.debug(f\"MF Class counts: {class_counts}\")\n",
        "        class_weight_dict = compute_class_weights_from_counts(class_counts, class_names)\n",
        "        logger.debug(f\"MF Computed class weights: {class_weight_dict}\")\n",
        "\n",
        "        # Retrieve training parameters\n",
        "        initial_epochs = config['training'].get('initial_epochs', 10)\n",
        "        fine_tune_epochs = config['training'].get('fine_tune_epochs', 0)\n",
        "        total_epochs = initial_epochs + fine_tune_epochs\n",
        "\n",
        "        # Check if using pre-trained weights\n",
        "        use_pretrained_weights = config['model'].get('use_pretrained_weights', True)\n",
        "\n",
        "        if performance_tuning:\n",
        "            # Instantiate the hypermodel\n",
        "            hypermodel = MyHyperModel(config, num_classes)\n",
        "            logger.debug(\"MF HyperModel instantiated successfully.\")\n",
        "\n",
        "            # Get callbacks for tuning (only deepcopyable ones)\n",
        "            tuning_callbacks = get_callbacks(config, train_dataset, val_dataset, validation_steps, for_tuning=True)\n",
        "\n",
        "            # Set up the tuner\n",
        "            tuner = RandomSearch(\n",
        "                hypermodel,\n",
        "                objective='val_accuracy',\n",
        "                max_trials=config['tuning']['max_trials'],\n",
        "                executions_per_trial=config['tuning']['executions_per_trial'],\n",
        "                directory='hyperparameter_tuning',\n",
        "                project_name='keras_tuner_project'\n",
        "            )\n",
        "            logger.debug(\"Keras Tuner initialized successfully.\")\n",
        "\n",
        "            # Run the hyperparameter search\n",
        "            tuner.search(\n",
        "                train_dataset,\n",
        "                epochs=initial_epochs,\n",
        "                validation_data=val_dataset,\n",
        "                callbacks=tuning_callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                steps_per_epoch=steps_per_epoch\n",
        "            )\n",
        "\n",
        "            # Get the optimal hyperparameters\n",
        "            best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "            # print(\"Available hyperparameters:\", best_hps.values)\n",
        "            \n",
        "            save_best_hyperparameters(best_hps)\n",
        "            logger.debug(\"MF Best hyperparameters saved to file.\")\n",
        "\n",
        "            # Build the best model\n",
        "            model = tuner.hypermodel.build(best_hps)\n",
        "            logger.debug(\"MF Best model built with optimal hyperparameters.\")\n",
        "            \n",
        "            # # Print the best model summary\n",
        "            # print(\"\\nMF Best Model Summary:\")\n",
        "            # model.summary()\n",
        "            # print(\"\\n\")\n",
        "            \n",
        "        else:\n",
        "            # Check if best_hyperparameters.json exists\n",
        "            if os.path.exists('best_hyperparameters.json'):\n",
        "                best_hps = load_best_hyperparameters()\n",
        "                logger.debug(\"MF Loaded best hyperparameters from file.\")\n",
        "                filepath = 'best_hyperparameters.json'\n",
        "\n",
        "                # Instantiate the hypermodel with best hyperparameters\n",
        "                hypermodel = MyHyperModel(config, num_classes, best_hyperparameters=best_hps)\n",
        "                model = hypermodel.build(hp=None)  # hp is None, so it uses best_hyperparameters\n",
        "                print(f\"MF Model built with loaded hyperparameters: {best_hps}\")\n",
        "            else:\n",
        "                if override:\n",
        "                    input_shape = tuple(config['data']['input_shape'])\n",
        "                    model = create_custom_model(config, num_classes, input_shape)\n",
        "                else:\n",
        "                    logger.debug(f\"MF Best hyperparameters file not found. Building model with default hyperparameters from config.\")\n",
        "                    best_hps = None # No best hyperparameters available and uses default/fixed hyperparameters\n",
        "                    hypermodel = MyHyperModel(config, num_classes, best_hyperparameters=best_hps)\n",
        "                    model = hypermodel.build(hp=None) # hp is None, so it uses default hyperparameters\n",
        "\n",
        "        # Get all callbacks including custom ones for final training\n",
        "        final_callbacks = get_callbacks(config, train_dataset, val_dataset, validation_steps, for_tuning=False)\n",
        "\n",
        "        print(f\"\\nMF Starting Initial Training and Evaluation.\\n\")\n",
        "        # **Initial Training Phase**\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            epochs=initial_epochs,\n",
        "            validation_data=val_dataset,\n",
        "            callbacks=final_callbacks,\n",
        "            class_weight=class_weight_dict,\n",
        "            steps_per_epoch=steps_per_epoch,\n",
        "            # verbose=3\n",
        "        )\n",
        "\n",
        "        print(f\"\\nMF Initial training completed.\")\n",
        "\n",
        "        # **Fine-Tuning Phase**\n",
        "        if use_pretrained_weights and fine_tune_epochs > 0:\n",
        "            logger.debug(\"MFFT Starting fine-tuning phase.\")\n",
        "\n",
        "            # Unfreeze the base model\n",
        "            # Ensure that the base model was named 'base_model' in MyHyperModel\n",
        "            try:\n",
        "                base_model = model.get_layer('base_model')\n",
        "            except ValueError:\n",
        "                # If not found, iterate through layers to find the base model\n",
        "                base_model = None\n",
        "                for layer in model.layers:\n",
        "                    if isinstance(layer, Model):\n",
        "                        base_model = layer\n",
        "                        break\n",
        "                if base_model is None:\n",
        "                    raise ValueError(\"MFFT Base model layer not found for fine-tuning.\")\n",
        "\n",
        "            base_model.trainable = True\n",
        "\n",
        "            # Recompile the model with a lower learning rate\n",
        "            if use_pretrained_weights:\n",
        "                if performance_tuning:\n",
        "                    optimizer_choice = best_hps.get('optimizer', 'adam')\n",
        "                    fine_tune_learning_rate = float(config['model'].get('fine_tune_learning_rate', 1e-5))\n",
        "                else:\n",
        "                    optimizer_choice = config['hyperparameters']['pretrained_model']['optimizer']['default']\n",
        "                    fine_tune_learning_rate = float(config['model'].get('fine_tune_learning_rate', 1e-5))\n",
        "            else:\n",
        "                if performance_tuning:\n",
        "                    optimizer_choice = best_hps.get('optimizer', 'adam')\n",
        "                    fine_tune_learning_rate = float(config['model'].get('fine_tune_learning_rate_scratch', 1e-5))\n",
        "                else:\n",
        "                    optimizer_choice = config['hyperparameters']['scratch_model']['optimizer_scratch']['default']\n",
        "                    fine_tune_learning_rate = float(config['model'].get('fine_tune_learning_rate_scratch', 1e-5))\n",
        "\n",
        "            if optimizer_choice == 'adam':\n",
        "                optimizer = Adam(learning_rate=fine_tune_learning_rate)\n",
        "            elif optimizer_choice == 'sgd':\n",
        "                optimizer = SGD(learning_rate=fine_tune_learning_rate)\n",
        "            else:\n",
        "                optimizer = Adam(learning_rate=fine_tune_learning_rate)  # Default to Adam\n",
        "\n",
        "            # Recompile the model\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'] + [METRICS[metric] for metric in config['model']['additional_metrics']]\n",
        "            )\n",
        "\n",
        "            logger.debug(\"MFFT Model recompiled for fine-tuning.\")\n",
        "\n",
        "            # Continue training\n",
        "            history_fine = model.fit(\n",
        "                train_dataset,\n",
        "                epochs=total_epochs,\n",
        "                initial_epoch=initial_epochs,\n",
        "                validation_data=val_dataset,\n",
        "                callbacks=final_callbacks,\n",
        "                class_weight=class_weight_dict,\n",
        "                steps_per_epoch=steps_per_epoch\n",
        "            )\n",
        "\n",
        "            logger.debug(\"MFFT Fine-tuning completed.\")\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(f\"\\nMF Final evaluation:\")\n",
        "        final_evaluation = model.evaluate(test_dataset)\n",
        "        print(f\"\\nMF Final evaluation metrics: {final_evaluation}\")\n",
        "        # Build the list of metric names\n",
        "        metric_names = ['loss', 'accuracy']\n",
        "        additional_metric_names = [METRICS[metric].name for metric in config['model']['additional_metrics']]\n",
        "        metric_names.extend(additional_metric_names)\n",
        "\n",
        "        # Print the metrics\n",
        "        print(\"MF Final evaluation metrics:\")\n",
        "        for name, value in zip(metric_names, final_evaluation):\n",
        "            print(f\"  {name}: {value}\")\n",
        "\n",
        "        # Save the trained model\n",
        "        model.save('final_model.keras')\n",
        "        print(f\"\\nMF Model saved successfully\\n\")\n",
        "\n",
        "        return history, model, class_names, config['data']['target_size'], config['data']['preprocessing_function'], config['model']['name']\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"MF An error occurred: {e}\")\n",
        "        logger.debug(traceback.format_exc())\n",
        "\n",
        "        return None, None, None, None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model, class_names, target_size, preprocessing_function, ptm_name = main('config.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "* This codebase implements a Convolutional Neural Network (CNN) using transfer learning with the ResNet50V2 ptm_name for image classification. It uses a dataset of architectural elements (e.g., altars, domes, columns) split into training and test sets. The model is compiled with Adam optimizer and categorical crossentropy loss. Data augmentation is applied to increase the diversity of training samples. The training process includes callbacks for early stopping, learning rate scheduling, and custom validation. The model's performance is evaluated and visualized over 20 epochs.\n",
        "\n",
        "1. `DataGenerator` class:\n",
        "  - Optimizable parameters:\n",
        "    - None directly, but it relies on the ImageDataGenerator settings.\n",
        "\n",
        "2. `CustomAugmentationLayer` class:\n",
        "   - Optimizable parameters:\n",
        "    - Contrast range (lower, upper)\n",
        "    - Saturation range (lower, upper)\n",
        "   - Role in optimization:\n",
        "    - Adjusting these can increase data diversity and potentially reduce overfitting.\n",
        "\n",
        "3. `create_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Number and size of dense layers\n",
        "    - Dropout rate (currently 0.5)\n",
        "    - L2 regularization strength\n",
        "   - Role in optimization:\n",
        "    - Adjusting model complexity can help balance underfitting and overfitting.\n",
        "\n",
        "4. `compile_model` function:\n",
        "   - Optimizable parameters:\n",
        "    - Initial learning rate (currently 0.001)\n",
        "    - Learning rate decay steps and rate\n",
        "    - Choice of optimizer (currently Adam)\n",
        "   - Role in optimization:\n",
        "    - Proper learning rate and optimizer settings are crucial for efficient training and convergence.\n",
        "\n",
        "5. `AccuracyCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - Target accuracy threshold\n",
        "   - Role in optimization:\n",
        "    - Adjusting this can prevent premature stopping or unnecessarily long training.\n",
        "\n",
        "6. `count_samples` function:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "7. `DebugCallback` class:\n",
        "   - Optimizable parameters:\n",
        "    - None, but frequency of debug prints could be adjusted.\n",
        "\n",
        "8. `create_data_generators` function:\n",
        "   - Optimizable parameters:\n",
        "    - Batch size (currently 32)\n",
        "    - Data augmentation parameters (rotation_range, width_shift_range, etc.)\n",
        "   - Role in optimization:\n",
        "    - Proper batch size and augmentation can improve training stability and reduce overfitting.\n",
        "\n",
        "9. `DatasetLogger` class:\n",
        "   - Optimizable parameters:\n",
        "    - None\n",
        "\n",
        "10. `CustomValidationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "      - None, but custom metrics could be added.\n",
        "\n",
        "11. `visualize_augmentation` function:\n",
        "    - Optimizable parameters:\n",
        "      - None\n",
        "\n",
        "12. `AugmentationCallback` class:\n",
        "    - Optimizable parameters:\n",
        "     - Frequency of augmentation visualization\n",
        "    - Role in optimization:\n",
        "      - Adjusting this helps in monitoring augmentation effects without slowing training.\n",
        "\n",
        "13. Main execution block:\n",
        "    - Optimizable parameters:\n",
        "      - Number of epochs (currently 20)\n",
        "      - Early stopping patience (currently 10)\n",
        "      - Class weight calculation method\n",
        "    - Role in optimization:\n",
        "      - These parameters affect overall training duration and handling of class imbalance.\n",
        "\n",
        "Additional global optimizations:\n",
        "1. Learning rate scheduling: Implement more sophisticated schedules like cyclic learning rates or warm restarts.\n",
        "2. Model ptm_name: Experiment with different base models (e.g., EfficientNet, VGG) or custom ptm_names.\n",
        "3. Regularization: Add or adjust L2 regularization, increase dropout rates, or implement other techniques like label smoothing.\n",
        "4. Data preprocessing: Normalize input data, apply additional augmentation techniques like mixup or cutout.\n",
        "5. Ensemble methods: Train multiple models and combine their predictions for improved reliability.\n",
        "6. Cross-validation: Implement k-fold cross-validation for more robust performance estimation.\n",
        "7. Gradient clipping: Add gradient clipping to prevent exploding gradients and stabilize training.\n",
        "8. Batch normalization: Add batch normalization layers for improved training stability and potentially faster convergence.\n",
        "9. Learning rate finder: Implement a learning rate finder to determine optimal initial learning rates.\n",
        "10. Progressive resizing: Start training with smaller image sizes and gradually increase, potentially speeding up early training stages.\n",
        "\n",
        "By carefully tuning these parameters and implementing these techniques, you can work towards optimizing the model's performance, improving its reliability, and finding the right balance between underfitting and overfitting for your specific architectural element classification task.\n",
        "\n",
        "**Why It Is Important:**\n",
        "* Using a CNN with transfer learning is important because it leverages pre-trained weights on a large dataset (ImageNet), allowing the model to learn high-level features more quickly and effectively, especially when dealing with a relatively small dataset. This approach often leads to better performance and faster convergence compared to training a model from scratch, making it particularly useful for specialized image classification tasks like identifying architectural elements.\n",
        "\n",
        "**Observations:**\n",
        "* The model quickly achieves high accuracy (>80%) within the first few epochs.\n",
        "* Validation accuracy peaks around epoch 6 at about 86% and then fluctuates.\n",
        "* There's some overfitting, as training accuracy consistently exceeds validation accuracy.\n",
        "* The learning rate decreases steadily over the epochs, as designed by the exponential decay schedule.\n",
        "* Both training and validation loss decrease rapidly initially and then plateau.\n",
        "* The model's performance seems to stabilize after about 10-15 epochs.\n",
        "\n",
        "**Conclusions:**\n",
        "* The transfer learning approach is effective, allowing the model to achieve good performance quickly.\n",
        "* The model reaches a reasonable accuracy (~83% on the validation set) for classifying architectural elements.\n",
        "* There's room for improvement, as the model shows signs of overfitting and doesn't consistently improve after the initial rapid progress.\n",
        "* The current learning rate schedule and data augmentation help, but may not be optimal for this specific task.\n",
        "\n",
        "**Recommendations:**\n",
        "* Experiment with stronger regularization techniques (e.g., increased dropout, L2 regularization) to combat overfitting.\n",
        "* Try different learning rate schedules, such as cyclical learning rates or a more aggressive decay, to potentially improve convergence.\n",
        "* Increase data augmentation to provide more diverse training samples and potentially reduce overfitting.\n",
        "* Consider implementing techniques like gradient clipping to stabilize training.\n",
        "* Explore ensemble methods or cross-validation to improve overall performance and reliability.\n",
        "* Analyze misclassifications to identify patterns and potentially refine the model ptm_name or data preprocessing steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.15. <a id='toc1_4_1_15_'></a>[**Visualize Training And Validation Accuracy**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5dzjyc3XxTL"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def training_subplot(hist, metric: str, plotnum: int, lim=None, test_val=None):\n",
        "    sp = plt.subplot(3, 2, plotnum)\n",
        "    metric_nm = metric.replace('_', ' ').capitalize()\n",
        "    plt.plot(hist.history[metric], label='Training')\n",
        "    plt.plot(hist.history['val_' + metric], label='Validation')\n",
        "\n",
        "    if test_val is not None:\n",
        "        test_lbl = 'Test ' + (f'({test_val:.1%})' if lim == 1 else f'({test_val:.2f})')\n",
        "        plt.axhline(y=test_val, label=test_lbl, color='green', linestyle='-')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(metric_nm)\n",
        "    plt.ylim(0, lim)\n",
        "    plt.xlim(0, len(hist.history['loss']))\n",
        "    \n",
        "    if lim == 1:\n",
        "        sp.yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1.0))\n",
        "\n",
        "    plt.legend()\n",
        "    plt.grid(visible=True, which='both', axis='both', linestyle='--', linewidth=0.5, color='grey')\n",
        "    plt.title(metric_nm)\n",
        "\n",
        "def plot_training_history(hist, ptm_name, title='Training History'):\n",
        "    # Assuming that you might have test values (quality scores) elsewhere in your project\n",
        "    # We'll set `None` for now. If you have test values, replace `None` with those values.\n",
        "    quality = {\n",
        "        'loss': None,         # Replace with actual test loss if available\n",
        "        'accuracy': None,     # Replace with actual test accuracy if available\n",
        "        'precision': None,    # Replace with actual test precision if available\n",
        "        'recall': None,       # Replace with actual test recall if available\n",
        "        'f1_score': None      # Replace with actual test F1 score if available\n",
        "    }\n",
        "\n",
        "    # Create the main figure and subplots\n",
        "    plt.figure(figsize=(8, 12))\n",
        "    plt.suptitle(f'{title} using {ptm_name} with Best Performing Hyperparameters', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot loss\n",
        "    training_subplot(hist, 'loss', 1, test_val=quality['loss'])\n",
        "\n",
        "    # Plot accuracy\n",
        "    training_subplot(hist, 'accuracy', 2, lim=1, test_val=quality['accuracy'])\n",
        "\n",
        "    # Plot precision (check if precision exists in history)\n",
        "    if 'precision' in hist.history:\n",
        "        training_subplot(hist, 'precision', 3, lim=1, test_val=quality['precision'])\n",
        "\n",
        "    # Plot recall (check if recall exists in history)\n",
        "    if 'recall' in hist.history:\n",
        "        training_subplot(hist, 'recall', 4, lim=1, test_val=quality['recall'])\n",
        "\n",
        "    # Plot F1 score (check if F1 score exists in history)\n",
        "    if 'f1_score' in hist.history:\n",
        "        training_subplot(hist, 'f1_score', 5, lim=1, test_val=quality['f1_score'])\n",
        "\n",
        "    # Adjust layout to prevent overlapping\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example call after training:\n",
        "plot_training_history(history, 'ResNet50V2')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- This code defines a function `plot_training_history` that visualizes the training and validation accuracy and loss over epochs. It then applies this function to the training history of the augmented model.\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- Visualizing the training and validation metrics helps us understand how the model is learning over time. It allows us to identify potential issues such as overfitting (when training accuracy continues to improve but validation accuracy plateaus or decreases) or underfitting (when both training and validation accuracy are low and not improving). This information is crucial for deciding whether to adjust the model ptm_name, change hyperparameters, or modify the training process.\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.16. <a id='toc1_4_1_16_'></a>[**Plot Hyperparameter Search Results**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import logging\n",
        "\n",
        "# Set logging level to INFO\n",
        "logger.basicConfig(level=logger.CRITICAL)\n",
        "\n",
        "# Define the directory containing the trial folders\n",
        "base_dir = \"hyperparameter_tuning/keras_tuner_project\"\n",
        "\n",
        "# Metrics to extract and plot\n",
        "metrics_to_plot = [\n",
        "    \"accuracy\", \"loss\", \"auc\", \"precision\", \"recall\", \"f1_score\",\n",
        "    \"val_accuracy\", \"val_loss\", \"val_auc\", \"val_precision\", \"val_recall\", \"val_f1_score\"\n",
        "]\n",
        "\n",
        "# Initialize storage for metrics\n",
        "all_metrics = {metric: [] for metric in metrics_to_plot}\n",
        "trial_ids = []\n",
        "scores = []\n",
        "\n",
        "# Iterate through each trial folder and extract metrics\n",
        "for trial_folder in sorted(os.listdir(base_dir)):\n",
        "    trial_path = os.path.join(base_dir, trial_folder)\n",
        "    trial_json_path = os.path.join(trial_path, \"trial.json\")\n",
        "\n",
        "    if os.path.isdir(trial_path) and os.path.exists(trial_json_path):\n",
        "        with open(trial_json_path, \"r\") as f:\n",
        "            trial_data = json.load(f)\n",
        "            trial_id = trial_data.get(\"trial_id\")\n",
        "            metrics = trial_data.get(\"metrics\", {}).get(\"metrics\", {})\n",
        "\n",
        "            # Skip trials with missing score or metrics\n",
        "            if not trial_data.get(\"score\") or not metrics:\n",
        "                continue\n",
        "\n",
        "            trial_ids.append(trial_id)\n",
        "            scores.append(trial_data.get(\"score\", 0))\n",
        "\n",
        "            # Extract the last recorded value for each metric\n",
        "            for metric in metrics_to_plot:\n",
        "                if metric in metrics:\n",
        "                    observations = metrics[metric][\"observations\"]\n",
        "                    if observations:\n",
        "                        value = observations[-1][\"value\"][0]  # Get the last value\n",
        "                        all_metrics[metric].append(value if value is not None else 0)\n",
        "                    else:\n",
        "                        all_metrics[metric].append(0)\n",
        "                else:\n",
        "                    all_metrics[metric].append(0)\n",
        "\n",
        "# Plot separate subplots for each metric\n",
        "fig, axs = plt.subplots(len(metrics_to_plot), 1, figsize=(15, 5 * len(metrics_to_plot)))\n",
        "\n",
        "# Use a color palette from seaborn\n",
        "colors = sns.color_palette(\"husl\", len(trial_ids))\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    bars = axs[idx].bar(trial_ids, all_metrics[metric], color=colors)\n",
        "    axs[idx].set_title(f'{metric} across Trials')\n",
        "    axs[idx].set_xlabel('Trial ID')\n",
        "    axs[idx].set_ylabel(metric)\n",
        "    axs[idx].grid(True)\n",
        "\n",
        "    # Add labels to each bar\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        axs[idx].text(bar.get_x() + bar.get_width() / 2, yval + 0.02, f'{yval:.3f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot heatmap of metrics\n",
        "metrics_array = np.array([all_metrics[metric] for metric in metrics_to_plot]).T\n",
        "plt.figure(figsize=(15, 10))\n",
        "sns.heatmap(metrics_array, annot=True, cmap='viridis', xticklabels=metrics_to_plot, yticklabels=trial_ids)\n",
        "plt.title(\"Heatmap of Metrics across Trials\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Trial ID\")\n",
        "plt.show()\n",
        "\n",
        "# Scatter plots for hyperparameters vs. score (example)\n",
        "# Assuming hyperparameters like learning_rate, dropout_rate are available in trial data\n",
        "hyperparameters_to_plot = [\"learning_rate\", \"dropout_rate\"]\n",
        "hyperparameters = {param: [] for param in hyperparameters_to_plot}\n",
        "\n",
        "# Extract hyperparameters for scatter plot\n",
        "for trial_folder in sorted(os.listdir(base_dir)):\n",
        "    trial_path = os.path.join(base_dir, trial_folder)\n",
        "    trial_json_path = os.path.join(trial_path, \"trial.json\")\n",
        "\n",
        "    if os.path.isdir(trial_path) and os.path.exists(trial_json_path):\n",
        "        with open(trial_json_path, \"r\") as f:\n",
        "            trial_data = json.load(f)\n",
        "            hyper_params = trial_data.get(\"hyperparameters\", {}).get(\"values\", {})\n",
        "\n",
        "            for param in hyperparameters_to_plot:\n",
        "                if param in hyper_params:\n",
        "                    hyperparameters[param].append(hyper_params[param])\n",
        "                else:\n",
        "                    hyperparameters[param].append(None)\n",
        "\n",
        "# Filter out trials with missing hyperparameters\n",
        "valid_indices = [i for i, score in enumerate(scores) if all(hyperparameters[param][i] is not None for param in hyperparameters_to_plot)]\n",
        "\n",
        "filtered_scores = [scores[i] for i in valid_indices]\n",
        "filtered_hyperparameters = {param: [hyperparameters[param][i] for i in valid_indices] for param in hyperparameters_to_plot}\n",
        "\n",
        "# Plot scatter plots of hyperparameters vs. score\n",
        "for param in hyperparameters_to_plot:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(filtered_hyperparameters[param], filtered_scores, color='darkred')\n",
        "    plt.xlabel(param)\n",
        "    plt.ylabel('Score')\n",
        "    plt.title(f'{param} vs. Score')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot score values across trials with filled area below the line\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(trial_ids, scores, color='blue', marker='o', linestyle='-', linewidth=2, label='Score')\n",
        "plt.fill_between(trial_ids, scores, color='blue', alpha=0.2)\n",
        "for i, score in enumerate(scores):\n",
        "    plt.text(i, score + 0.02, f'{score:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
        "plt.xlabel('Trial ID')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Score Values across Trials')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.1.17. <a id='toc1_4_1_17_'></a>[**Test Trained Model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWZIK3E1XxTL"
      },
      "outputs": [],
      "source": [
        "from keras.applications.efficientnet import preprocess_input as efficientnet_preprocess\n",
        "from keras.applications.inception_v3 import preprocess_input as inception_preprocess\n",
        "from keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n",
        "from keras.applications.resnet_v2 import preprocess_input as resnet_preprocess\n",
        "from keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "\n",
        "# Create a dictionary mapping string names to preprocessing functions\n",
        "preprocessing_functions = {\n",
        "    'efficientnet_preprocess': efficientnet_preprocess,\n",
        "    'inception_preprocess': inception_preprocess,\n",
        "    'mobilenet_preprocess': mobilenet_preprocess,\n",
        "    'resnet_preprocess': resnet_preprocess,\n",
        "    'vgg_preprocess': vgg_preprocess\n",
        "}\n",
        "\n",
        "def predict_image_class(model, img_path, class_names, target_size, preprocessing_function_name):\n",
        "    target_size = target_size  # Get the target size for the chosen architecture\n",
        "    # Retrieve the actual preprocessing function from the dictionary\n",
        "    preprocessing_function = preprocessing_functions.get(preprocessing_function_name)\n",
        "    \n",
        "    if preprocessing_function is None:\n",
        "        raise ValueError(f\"Unknown preprocessing function: {preprocessing_function_name}\")\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array = preprocessing_function(img_array)\n",
        "\n",
        "    # Make prediction\n",
        "    predictions = model.predict(img_array)\n",
        "    predicted_class_index = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_index]\n",
        "    confidence = predictions[0][predicted_class_index]\n",
        "\n",
        "    return predicted_class, confidence\n",
        "\n",
        "def visualize_prediction(img_path, target_size, predicted_class, confidence):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2f}\")\n",
        "    plt.show()\n",
        "\n",
        "# Mount Google Drive if using Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DATASET_PATH = '/content/drive/MyDrive/Colab Notebooks/datasets/1703138137_dataset/part_1/dataset_hist_structures'\n",
        "except ImportError:\n",
        "    load_dotenv(verbose=True, dotenv_path='.env', override=True)\n",
        "    DATASET_PATH = os.getenv('DATASET_PATH')\n",
        "\n",
        "print()\n",
        "print(\"\\nTesting model on new images:\")\n",
        "print()\n",
        "\n",
        "# Get the list of all files in the model_test_images directory\n",
        "test_image_path = f'{DATASET_PATH}/model_test_images'\n",
        "image_files = os.listdir(f'{test_image_path}/')\n",
        "\n",
        "# Optionally, filter the list to include only image files\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n",
        "image_files = [file for file in image_files if file.lower().endswith(image_extensions)]\n",
        "\n",
        "# Print the list of image files\n",
        "print(image_files)\n",
        "\n",
        "print(f'The preprocessing function is: {preprocessing_function}')\n",
        "print(f'The compiled model is: {compiled_model}')\n",
        "print(f'The class names are: {class_names}')\n",
        "\n",
        "# for img_path in test_image_paths:\n",
        "for image_file in image_files:\n",
        "    img_path = f'{test_image_path}/{image_file}'\n",
        "    predicted_class, confidence = predict_image_class(compiled_model, img_path, class_names, target_size, preprocessing_function)\n",
        "    print()\n",
        "    print(f\"Image: {img_path}\")\n",
        "    print(f\"Predicted class: {predicted_class}\")\n",
        "    print(f\"Confidence: {confidence:.2f}\")\n",
        "    print()\n",
        "\n",
        "    # Visualize prediction\n",
        "    visualize_prediction(img_path, target_size, predicted_class, confidence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.2. <a id='toc1_4_2_'></a>[**Part 2 - Data Science**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.2.1. <a id='toc1_4_2_1_'></a>[**Data Analysis**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-haQ_GrgXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "DATASET_PATH = '/Users/toddwalters/Development/data/1703138137_dataset/part_2'\n",
        "\n",
        "# Load the datasets\n",
        "user_df = pd.read_csv(f'{DATASET_PATH}/user.csv')\n",
        "tourism_df = pd.read_excel(f'{DATASET_PATH}/tourism_with_id.xlsx')\n",
        "ratings_df = pd.read_csv(f'{DATASET_PATH}/tourism_rating.csv')\n",
        "\n",
        "# Task 1: Preliminary inspections\n",
        "print(\"1. Preliminary Inspections\")\n",
        "\n",
        "## Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(\"User data:\\n\", user_df.isnull().sum())\n",
        "print(\"\\nTourism data:\\n\", tourism_df.isnull().sum())\n",
        "print(\"\\nRatings data:\\n\", ratings_df.isnull().sum())\n",
        "\n",
        "## Check for duplicates\n",
        "print(\"\\nDuplicates:\")\n",
        "print(\"User data:\", user_df.duplicated().sum())\n",
        "print(\"Tourism data:\", tourism_df.duplicated().sum())\n",
        "print(\"Ratings data:\", ratings_df.duplicated().sum())\n",
        "\n",
        "# Task 2: Explore user group providing tourism ratings\n",
        "print(\"\\n2. User Group Analysis\")\n",
        "\n",
        "## Analyze age distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(user_df['Age'], bins=20, kde=True)\n",
        "plt.title('Age Distribution of Users')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Identify places where most users are coming from\n",
        "top_locations = user_df['Location'].value_counts().head(10)\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_locations.plot(kind='bar')\n",
        "plt.title('Top 10 User Locations')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 3: Explore locations and categories of tourist spots\n",
        "print(\"\\n3. Tourist Spot Analysis\")\n",
        "\n",
        "## Different categories of tourist spots\n",
        "print(\"Categories of Tourist Spots:\")\n",
        "print(tourism_df['Category'].value_counts())\n",
        "\n",
        "## Analyze tourism types by location\n",
        "location_category = tourism_df.groupby('City')['Category'].value_counts().unstack()\n",
        "location_category_norm = location_category.div(location_category.sum(axis=1), axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "location_category_norm.plot(kind='bar', stacked=True)\n",
        "plt.title('Tourism Categories by City')\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('Proportion of Categories')\n",
        "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## Identify best city for nature enthusiasts\n",
        "nature_spots = tourism_df[tourism_df['Category'] == 'Nature']\n",
        "nature_by_city = nature_spots.groupby('City').size().sort_values(ascending=False)\n",
        "print(\"Best Cities for Nature Enthusiasts:\")\n",
        "print(nature_by_city)\n",
        "\n",
        "# Task 4: Analyze combined data with places and user ratings\n",
        "print(\"\\n4. Combined Data Analysis\")\n",
        "\n",
        "## Merge tourism and ratings data\n",
        "combined_data = pd.merge(tourism_df, ratings_df, on='Place_Id')\n",
        "\n",
        "## Find spots most loved by tourists\n",
        "top_spots = combined_data.groupby('Place_Name')['Place_Ratings'].mean().sort_values(ascending=False).head(10)\n",
        "print(\"Top 10 Most Loved Tourist Spots:\")\n",
        "print(top_spots)\n",
        "\n",
        "## Find city with most loved tourist spots\n",
        "city_ratings = combined_data.groupby('City')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nCities with Most Loved Tourist Spots:\")\n",
        "print(city_ratings)\n",
        "\n",
        "## Analyze which categories users like most\n",
        "category_ratings = combined_data.groupby('Category')['Place_Ratings'].mean().sort_values(ascending=False)\n",
        "print(\"\\nMost Liked Categories:\")\n",
        "print(category_ratings)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Task 5: Build a recommender model\n",
        "print(\"\\n5. Recommender Model\")\n",
        "\n",
        "## Create a pivot table for user-item ratings\n",
        "user_item_matrix = combined_data.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "## Fill NaN values with 0\n",
        "user_item_matrix = user_item_matrix.fillna(0)\n",
        "\n",
        "## Calculate cosine similarity between items\n",
        "item_similarity = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "## Create a DataFrame from the item similarity matrix\n",
        "item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix.columns, columns=user_item_matrix.columns)\n",
        "\n",
        "def get_similar_places(place_name, user_rating):\n",
        "    similar_places = item_similarity_df[place_name] * (user_rating - 2.5)\n",
        "    similar_places = similar_places.sort_values(ascending=False)\n",
        "    return similar_places\n",
        "\n",
        "## Example recommendation\n",
        "place_name = \"Monumen Nasional\"  # You can change this to any place name\n",
        "user_rating = 5  # You can change this to any rating\n",
        "\n",
        "print(\"Recommendations based on\", place_name, \"with a rating of\", user_rating)\n",
        "print(get_similar_places(place_name, user_rating).head(5))\n",
        "\n",
        "# Additional insights\n",
        "print(\"\\nAdditional Insights:\")\n",
        "\n",
        "## User activity analysis\n",
        "user_activity = ratings_df.groupby('User_Id')['Place_Ratings'].count().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 Most Active Users:\")\n",
        "print(user_activity.head(10))\n",
        "\n",
        "## Rating distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(ratings_df['Place_Ratings'], bins=5, kde=True)\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## Correlation between place features and ratings\n",
        "if 'Time_Minutes' in tourism_df.columns and 'Price' in tourism_df.columns:\n",
        "    feature_ratings = pd.merge(tourism_df[['Place_Id', 'Time_Minutes', 'Price']],\n",
        "                               ratings_df[['Place_Id', 'Place_Ratings']],\n",
        "                               on='Place_Id')\n",
        "    correlation_matrix = feature_ratings[['Time_Minutes', 'Price', 'Place_Ratings']].corr()\n",
        "    print(\"\\nCorrelation between features and ratings:\")\n",
        "    print(correlation_matrix['Place_Ratings'])\n",
        "\n",
        "print(\"\\nAnalysis Complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.2.2. <a id='toc1_4_2_2_'></a>[**Data Analysis v2**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJ1lXnjXxTP"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "# Tourism Data Analysis and Recommendation Engine Project\n",
        "\n",
        "## Project Context\n",
        "This project aims to analyze tourism data in Indonesia's five largest cities, comprehend and predict the demand for various tourist attractions, and build a recommendation system based on user ratings.\n",
        "\n",
        "## Project Objectives\n",
        "1. Analyze user demographics and locations\n",
        "2. Explore tourist spot categories and locations\n",
        "3. Analyze user ratings and popular spots\n",
        "4. Build a recommendation system\n",
        "5. Forecast ratings using machine learning and deep learning algorithms\n",
        "\n",
        "## Dataset Description\n",
        "- user.csv: Contains user demographic data\n",
        "- tourism_with_id.csv: Provides details on tourist attractions\n",
        "- tourism_rating.csv: Contains user ratings for tourist spots\n",
        "\"\"\"\n",
        "\n",
        "# 4.1 Preliminary analysis\n",
        "# 4.1.1 Import the datasets into the Python environment\n",
        "user_df = pd.read_csv('user.csv')\n",
        "tourism_df = pd.read_csv('tourism_with_id.csv')\n",
        "ratings_df = pd.read_csv('tourism_rating.csv')\n",
        "\n",
        "# 4.1.2 Examine the dataset's shape and structure, and look out for any outlier\n",
        "print(\"User dataset shape:\", user_df.shape)\n",
        "print(\"Tourism dataset shape:\", tourism_df.shape)\n",
        "print(\"Ratings dataset shape:\", ratings_df.shape)\n",
        "\n",
        "user_df.info()\n",
        "tourism_df.info()\n",
        "ratings_df.info()\n",
        "\n",
        "# Check for missing values and duplicates\n",
        "for df, name in zip([user_df, tourism_df, ratings_df], ['User', 'Tourism', 'Ratings']):\n",
        "    print(f\"\\nMissing values in {name} dataset:\")\n",
        "    print(df.isnull().sum())\n",
        "    print(f\"Duplicates in {name} dataset: {df.duplicated().sum()}\")\n",
        "\n",
        "# 4.1.3 Merge the datasets\n",
        "merged_df = pd.merge(ratings_df, tourism_df, on='Place_Id')\n",
        "merged_df = pd.merge(merged_df, user_df, on='User_Id')\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.2 Exploratory Data Analysis\n",
        "\n",
        "### 4.2.1 Examine the overall date wise ratings\n",
        "\"\"\"\n",
        "\n",
        "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
        "daily_ratings = merged_df.groupby('date')['Place_Ratings'].mean().reset_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(daily_ratings['date'], daily_ratings['Place_Ratings'])\n",
        "plt.title('Average Daily Ratings Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.2 Rating fluctuations across different days of the week\n",
        "\"\"\"\n",
        "\n",
        "merged_df['day_of_week'] = merged_df['date'].dt.day_name()\n",
        "day_of_week_ratings = merged_df.groupby('day_of_week')['Place_Ratings'].mean().reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
        "day_of_week_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Day of Week')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.3 Rating trends for different months of the year\n",
        "\"\"\"\n",
        "\n",
        "merged_df['month'] = merged_df['date'].dt.month_name()\n",
        "monthly_ratings = merged_df.groupby('month')['Place_Ratings'].mean()\n",
        "monthly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Month')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.4 Rating distribution across different quarters\n",
        "\"\"\"\n",
        "\n",
        "merged_df['quarter'] = merged_df['date'].dt.quarter\n",
        "quarterly_ratings = merged_df.groupby('quarter')['Place_Ratings'].mean()\n",
        "quarterly_ratings.plot(kind='bar')\n",
        "plt.title('Average Ratings by Quarter')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.5 Compare performances of different tourist spots\n",
        "\"\"\"\n",
        "\n",
        "spot_performance = merged_df.groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('mean', ascending=False)\n",
        "print(spot_performance.head(10))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.6 Identify most popular tourist spots and their locations\n",
        "\"\"\"\n",
        "\n",
        "popular_spots = merged_df.groupby(['Place_Name', 'City'])['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "print(\"Most popular tourist spots overall:\")\n",
        "print(popular_spots.head(10))\n",
        "\n",
        "# For each city\n",
        "for city in merged_df['City'].unique():\n",
        "    city_spots = merged_df[merged_df['City'] == city].groupby('Place_Name')['Place_Ratings'].agg(['mean', 'count']).sort_values('count', ascending=False)\n",
        "    print(f\"\\nMost popular spot in {city}:\")\n",
        "    print(city_spots.head(1))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.7 Determine if the spot with highest visit count has the highest average rating\n",
        "\"\"\"\n",
        "\n",
        "spot_metrics = merged_df.groupby('Place_Name').agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "top_by_visits = spot_metrics.sort_values('visit_count', ascending=False).head(1)\n",
        "top_by_rating = spot_metrics.sort_values('Place_Ratings', ascending=False).head(1)\n",
        "\n",
        "print(\"Top spot by visit count:\")\n",
        "print(top_by_visits)\n",
        "print(\"\\nTop spot by average rating:\")\n",
        "print(top_by_rating)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "### 4.2.8 Identify the highest rated spot in each category and its visit count\n",
        "\"\"\"\n",
        "\n",
        "category_top_spots = merged_df.groupby(['Category', 'Place_Name']).agg({\n",
        "    'Place_Ratings': 'mean',\n",
        "    'User_Id': 'count'  # Using User_Id count as a proxy for visit count\n",
        "}).rename(columns={'User_Id': 'visit_count'})\n",
        "\n",
        "for category in category_top_spots.index.get_level_values('Category').unique():\n",
        "    top_spot = category_top_spots.loc[category].sort_values('Place_Ratings', ascending=False).head(1)\n",
        "    print(f\"\\nHighest rated spot in {category}:\")\n",
        "    print(top_spot)\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.3 Building a Recommendation System\n",
        "\n",
        "We'll use a simple collaborative filtering approach based on user-item interactions.\n",
        "\"\"\"\n",
        "\n",
        "# Create a user-item matrix\n",
        "user_item_matrix = merged_df.pivot_table(index='User_Id', columns='Place_Name', values='Place_Ratings')\n",
        "\n",
        "# Calculate cosine similarity between users\n",
        "user_similarity = cosine_similarity(user_item_matrix.fillna(0))\n",
        "\n",
        "# Function to get recommendations for a user\n",
        "def get_recommendations(user_id, n=5):\n",
        "    user_index = user_item_matrix.index.get_loc(user_id)\n",
        "    similar_users = user_similarity[user_index].argsort()[::-1][1:11]  # top 10 similar users\n",
        "\n",
        "    similar_users_ratings = user_item_matrix.iloc[similar_users]\n",
        "    user_ratings = user_item_matrix.loc[user_id]\n",
        "\n",
        "    recommendations = (similar_users_ratings.mean() - user_ratings).sort_values(ascending=False)\n",
        "    return recommendations.head(n)\n",
        "\n",
        "# Example recommendation\n",
        "example_user = user_item_matrix.index[0]\n",
        "print(f\"Recommendations for user {example_user}:\")\n",
        "print(get_recommendations(example_user))\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## 4.4 Forecasting using Deep Learning Algorithms\n",
        "\n",
        "We'll use an LSTM model to forecast future ratings for a specific tourist spot.\n",
        "\"\"\"\n",
        "\n",
        "# Choose a popular tourist spot for forecasting\n",
        "popular_spot = popular_spots.index[0][0]\n",
        "spot_ratings = merged_df[merged_df['Place_Name'] == popular_spot].set_index('date')['Place_Ratings']\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_ratings = scaler.fit_transform(spot_ratings.values.reshape(-1, 1))\n",
        "\n",
        "# Create sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i:(i + seq_length), 0])\n",
        "        y.append(data[i + seq_length, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 30\n",
        "X, y = create_sequences(scaled_ratings, seq_length)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build and train LSTM model\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Inverse transform predictions\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(spot_ratings.index[-len(y_test):], y_test, label='Actual')\n",
        "plt.plot(spot_ratings.index[-len(y_pred):], y_pred, label='Predicted')\n",
        "plt.title(f'Rating Forecast for {popular_spot}')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Rating')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Markdown cell\n",
        "\"\"\"\n",
        "## Conclusions and Recommendations\n",
        "\n",
        "[Add your conclusions and recommendations based on the analysis]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.2.2.1. <a id='toc1_4_2_2_1_'></a>[**Generate necessary features for the development of these models, like day of the week, quarter of the year, month, year, day of the month and so on**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh7GUi1iXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.2.2.2. <a id='toc1_4_2_2_2_'></a>[**Use the data from the last six months as the testing data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwFM7b7PXxTP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.2.2.3. <a id='toc1_4_2_2_3_'></a>[**Compute the root mean square error (RMSE) values for each model to compare their performances**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDxCuQUOXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.2.2.4. <a id='toc1_4_2_2_4_'></a>[**Use the best-performing models to make a forecast for the next year**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFFMhnlGXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4.3. <a id='toc1_4_3_'></a>[**4.4. Forecasting using deep learning algorithms**](#toc0_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.3.1. <a id='toc1_4_3_1_'></a>[**4.4.1. Use sales amount for predictions instead of item count**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNWJmttFXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.3.2. <a id='toc1_4_3_2_'></a>[**4.4.2. Build a long short-term memory (LSTM) model for predictions**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szr8uZOEXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.3.2.1. <a id='toc1_4_3_2_1_'></a>[**4.4.2.1. Define the train and test series**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1mnT3mKXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.3.2.2. <a id='toc1_4_3_2_2_'></a>[**4.4.2.2. Generate synthetic data for the last 12 months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGevcLfXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.3.2.3. <a id='toc1_4_3_2_3_'></a>[**4.4.2.3. Build and train an LSTM model**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0jK6AQvXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### 1.4.3.2.4. <a id='toc1_4_3_2_4_'></a>[**4.4.2.4. Use the model to make predictions for the test data**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMLYgzkWXxTQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.3.3. <a id='toc1_4_3_3_'></a>[**4.4.3. Calculate the mean absolute percentage error (MAPE) and comment on the model's performance**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Za61_FYXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1.4.3.4. <a id='toc1_4_3_4_'></a>[**4.4.4. Develop another model using the entire series for training, and use it to forecast for the next three months**](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEXfhjpUXxTR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN9oncYmXxTR"
      },
      "source": [
        "**Explanations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Why It Is Important:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Observations:**\n",
        "\n",
        "- [Placeholder for observations after running the code]\n",
        "\n",
        "**Conclusions:**\n",
        "\n",
        "- [Placeholder for conclusions based on initial data view]\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "- [Placeholder for recommendations based on initial data examination]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "capstone",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
