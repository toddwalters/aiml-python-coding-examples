{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# Implementing Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWs8JXRuGex"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMP7mglMuGT2"
   },
   "source": [
    "This tutorial focuses on the task of image segmentation, using a modified <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a>.\n",
    "\n",
    "## What Is image segmentation?\n",
    "\n",
    "In an image classification task, the network assigns a label (or class) to each input image. However, suppose you want to know the shape of that object, which pixel belongs to which object, and so on. In such case you need to assign a class to each pixel of the imageâ€”this task is known as segmentation. A segmentation model returns much more detailed information about the image. Image segmentation has many applications in medical imaging, self-driving cars, and satellite imaging, just to name a few.\n",
    "\n",
    "This tutorial uses the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). The dataset consists of images of 37 pet breeds, with 200 images per breed (~100 each in the training and test splits). Each image includes the corresponding labels and pixel-wise masks. The masks are class labels for each pixel. Each pixel is given one of three categories:\n",
    "\n",
    "- Class 1: Pixel belonging to the pet\n",
    "- Class 2: Pixel bordering the pet\n",
    "- Class 3: None of the above or a surrounding pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQWtaYdPq_cL"
   },
   "source": [
    "## Step 1: Install TensorFlow Examples\n",
    "- Install the TensorFlow examples package from the specified GitHub repository using pip\n",
    "\n",
    "**Note**: Install these packages only when using a local machine, not the Simplilearn lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26566,
     "status": "ok",
     "timestamp": 1724562883222,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "MQmKthrSBCld",
    "outputId": "1245584c-6c29-4646-82ac-cfc6dbb65c5a"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLFLCKo4sS-U"
   },
   "source": [
    "## Step 2: Import TensorFlow and TensorFlow Datasets\n",
    "- Import the TensorFlow library, which is a popular open-source framework for building and training various types of machine learning models\n",
    "- Import the TensorFlow Datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22135,
     "status": "ok",
     "timestamp": 1724562912540,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "YQX7R4bhZy5h",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XYC0QeKs8mM"
   },
   "source": [
    "## Step 3: Enhance Image Translation and Visualization with Pix2Pix and Matplotlib\n",
    "- Import the **pix2pix** module; it contains implementations of the Pix2Pix model, which is commonly used for image-to-image translation tasks like colorization and style transfer\n",
    "- Import the **clear_output** function to clear the output of the IPython notebook cell, providing cleaner and updated visualizations\n",
    "- Import the **matplotlib.pyplot** module, a popular plotting library in Python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1724562924774,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "g87--n2AtyO_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## Step 4: Download the Oxford-IIIT Pets Dataset\n",
    "\n",
    "The dataset is [available from TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). The segmentation masks are included in version 3+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1724563060841,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "40ITeStwDwZb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "In addition, the image color values are normalized to the **[0, 1]** range. Finally, as mentioned above, the pixels in the segmentation mask are labeled  **{1, 2, 3}**. For the sake of convenience, subtract 1 from the segmentation mask, resulting in labels **{0, 1, 2}**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCIaa_dr5FA6"
   },
   "source": [
    "## Step 5: Data Normalization for Improved Model Training\n",
    "- The **normalize** function takes **input_image** and **input_mask** as inputs, performs normalization operations on them, and returns the results.\n",
    "- The **input_image** is cast to **tf.float32** and divided by 255.0 to normalize the pixel values between 0 and 1.\n",
    "- The **input_mask** is subtracted by 1.\n",
    "- The function returns the normalized **input_image** and the updated **input_mask**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1724563078170,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "FD60EbcAQqov",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-Ke6vzq6oo3"
   },
   "source": [
    "## Step 6: Resize Image with Mask Loading and Perform Normalization in Machine Learning Tasks\n",
    "- Resize the image to (128, 128)\n",
    "- Call the **normalize** function to normalize the resized **input_image** and **input_mask**\n",
    "- Return the resized **input_image** and **input_mask**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1724563259761,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "Zf0S67hJRp3D",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_image(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "  input_mask = tf.image.resize(\n",
    "    datapoint['segmentation_mask'],\n",
    "    (128, 128),\n",
    "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "  )\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "## Step 7: Initialize Dataset and Training Parameters for Model Training\n",
    "\n",
    "**Note: The dataset already contains the required training and test splits, so continue to use the same splits.**\n",
    "\n",
    "- Define variables for training a model\n",
    "- Batch size is 64, which refers to the number of examples processed in one iteration during training\n",
    "- Set the buffer size to 1000\n",
    "- Calculate the number of steps needed to complete one epoch (a full pass through the dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1724563262711,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "yHwj2-8SaQli",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkUtMy4v9wdY"
   },
   "source": [
    "## Step 8: Load Image Data and Preprocess from the Dataset\n",
    "\n",
    "- Load the **train** split of the dataset and perform preprocessing or transformations on the images\n",
    "- Enable parallel processing of the images for improved performance\n",
    "- Load the **test** split of the dataset\n",
    "- Perform similar preprocessing or transformations on the test images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1724563267018,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "39fYScNz9lmo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9hGHyg8L3Y1"
   },
   "source": [
    "## Step 9: Perform Augmentation for Input and Label Data\n",
    "\n",
    "- The following class performs a simple augmentation by randomly flipping an image.\n",
    "Go to the [Image augmentation](data_augmentation.ipynb) tutorial to learn more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 511,
     "status": "ok",
     "timestamp": 1724563272936,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "fUWdDJRTL0PP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "\n",
    "  def call(self, inputs, labels):\n",
    "    inputs = self.augment_inputs(inputs)\n",
    "    labels = self.augment_labels(labels)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTIbNIBdcgL3"
   },
   "source": [
    "## Step 10: Perform Data Preparation and Batching\n",
    "Build the input pipeline, applying the augmentation after batching the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1724563276618,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "VPscskQcNCx4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_images\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_images.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "## Step 11: Perform Image Display Function for Visualization\n",
    "- Visualize an image example and its corresponding mask from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 612,
     "status": "ok",
     "timestamp": 1724563281026,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "3N2RPAAW9q4W",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERmdb4XX_Vbu"
   },
   "source": [
    "## Step 12: Visualize the Sample Images and Masks from Training Batches\n",
    "- Iterate over the first two batches of images and masks from the train_batches dataset\n",
    "- **sample_image** is assigned as the first image in the batch (images[0]), and **sample_mask** is assigned as the corresponding mask (masks[0]).\n",
    "- Visualize the sample_image and sample_mask using the **display** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10874,
     "status": "ok",
     "timestamp": 1724563373743,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "a6u_Rblkteqb",
    "outputId": "882e10f3-dca7-40bd-ec18-5d18eb8b6c22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(2):\n",
    "  sample_image, sample_mask = images[0], masks[0]\n",
    "  display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## Define the Model\n",
    "The model being used here is a modified [U-Net](https://arxiv.org/abs/1505.04597). A U-Net consists of an encoder (downsampler) and decoder (upsampler). To learn robust features and reduce the number of trainable parameters, use a pretrained modelâ€”[MobileNetV2](https://arxiv.org/abs/1801.04381)â€”as the encoder. For the decoder, you will use the upsample block, which is already implemented in the [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) example in the TensorFlow Examples repository. (Check out the [pix2pix: Image-to-image translation with a conditional GAN](../generative/pix2pix.ipynb) tutorial in the notebook.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv9VFb1qA47Q"
   },
   "source": [
    "## Step 13: Implement Feature Extraction with MobileNetV2 for Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4mQle3lthit"
   },
   "source": [
    "- Create an instance of the MobileNetV2 model as the base_model for feature extraction\n",
    "- The **input_shape** parameter defines the shape of the input images as **[128, 128, 3]**, representing 128x128 pixel RGB images.\n",
    "- A list of layer_names is defined, representing the intermediate layers of the MobileNetV2 model at specific resolutions.\n",
    "- By setting down_stack.trainable to False, the weights of the down_stack model are frozen, and they will not be updated during training.\n",
    "- Only the additional layers added on top of this feature extraction model will be trainable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3842,
     "status": "ok",
     "timestamp": 1724563469127,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "liCeLH0ctjq7",
    "outputId": "4203b318-7297-4580-cb5d-986e6427cd3d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',\n",
    "    'block_3_expand_relu',\n",
    "    'block_6_expand_relu',\n",
    "    'block_13_expand_relu',\n",
    "    'block_16_project',\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPw8Lzra5_T9"
   },
   "source": [
    "## Step 14: Upsample Layers for Feature Map Enhancement in Pix2Pix Architecture\n",
    "- The decoder or upsampler is simply a series of upsample blocks implemented in TensorFlow examples.\n",
    "- Each **pix2pix.upsample()** call increases the image size by performing an upsampling operation with a specified size and kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1724563474238,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "p0ZbfywEbZpJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),\n",
    "    pix2pix.upsample(256, 3),\n",
    "    pix2pix.upsample(128, 3),\n",
    "    pix2pix.upsample(64, 3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQZw1vyEnjNh"
   },
   "source": [
    "## Step 15: Perform Comprehensive Architecture with Downsampling, Upsampling, and Skip Connections\n",
    "- Define input layer\n",
    "- Downsample the input image\n",
    "- Reverse the skip connections\n",
    "- Upsample the image and perform skip connections\n",
    "- Concatenate the upsampled image and the corresponding skip connection\n",
    "- Perform the final convolutional transpose layer\n",
    "- Apply the final transpose convolution operation\n",
    "- Create and return the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1724563486041,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "45HByxpVtrPF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRsjdZuEnZfA"
   },
   "source": [
    "**Note**:\n",
    "The number of filters on the last layer is set to the number of **output_channels**. This will be one output channel per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## Step 16: Train the Model\n",
    "- Declare the number of output classes for segmentation\n",
    "- Create the U-Net model with the specified number of output channels or classes\n",
    "- Compile the model with the optimizer, loss function, and metrics\n",
    "- Sparse categorical cross entropy loss\n",
    "- Track accuracy as a metric during training\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- For a multiclass classification task, utilize the **tf.keras.losses.CategoricalCrossentropy loss function**.\n",
    "- Set the **from_logits** argument to **True** because the labels are given as single integers, not as score vectors for each class's pixel\n",
    "- When running inference, the label assigned to the pixel is the channel with the highest value. This is what the **create_mask** function is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1724563493712,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "6he36HK5uKAc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 3\n",
    "\n",
    "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVMzbIZLcyEF"
   },
   "source": [
    "## Step 17: Plot the Resulting Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2349,
     "status": "ok",
     "timestamp": 1724563499497,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "sw82qF1Gcovr",
    "outputId": "ad8f1e10-d828-4e82-a08f-bbe8bc94218f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "## Step 18: Try Out the Model to Check What It Predicts Before Training\n",
    "- Convert predicted mask to a single-channel mask\n",
    "- Get the channel with the highest value\n",
    "- Add a new axis to make it a single-channel mask\n",
    "- Return the first (and presumably only) mask in the batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1724563506311,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "UwvIKLZPtxV_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqQJhiMpqIUj"
   },
   "source": [
    "## Step 19: Visualize Model Predictions: Show Image Segmentation Results with Predicted Masks\n",
    "- Iterate over the dataset and display predictions\n",
    "- Generate a predicted mask for the current image\n",
    "- Display the original image, true mask, and predicted mask\n",
    "- Display predictions for a sample image\n",
    "- Generate and display the predicted mask for the sample image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 516,
     "status": "ok",
     "timestamp": 1724563510200,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "YLNsrynNtx4d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "executionInfo": {
     "elapsed": 5247,
     "status": "ok",
     "timestamp": 1724563520666,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "X_1CC0T4dho3",
    "outputId": "c1e65540-c706-4fd0-d78d-670397abb2fb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luhrWHQ6slLy"
   },
   "source": [
    "**Observation:**\n",
    "\n",
    "- Overall, the show_predictions function allows for the visual examination of image segmentation predictions, providing insights into the performance of the model on the dataset or a single sample image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEChTxyDrUdH"
   },
   "source": [
    "## Step 20: DisplayCallback - Callback for Visualizing Sample Predictions at the End of Each Epoch\n",
    "- Clear the output to update the display\n",
    "- Display sample predictions\n",
    "- Print the epoch number for reference\n",
    "\n",
    "**Note:**\n",
    "- The callback defined below is used to observe how the model improves while it is trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1724563563518,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "wHrHsqijdmL6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPZvZNbCrlx_"
   },
   "source": [
    "## Step 21: Training and Validation with Visualized Sample Predictions - Monitor Model Performance and Display Results\n",
    "- The number of epochs for training is 7.\n",
    "- The number of validation sub-splits is 5.\n",
    "- Train the model and store the training history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 825389,
     "status": "ok",
     "timestamp": 1724564392626,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "StKDH_B9t4SD",
    "outputId": "0412034c-0a4b-4133-9be4-3c968c23bd36",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4Y-ispDs_1k"
   },
   "source": [
    "**Observation:**\n",
    "- The code trains the model for a specific number of epochs, performs validation during training, and includes a callback to display sample predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmtV3gJstWOw"
   },
   "source": [
    "## Step 22: Visualize the Training and Validation Loss over Epochs\n",
    "- Extract loss values from model_history\n",
    "- Plot the loss values\n",
    "- Plot training loss in red\n",
    "- Plot validation loss as blue dots\n",
    "- Set the title of the plot and the label for the x-axis and y-axis\n",
    "- Set the y-axis limits\n",
    "- Add a legend to the plot\n",
    "- Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1724564397649,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "P_mu0SAbt40Q",
    "outputId": "d8f562d0-4a7a-4dc2-94cf-dcf0eb73b9e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## Step 23: Make Predictions\n",
    "- Display predictions for three samples from the **test_batches** dataset\n",
    "\n",
    "**Note:**\n",
    "- In the interest of saving time, the number of epochs was kept small, but it can be set to higher values to achieve more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 21109,
     "status": "ok",
     "timestamp": 1724564424653,
     "user": {
      "displayName": "Dr. Nisarg Gandhewar",
      "userId": "08865821500269811620"
     },
     "user_tz": -330
    },
    "id": "ikrzoG24qwf5",
    "outputId": "712d0256-c92c-486d-8ea2-892d28af4ca1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_predictions(test_batches, 3)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
