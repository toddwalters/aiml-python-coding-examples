{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzofrkekwI74"
   },
   "source": [
    "# Photo Editing Feature Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FMzPHaVT62z"
   },
   "source": [
    "## Step 1: Import Dependencies and VGG19 Model from TensorFlow.Keras\n",
    "- Import TensorFlow, NumPy, and Matplotlib library\n",
    "- Import the **VGG19 model** from tensorflow.keras\n",
    "- Import requests library\n",
    "- Import **Image** module from PIL library\n",
    "-Import **BytesIO** module from io library\n",
    "- Print TensorFlow version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09iY28nVGq0I",
    "outputId": "0a27797f-73b8-412b-840d-6afd04d0e15c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.applications.vgg19 as vgg19\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uF3RTzNKwkf_"
   },
   "source": [
    "## Step 2: Visualize Images with Matplotlib\n",
    "- Load an image from a given URL and resize it to the specified target size\n",
    "- Return the loaded image as a numpy array\n",
    "- Display multiple images in a grid layout\n",
    "- If only one image is provided, display it in isolation\n",
    "- Plot a simple line graph using a list of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRSG76NKH1iC"
   },
   "outputs": [],
   "source": [
    "'''Parameters to load image\n",
    "    url: url of image\n",
    "    target_size: size of image\n",
    "   Returns\n",
    "    Image in form of numpy array\n",
    "'''\n",
    "def load_image_from_url(url,target_size=None):\n",
    "    img_request=requests.get(url)\n",
    "    img=Image.open(BytesIO(img_request.content))\n",
    "    if target_size==None:\n",
    "        return np.array(img)\n",
    "    else:\n",
    "        return np.array(img.resize(target_size))\n",
    "\n",
    "'''Parameters\n",
    "    images= list of images to plot\n",
    "    num_rows= number of images in a row (for multiple image plotting)\n",
    "'''\n",
    "def plot_image_grid(images,num_rows=1):\n",
    "    n=len(images)\n",
    "    if n > 1:\n",
    "        num_cols=np.ceil(n/num_rows)\n",
    "        fig,axes=plt.subplots(ncols=int(num_cols),nrows=int(num_rows))\n",
    "        axes=axes.flatten()\n",
    "        fig.set_size_inches((15,15))\n",
    "        for i,image in enumerate(images):\n",
    "            axes[i].imshow(image)\n",
    "    else:\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.imshow(images[0])\n",
    "\n",
    "'''Parameters\n",
    "    data= list of data to plot\n",
    "'''\n",
    "def plot_graph(data):\n",
    "    plt.plot(data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8u4WuhzUyabJ"
   },
   "source": [
    "## Step 3: Provide Style Image and Content Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6UBmoc7tKXJ6"
   },
   "outputs": [],
   "source": [
    "style_url=\"https://assets.catawiki.nl/assets/2019/7/30/f/8/5/f8508825-ec5a-4f5e-bc61-73ff3ded88e2.jpg\"\n",
    "content_url=\"https://img.bleacherreport.net/img/images/photos/003/825/453/hi-res-ade0501d521ed2716586baa68416bf81_crop_north.jpg?h=533&w=800&q=70&crop_x=center&crop_y=top\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szQK_GV3yot0"
   },
   "source": [
    "## Step 4: Load and Plot Images from URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "xnoAKc-mLgA4",
    "outputId": "5b15050d-0ff3-4627-ba0a-41e7bf75dbc3"
   },
   "outputs": [],
   "source": [
    "style_image=load_image_from_url(style_url,target_size=(1024,720))\n",
    "\n",
    "content_image=load_image_from_url(content_url,target_size=(1024,720))\n",
    "\n",
    "plot_image_grid([style_image,content_image])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VPNmWRyzJwB"
   },
   "source": [
    "## Step 5: Content and Style Layers Configuration for Neural Network Models\n",
    "- Define two lists to specify the layers to be used for content and style extraction in a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq1K0IvBMlMd"
   },
   "outputs": [],
   "source": [
    "CONTENT_LAYERS=['block4_conv2']\n",
    "\n",
    "STYLE_LAYERS=['block1_conv1','block2_conv1','block3_conv1', 'block4_conv1', 'block5_conv1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3kT0mpX0C3C"
   },
   "source": [
    "## Step 6: Create a Model Using Keras Functional API\n",
    "\n",
    "- Define a function for creating the VGG model with average pooling\n",
    "- Set the flag to include or exclude the top (fully connected) layers (set to False)\n",
    "- Specify the pooling type to use (set to **avg** for average pooling) and type of weights to load (set to **imagenet**)\n",
    "- Freeze the model's weights to prevent training\n",
    "- Assign the CONTENT_LAYERS list to the variable content_layer\n",
    "and  STYLE_LAYERS list to the variable style_layer\n",
    "- Obtain the output tensors of the specified content and style layers in the VGG model\n",
    "- Return a new Keras model that takes the input of the original VGG model and outputs the selected layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdUDckaxNxjv"
   },
   "outputs": [],
   "source": [
    "def create_vgg_model():\n",
    "    model=vgg19.VGG19(include_top=False,pooling='avg',weights='imagenet')\n",
    "    model.trainable=False\n",
    "    content_layer=CONTENT_LAYERS\n",
    "    style_layer=STYLE_LAYERS\n",
    "    output_layers=[model.get_layer(layer).output for layer in (content_layer + style_layer)]\n",
    "    return tf.keras.models.Model(model.input,output_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZyj2XVx0krd"
   },
   "source": [
    "## Step 7: Preprocess Content and Style Images Using VGG19\n",
    "The VGG model requires images to be in the BGR format instead of the RGB format, necessitating preprocessing before use.\n",
    "- Preprocess the content and style image by expanding its dimensions and applying VGG19-specific preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q7vdhdYQWps"
   },
   "outputs": [],
   "source": [
    "processed_content_image=vgg19.preprocess_input(np.expand_dims(content_image,axis=0))\n",
    "processed_style_image=vgg19.preprocess_input(np.expand_dims(style_image,axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMM0xOlS1CqF"
   },
   "source": [
    "## Step 8: Deprocess Function for VGG19-Preprocessed Images\n",
    "- Define a function to deprocess an image that has been preprocessed with VGG19-specific preprocessing\n",
    "- Make a copy of the processed image\n",
    "- If the image has four dimensions, squeeze it to remove the batch dimension\n",
    "- Check that the image has three dimensions [height, width, and channel]\n",
    "- Input to deprocess image must be an image of dimension [1, height, width, and channel] or [height, width, and channel]\n",
    "- Raise a ValueError if the image has an invalid number of dimensions\n",
    "- Deprocess the image by reversing the VGG19-specific preprocessing steps\n",
    "- Return the deprocessed image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNQCApcFRWu-"
   },
   "outputs": [],
   "source": [
    "'''Parameters\n",
    "    processed_img= Processed Image in BGR Format\n",
    "   Returns\n",
    "    Image with RGB Format\n",
    "'''\n",
    "\n",
    "# The deprocess_image function is used to convert an image that has been processed or normalized back into its original form,\n",
    "# suitable for visualization. This is typically done after neural network operations, such as feature extraction or style transfer,\n",
    "# where images are often normalized and need to be converted back to their original pixel values.\n",
    "\n",
    "def deprocess_image(processed_img):\n",
    "    x = processed_img.copy()\n",
    "    if len(x.shape) == 4:\n",
    "        x = np.squeeze(x, 0)\n",
    "    assert len(x.shape) == 3, (\"Input to deprocess image must be an image of dimension [1, height, width, channel] or [height, width, channel]\")\n",
    "    if len(x.shape) != 3:\n",
    "        raise ValueError(\"Invalid input to deprocessing image\")\n",
    "\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTQcSIuI1-Eu"
   },
   "source": [
    "## Step 9: Create Instance of VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHpOMbuaSU3A",
    "outputId": "ec19cc28-32ec-48e2-df54-2ad51a290a12"
   },
   "outputs": [],
   "source": [
    "model=create_vgg_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6BUoPVd2w-y"
   },
   "source": [
    "## Step 10: Calculate Content Loss for Neural Style Transfer\n",
    "- Compute the squared difference between the content representations\n",
    "- Calculate the mean of the squared differences\n",
    "-Return the content loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_XV_vyRUa_h"
   },
   "outputs": [],
   "source": [
    "'''\n",
    " Content Loss= Mean((new_image-base_image)^2)\n",
    "'''\n",
    "def get_content_loss(new_image_content,base_image_content):\n",
    "    return tf.reduce_mean(tf.square(new_image_content-base_image_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMDJ63NW56u7"
   },
   "source": [
    "## Step 11: Calculate Normalized Gram Matrix for Linear Independence\n",
    "The Gram matrix plays a crucial role in determining linear independence. A collection of vectors is considered linearly independent only if the Gram determinant, that is, the determinant of the Gram matrix, is not equal to zero.\n",
    "\n",
    "- Get the number of channels in the output\n",
    "- Reshape the output tensor to have a shape of [-1, channels]\n",
    "- Compute the Gram matrix by multiplying the reshaped tensor with its transpose\n",
    "- Get the number of elements in the Gram matrix\n",
    "- Normalize the Gram matrix by dividing it by the number of elements\n",
    "- Return the normalized Gram matrix and the number of elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GwS5B2eUyd7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    " Gram Matrix of a Image is a matrix which we will use to compute correlation between images and will be used in style loss to compute how style of one image\n",
    " is similar to other\n",
    "'''\n",
    "def get_gram_matrix(output):\n",
    "    channels=output.shape[-1]\n",
    "    a=tf.reshape(output,[-1,channels])\n",
    "    gram_matrix=tf.matmul(a,a,transpose_a=True)\n",
    "    n=int(gram_matrix.shape[0])\n",
    "    return gram_matrix/tf.cast(n,'float32'),n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1ADo6iqSW3o"
   },
   "source": [
    "## Step 12: Calculate Style Loss Using Gram Matrices in Neural Style Transfer\n",
    "- Compute the Gram matrices and heights for the new image and base style\n",
    "- Ensure that the heights of the Gram matrices are the same\n",
    "- Get the number of gram features and channels from the new style Gram matrix\n",
    "- Calculate the style loss using the formula\n",
    "- Return the style loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzBiqQsJVq41"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Style Loss= Mean(1/(2 X nH X nW X nC)^2 X (GramMatrix(style_image)-GramMatrix(new_image))^2)\n",
    "'''\n",
    "def get_style_loss(new_image_style,base_style):\n",
    "    new_style_gram,new_gram_height=get_gram_matrix(new_image_style)\n",
    "    base_style_gram,base_gram_height=get_gram_matrix(base_style)\n",
    "    assert new_gram_height==base_gram_height\n",
    "    gram_features=int(new_style_gram.shape[0])\n",
    "    gram_channels=int(new_style_gram.shape[-1])\n",
    "    loss=tf.reduce_mean(tf.square(base_style_gram-new_style_gram)/(2*int(new_gram_height)*(gram_features)*(gram_channels))**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zQ1-JTJWBPG"
   },
   "source": [
    "## Step 13: Calculate Total Loss for Neural Style Transfer with Content and Style Losses\n",
    "\n",
    "- Separate the style representations of the new image and base style\n",
    "- Compute the style loss\n",
    "- Separate the content representations of the new image and base content\n",
    "- Compute the content loss\n",
    "- Compute the total loss by combining the content and style losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x9qvEmNXIwp"
   },
   "outputs": [],
   "source": [
    "def get_total_loss(new_image_output,base_content_output,base_style_output,alpha=0.001,beta=1):\n",
    "    new_image_styles=new_image_output[len(CONTENT_LAYERS):]\n",
    "    base_image_styles=base_style_output[len(CONTENT_LAYERS):]\n",
    "    style_loss=0\n",
    "    n=len(new_image_styles)\n",
    "    for i in range(n):\n",
    "        style_loss+=get_style_loss(new_image_styles[i],base_image_styles[i])\n",
    "    new_image_contents=new_image_output[:len(CONTENT_LAYERS)]\n",
    "    base_image_contents=base_content_output[:len(CONTENT_LAYERS)]\n",
    "    content_loss=0\n",
    "    n=len(new_image_contents)\n",
    "    for i in range(n):\n",
    "        content_loss+=get_content_loss(new_image_contents[i],base_image_contents[i])/n\n",
    "    return alpha*content_loss+ beta*style_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRMUblfu2X0H"
   },
   "source": [
    "## Step 14: Extract Output and Generating Image in Neural Style Transfer\n",
    "- Generated_image is a tensorflow variable which represents new generated image by a network.\n",
    "- Its value will be changed by training it to minimize the total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQa24LQ7evgV"
   },
   "outputs": [],
   "source": [
    "base_style_outputs=model(processed_style_image)\n",
    "base_content_outputs=model(processed_content_image)\n",
    "\n",
    "generated_image=tf.Variable(processed_content_image+tf.random.normal(processed_content_image.shape))\n",
    "optimizer=tf.optimizers.Adam(learning_rate=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JO0CmLv79QCJ"
   },
   "source": [
    "## Step 15: Optimize Generated Image Values Based on Minimizing Total Loss Function\n",
    "- Create lists to store generated images and store loss values\n",
    "- Initialize value for best loss\n",
    "- Initialize the best image with the generated image\n",
    "- Observe the generated image and optimize its values\n",
    "- Get the output from the model for the generated image\n",
    "- Calculate the total loss of the images and append it to the list\n",
    "- Compute the gradient of the loss with respect to the generated image and apply the gradient update to the generated image\n",
    "- Clip the image to be in the range of 0-255 and assign the clipped value to the generated image variable\n",
    "- Store the deprocessed generated image in the list every 10 iterations\n",
    "- Update the best image if the current loss is better than the previous best loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5jLuv0x-f76P"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Parameters\n",
    "        iterations= number of times to run optimization\n",
    "    Returns\n",
    "        best_image : Best Image in all iterations from network\n",
    "        images: List of Images that are optimized in every iterations this is to track how number of iterations change image appearence\n",
    "        losses: Total Loss which can be tracked or plot in each iteration\n",
    "'''\n",
    "def optimize(iterations):\n",
    "    images=[]\n",
    "    losses=[]\n",
    "    best_loss=20000000\n",
    "    best_image=generated_image.numpy()\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"Iteration \",i)\n",
    "            tape.watch(generated_image) ## Keep Eye on our generated image and optimize its values\n",
    "            generated_image_content=model(generated_image) ## get output from model for generated image\n",
    "            loss=get_total_loss(generated_image_content,base_content_outputs,base_style_outputs) ## find total loss of images\n",
    "            losses.append(loss.numpy())\n",
    "            gradient=tape.gradient(loss,generated_image)\n",
    "            optimizer.apply_gradients(zip([gradient],[generated_image]))\n",
    "            generated_image_clipped=tf.clip_by_value(generated_image, 0, 255) ## Clip image to be in range 0-255\n",
    "            generated_image.assign(generated_image_clipped) ## assign clipped value of to generated_image variable\n",
    "            print(\"LOSS= {0}\".format(loss.numpy()))\n",
    "            if i%10==0:\n",
    "                images.append(deprocess_image(generated_image.numpy()))\n",
    "            if loss<best_loss:\n",
    "                best_image=generated_image.numpy()\n",
    "                best_loss=loss\n",
    "    return best_image,images,losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3qGR2-T-Hqb"
   },
   "source": [
    "## Step 16: Optimize Neural Style Transfer and Visualize Loss\n",
    "- Perform optimization for 10 iterations\n",
    "- Plot the loss values over iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777
    },
    "id": "Gte3wZkKj0WP",
    "outputId": "6ff86d7d-a2a0-40ec-88a9-f0a4f16fe528"
   },
   "outputs": [],
   "source": [
    "result,images,losses=optimize(10)\n",
    "plot_graph(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJUWienm_HdP"
   },
   "source": [
    "## Step 17: Plot All Images Obtained in Each Iteration of Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "Qyj8qeuyl2hN",
    "outputId": "624f73e3-fab0-42d0-9161-517d0e2037e9"
   },
   "outputs": [],
   "source": [
    "plot_image_grid(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOBLiVZi_YIn"
   },
   "source": [
    "## Step 18: Plot the Final Best Output from the Network\n",
    "- Set the figure size\n",
    "- Display the final best output from the network\n",
    "- Remove the axis labels\n",
    "- Add a title and show the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "id": "JIa6n3s7GpCO",
    "outputId": "d2501bda-4872-4dee-92fd-a2d967b29d4a"
   },
   "outputs": [],
   "source": [
    "plot_image_grid([deprocess_image(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 627
    },
    "id": "oPFoI2q0NxtN",
    "outputId": "ff8f27f5-3825-49b9-83da-f840e402796b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(deprocess_image(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1jV7bIO8k2k"
   },
   "source": [
    "**Observation:**\n",
    "- The code fetches a style image and a content image from provided URLs.\n",
    "- It utilizes a VGG19 model to perform neural style transfer, blending the style of the style image with the content of the content image.\n",
    "- After running optimization for 10 iterations, the code plots the loss, displays intermediate results every 10 iterations, and finally displays the stylized image."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
