{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X4LOpuPqoKJ"
   },
   "source": [
    "# __Dropout Regularization__\n",
    "\n",
    "Dropout is a technique where:\n",
    "\n",
    "- Randomly selected neurons are ignored during training. They are \"dropped out\" randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass, and any weight updates are not applied to the neuron on the backward pass.\n",
    "\n",
    "\n",
    "- If neurons are randomly dropped out of the network during training, other neurons will have to step in and handle the representation required to make predictions for the missing neurons. This is believed to result in multiple independent internal representations being learned by the network.\n",
    "\n",
    "- The effect is that the network becomes less sensitive to the specific weights of neurons. This, in turn, results in a network that is capable of better generalization and is less likely to overfit the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmVB_n8GR4wD"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing the required libraries\n",
    "2. Reading a CSV file into a DataFrame\n",
    "3. Creating the dummies\n",
    "4. Preparing the data for modeling\n",
    "5. Performing K-fold cross-validation and model training\n",
    "6. Calculating the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XoOcXJ0qoKM"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- Imports libraries for data preprocessing, including z-score standardization using **scipy.stats.zscore**, and data manipulation using **pandas**. It also imports libraries for model evaluation, such as metrics from **sklearn** and train-test splitting from **sklearn.model_selection**.\n",
    "- Imports the necessary components from TensorFlow Keras (**Sequential** and **Dense**) to build a neural network model. These components allow for the creation of a sequential model with dense layers and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdUY7abFqoKO"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeKij9Zzs3SU"
   },
   "source": [
    "### Step 2: Reading a CSV File into a DataFrame\n",
    "- It reads a CSV file from a given URL and stores it in a Pandas DataFrame by using **na_values** to replace specified values with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q4ycIFxqoKP"
   },
   "outputs": [],
   "source": [
    "# dataset link : \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKmLCf6hqoKP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the data set\n",
    "\n",
    "\n",
    "\n",
    "# Read the data set\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
    "    na_values=['NA','?'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgM45IbLqoKP",
    "outputId": "ba56d9b5-8a4c-43e2-ea25-33d4821c774d"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYT-VXxJthjR"
   },
   "source": [
    "**Observation**\n",
    "- The output appears to be a tabular representation of a dataset with various columns.\n",
    "- Each row represents a sample or instance, while each column represents a different attribute or feature of that instance.\n",
    "- The columns contain information such as the ID, job, area, income, aspect, subscriptions, dist_healthy, save_rate, dist_unhealthy, age, pop_dense, retail_dense, crime, and product.\n",
    "- The values in the columns represent specific measurements or categories related to each attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtehu89Aunu3"
   },
   "source": [
    "### Step 3: Creating the Dummies\n",
    "\n",
    "- It uses the **pd.get_dummies()** function to convert categorical columns **'job'** and **'area'** into dummy variables, which represent the presence or absence of each category as binary values.\n",
    "\n",
    "- The resulting dummy variables are concatenated with the original DataFrame **df** using **pd.concat()**, which adds the dummy variables as new columns.\n",
    "\n",
    "- Finally, the original categorical columns **'job'** and **'area'** are dropped from the DataFrame using the **df.drop()** function with the **axis=1** parameter set to remove columns. This ensures that only the dummy variables remain in the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pb88StyqoKQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\",dtype=\"int\")],axis=1)\n",
    "df.drop('job', axis=1, inplace=True)\n",
    "\n",
    "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\",dtype=\"int\")],axis=1)\n",
    "df.drop('area', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv2zYQ0MwvAg"
   },
   "source": [
    "- The missing values in the **'income'** column of the DataFrame **'df'** are filled with the median value of the **'income'** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaqcPQtHqoKQ"
   },
   "outputs": [],
   "source": [
    "med = df['income'].median()\n",
    "df['income'] = df['income'].fillna(med)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-5E_lZaxL1d"
   },
   "source": [
    "- The specified columns **('income', 'aspect', 'save_rate', 'age', 'subscriptions')** in the DataFrame **df** are standardized using z-score normalization, which transforms the values to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnYtRHSvqoKQ"
   },
   "outputs": [],
   "source": [
    "df['income'] = zscore(df['income'])\n",
    "df['aspect'] = zscore(df['aspect'])\n",
    "df['save_rate'] = zscore(df['save_rate'])\n",
    "df['age'] = zscore(df['age'])\n",
    "df['subscriptions'] = zscore(df['subscriptions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zfg9pumOx4WQ"
   },
   "source": [
    "### Step 4: Preparing the Data for Modeling\n",
    "- It selects the relevant columns from the DataFrame df by dropping the **'product'** and **'id'** columns and assigns them to the variable **x_columns**.\n",
    "- It creates dummy variables for the **'product'** column using one-hot encoding and assigns the column names to the variable products. The target variable **'y'** is assigned the corresponding dummy variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhxMe5jxqoKQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "x_columns = df.columns.drop('product').drop('id')\n",
    "x = df[x_columns].values\n",
    "dummies = pd.get_dummies(df['product'])\n",
    "products = dummies.columns\n",
    "y = dummies.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q70MjRl2y01_"
   },
   "source": [
    "### Step 5: Performing K-Fold Cross-Validation and Model Training\n",
    "- Train a model using K-fold cross-validation with 5 folds.\n",
    "- The model consists of a sequential neural network with two hidden layers, using ReLU activation for the first hidden layer and L1 regularization for the second hidden layer.\n",
    "- Dropout is applied to the first hidden layer to prevent overfitting.\n",
    "- The model is trained using the Adam optimizer and categorical cross-entropy loss function.\n",
    "- The accuracy of each fold is calculated and printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxvIm-imqoKR",
    "outputId": "43a559cd-989e-4889-af48-d33ca504f3c3"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# cross_validate\n",
    "kf=  KFold(5, shuffle =True, random_state =42)\n",
    "\n",
    "oos_y = []\n",
    "oos_pred = []\n",
    "fold = 0\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "\n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "\n",
    "    #kernel_regularizer=regularizers.l2(0.01),\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(25, activation='relu', \\\n",
    "                activity_regularizer=regularizers.l1(1e-4))) # Hidden 2\n",
    "    # Usually do not add dropout after final hidden layer\n",
    "    #model.add(Dropout(0.5))\n",
    "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    model.fit(x_train,y_train,validation_data=(x_test,y_test),\\\n",
    "              verbose=0,epochs=10)\n",
    "\n",
    "    pred = model.predict(x_test)\n",
    "    oos_y.append(y_test)\n",
    "    # raw probabilities to chosen class (highest probability)\n",
    "    pred = np.argmax(pred,axis=1)\n",
    "    oos_pred.append(pred)\n",
    "\n",
    "    # Measure this fold's accuracy\n",
    "    y_compare = np.argmax(y_test,axis=1) # For accuracy calculation\n",
    "    score = metrics.accuracy_score(y_compare, pred)\n",
    "    print(f\"Fold score (accuracy): {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqaJM7COzpSm"
   },
   "source": [
    "**Observation**\n",
    "- The output shows the accuracy scores for each fold of the cross-validation process:\n",
    "\n",
    "  - Fold Scores: The output displays the fold number (e.g., Fold #1) and the corresponding accuracy score (e.g., 0.67) for each fold. The accuracy score represents the proportion of correctly predicted labels to the total number of labels in the test set. Higher accuracy scores indicate better performance of the model on the test data.\n",
    "\n",
    "  - Performance Variation: The output demonstrates that the model's performance varies across different folds. This variation can provide insights into the stability and robustness of the model. The accuracy scores range from 0.61 to 0.67, suggesting that the model performs reasonably well but with some degree of variability across different subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB61-VRSz0J7"
   },
   "source": [
    "### Step 6: Calculating the Error\n",
    "- It calculates the final accuracy score and creates a DataFrame combining the original data with the true values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYicfMe1qoKS",
    "outputId": "e41250a3-0857-44ce-9798-b1f00ef5abeb"
   },
   "outputs": [],
   "source": [
    "# Calculate the error\n",
    "\n",
    "oos_y = np.concatenate(oos_y)\n",
    "oos_pred = np.concatenate(oos_pred)\n",
    "oos_y_compare = np.argmax(oos_y,axis=1) # For accuracy calculation\n",
    "\n",
    "score = metrics.accuracy_score(oos_y_compare, oos_pred)\n",
    "print(f\"Final score (accuracy): {score}\")\n",
    "\n",
    "# Write the cross-validated prediction\n",
    "oos_y = pd.DataFrame(oos_y)\n",
    "oos_pred = pd.DataFrame(oos_pred)\n",
    "oosDF = pd.concat( [df, oos_y, oos_pred],axis=1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isI6yXg001L2"
   },
   "source": [
    "\n",
    "**Observation**\n",
    "- The final accuracy score achieved by the model is 0.6455."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebcsmB7_qoKS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
