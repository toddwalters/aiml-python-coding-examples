{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUdqSnzsBy2o"
   },
   "source": [
    "## __RMSprop__\n",
    "- A problem with AdaGrad is that it can slow the search down too much, resulting in very small learning rates for each parameter or dimension of the search by the end of the run.\n",
    "\n",
    "- This has the effect of stopping the search too soon before the minimum can be located.\n",
    "\n",
    "- It is an extension of AdaGrad, which uses a decaying average or moving average of the partial derivatives instead of the sum in the calculation of\n",
    "the learning rate for each parameter.\n",
    "- This is achieved by adding a new hyperparameter called rho that acts like momentum for the partial derivatives.\n",
    "- Using a decaying moving average of the partial derivative allows the search to forget early partial derivative values and focus on the most recently seen shape of the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQlCp_LTlQp9"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing the required libraries\n",
    "2. Defining the objective function\n",
    "3. Defining the derivative of the objective function\n",
    "4. Implementing RMSprop algorithm\n",
    "5. Plotting the objective function\n",
    "6. Optimizing the RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zn1YvKHUCcfw"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- Import NumPy packages\n",
    "- Import the Python package matplotlib, which sets the padding between and around the subplots as well as the figure size\n",
    "- Import all other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljPieeyaBy2t"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from numpy import arange\n",
    "from numpy import meshgrid\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKqW4ZB6CmQV"
   },
   "source": [
    "### Step 2: Defining the Objective Function\n",
    "- The objective function is a mathematical function that takes two variables, x and y, as inputs.\n",
    "- It returns the sum of the squares of x and y, representing a measure of the quality or fitness of a particular solution in an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFRrj_5cBy2t"
   },
   "outputs": [],
   "source": [
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv2e43_WC4yL"
   },
   "source": [
    "### Step 3: Defining the Derivative of the Objective Function\n",
    "- The derivative of the objective function is defined as an array containing the partial derivatives with respect to x and y, calculated as **[x * 2.0, y * 2.0]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbtJWG01By2t"
   },
   "outputs": [],
   "source": [
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfQpQ_gVEXQC"
   },
   "source": [
    "### Step 4: Implementing RMSprop Algorithm\n",
    " - The RMSProp algorithm is a variant of the gradient descent optimization algorithm that adjusts the step size for each variable based on the average of the squared gradients.\n",
    "\n",
    "- In each iteration, RMSProp calculates the gradient of the objective function, updates the average of the squared partial derivatives, adjusts the step size for each variable, and updates the solution accordingly. The process is repeated for a specified number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoOOdS5yBy2u"
   },
   "outputs": [],
   "source": [
    "def rmsprop(objective, derivative, bounds, n_iter, step_size, rho):\n",
    "\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    sq_grad_avg = [0.0 for _ in range(bounds.shape[0])]\n",
    "    for it in range(n_iter):\n",
    "        gradient = derivative(solution[0], solution[1])\n",
    "        for i in range(gradient.shape[0]):\n",
    "            sg = gradient[i]**2.0\n",
    "            sq_grad_avg[i] = (sq_grad_avg[i] * rho) + (sg * (1.0-rho))\n",
    "        new_solution = list()\n",
    "        for i in range(solution.shape[0]):\n",
    "            alpha = step_size / (1e-8 + sqrt(sq_grad_avg[i]))\n",
    "            value = solution[i] - alpha * gradient[i]\n",
    "            new_solution.append(value)\n",
    "        solution = asarray(new_solution)\n",
    "        solution_eval = objective(solution[0], solution[1])\n",
    "        print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n",
    "    return [solution, solution_eval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INTuMqQ2wVCp"
   },
   "source": [
    "### Step 5: Plotting the Objective Function\n",
    "- It defines the bounds for the x and y axes, then creates a meshgrid x, y using meshgrid. This meshgrid represents a grid of values covering the specified range ([-1.0, 1.0] for both x and y), with a step size of 0.1.\n",
    "- It generates a filled contour plot to visualize the variations of the objective function across a specified range of x and y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "PM2A-umGBy2t",
    "outputId": "7243cefc-8591-442b-d39c-e848a187f2b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "\n",
    "xaxis = arange(bounds[0,0], bounds[0,1], 0.1)\n",
    "yaxis = arange(bounds[1,0], bounds[1,1], 0.1)\n",
    "\n",
    "x, y = meshgrid(xaxis, yaxis)\n",
    "\n",
    "results = objective(x, y)\n",
    "\n",
    "pyplot.contourf(x, y, results, levels=50, cmap='jet')\n",
    "\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gbCKFntD6KY"
   },
   "source": [
    "**Observation**\n",
    "- The contour plot shows a symmetric bowl-shaped pattern, indicating that the objective function has a minimum point at or near the origin (x = 0, y = 0).\n",
    "- The color intensity in the plot represents the function's values, with darker regions indicating lower values and lighter regions indicating higher values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTwz9dRaBy2u"
   },
   "source": [
    "### Step 6: Optimizing the RMSprop\n",
    "\n",
    "- Calculate the square of the partial derivative and update the decaying average of the squared partial derivatives with the **rho** hyperparameter.\n",
    "- Use the moving average of the squared partial derivatives and gradient to calculate the step size for the next point.\n",
    "- We will do this one variable at a time, first calculating the step size for the variable and then the new value for the variable.\n",
    "- These values are built up in an array until we have a completely new solution that is in the steepest descent direction from the current point, using the custom step sizes.\n",
    "- This new solution can then be evaluated using the objective() function, and the performance of the search can be reported.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaLpoxRSBy2v",
    "outputId": "5a0710d4-ab84-4737-88cb-cfe6a30daf63"
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0,1.0]])\n",
    "n_iter = 50\n",
    "step_size = 0.01\n",
    "rho = .99\n",
    "\n",
    "best, score = rmsprop(objective, derivative, bounds, n_iter, step_size, rho)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best,score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-12xYqh8Gi82"
   },
   "source": [
    "**Observation:**\n",
    "- The algorithm successfully performs gradient descent using RMSProp with the given parameters, displaying the progress at each iteration. The final result provides the best solution and its corresponding objective function value."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
