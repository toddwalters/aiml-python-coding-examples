{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqOI9HE0UsVU"
   },
   "source": [
    "# __Adam Optimizer__\n",
    "- Adam is an optimization algorithm that combines the benefits of AdaGrad and RMSProp.\n",
    "- It adapts the learning rates based on both the average first moment (mean) and the average second moment (uncentered variance) of the gradients.\n",
    "- It uses exponential moving averages of the gradients and squared gradients, controlled by parameters beta1 and beta2, to update the model parameters effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPDXWT36edVb"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing the required libraries\n",
    "2. Defining the objective function\n",
    "3. Implementing Adam algorithm\n",
    "4. Plotting the objective function\n",
    "5. Optimizing the Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4O6eOJsbUsVY"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- Import NumPy packages\n",
    "- Import the Python package matplotlib, which sets the padding between and around the subplots as well as the figure size\n",
    "- Import all other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPZrcxRuUsVa"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from numpy import arange\n",
    "from numpy import meshgrid\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg-hozW4VvIC"
   },
   "source": [
    "### Step 2: Defining the Objective Function\n",
    "- The objective function is a mathematical function that takes two variables, x and y, as inputs.\n",
    "- It returns the sum of the squares of x and y, representing a measure of the quality or fitness of a particular solution in an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anpKvTf9UsVb"
   },
   "outputs": [],
   "source": [
    "def objective (x,y) :\n",
    "    return x** 2.0 + y**2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyfCVmGeUsVe"
   },
   "source": [
    "### How Does Adam Algorithm Work?\n",
    "- Select a random point in the bounds of the problem as a starting point for the search\n",
    "- Generate an initial point\n",
    "  - x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "  - score = objective(x[0], x[1])\n",
    "- Initialize first and second moments\n",
    "  - m = [0.0 for _ in range(bounds.shape[0])]\n",
    "  - v = [0.0 for _ in range(bounds.shape[0])]\n",
    "- Run  fixed number of iterations of gradient descent\n",
    "- Calculate gradient\n",
    "   - gradient = derivative(solution[0], solution[1])\n",
    "- Calculate gradient g(t)\n",
    "- Derivative of the current set of parameters\n",
    "   - g(t) = derivative(x[0], x[1])\n",
    "- First moment\n",
    "  - m(t) = beta1 * m(t-1) + (1 - beta1) * g(t)\n",
    "  - m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
    "- Second moment\n",
    "   - v(t) = beta2 * v(t-1) + (1 - beta2) * g(t)^2\n",
    "   - v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
    "- Bias correction\n",
    "  - mhat(t) = m(t) / (1 - beta1(t))\n",
    "  - mhat = m[i] / (1.0 - beta1**(t+1))\n",
    "  - vhat(t) = v(t) / (1 - beta2(t))\n",
    "  - vhat = v[i] / (1.0 - beta2**(t+1))\n",
    "- The updated variable\n",
    "  - x(t) = x(t-1) - alpha * mhat(t) / (sqrt(vhat(t)) + eps)\n",
    "  - x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\n",
    "- This is repeated for each parameter that is being optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxZTdlbcbmTZ"
   },
   "source": [
    "### Step 3: Implementing Adam Algorithm\n",
    " - The code defines a gradient descent algorithm with the Adam optimization method.\n",
    " - It takes an objective function, derivative function, bounds of the variables, number of iterations, learning rate (alpha), and Adam parameters (beta1 and beta2) as inputs.\n",
    " - The algorithm iteratively updates the solution by calculating gradients, updating first and second moments, and adjusting the solution using the Adam update equation.\n",
    " - The progress is reported at each iteration, and the final solution along with its corresponding score is returned.\n",
    " - This code implements gradient descent with Adam optimization for finding the best solution to the provided objective function within the given bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4ilgTr4UsVf"
   },
   "outputs": [],
   "source": [
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])\n",
    "\n",
    "def adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2, eps=1e-8):\n",
    "\n",
    "    x = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    score = objective(x[0], x[1])\n",
    "\n",
    "    m = [0.0 for _ in range(bounds.shape[0])]\n",
    "    v = [0.0 for _ in range(bounds.shape[0])]\n",
    "\n",
    "    for t in range(n_iter):\n",
    "\n",
    "        g = derivative(x[0], x[1])\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "\n",
    "            m[i] = beta1 * m[i] + (1.0 - beta1) * g[i]\n",
    "            v[i] = beta2 * v[i] + (1.0 - beta2) * g[i]**2\n",
    "            mhat = m[i] / (1.0 - beta1**(t+1))\n",
    "            vhat = v[i] / (1.0 - beta2**(t+1))\n",
    "            x[i] = x[i] - alpha * mhat / (sqrt(vhat) + eps)\n",
    "        score = objective(x[0], x[1])\n",
    "        print('>%d f(%s) = %.5f' % (t, x, score))\n",
    "    return [x, score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sCxUG9rV2b3"
   },
   "source": [
    "### Step 4: Plotting the Objective Function\n",
    "\n",
    "- The code defines the range of input values using the \"bounds\" array and creates two arrays, \"x-axis\" and \"y-axis,\" with uniformly spaced values at 0.1 increments within the defined range.\n",
    "- It then generates a mesh grid of coordinates using the \"x-axis\" and \"y-axis\" arrays, computes the objective function values for each coordinate, and creates a filled contour plot with 50 levels using the \"contour\" function in Matplotlib with the jet color scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "qEY8hRByUsVb",
    "outputId": "c88172f7-0302-4b6c-b438-8fb835341914"
   },
   "outputs": [],
   "source": [
    "bounds = asarray([[-1.0, 1.0],[-1.0, 1.0]])\n",
    "\n",
    "xaxis = arange(bounds[0,0], bounds[0,1], 0.1)\n",
    "yaxis = arange(bounds[1,0], bounds[1,1], 0.1)\n",
    "\n",
    "x, y = meshgrid (xaxis, yaxis)\n",
    "\n",
    "results = objective(x,y)\n",
    "\n",
    "pyplot.contour(x, y, results, levels = 50,cmap = 'jet')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oony8J8MYjxe"
   },
   "source": [
    "**Observation:**\n",
    "- The code generates a contour plot of the objective function using the given input range.\n",
    "- The contour plot provides a visual representation of the function's values and their variations across the input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL3YFP5JUsVe"
   },
   "source": [
    "Note: The initial value of the moving averages and beta1 and beta2 values close to 1.0 (recommended) result in a bias of moment estimates toward zero.\n",
    "This bias is overcome by first calculating the biased estimate before calculating the bias-corrected estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaE4q0cbOAva"
   },
   "source": [
    "## Step 5: Optimizing the Adam\n",
    "- Set the random seed to ensure reproducibility\n",
    "- Define the bounds for the input variables\n",
    "- Specify the number of iterations\n",
    "- Set the value of alpha, beta1, and beta2 for the Adam algorithm\n",
    "- Apply the Adam algorithm to find the best solution and its corresponding score\n",
    "- Print messages indicating the completion of the optimization process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ti1lEsHTUsVf",
    "outputId": "1afdd43a-a55f-47db-a7d5-9c162728dc4d"
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "n_iter = 60\n",
    "alpha = 0.02\n",
    "beta1 = 0.8\n",
    "beta2 = 0.999\n",
    "\n",
    "best, score = adam(objective, derivative, bounds, n_iter, alpha, beta1, beta2)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBwrSWITdGWI"
   },
   "source": [
    "**Observation:**\n",
    "- The observation is that the optimization process using the Adam algorithm successfully converges to a minimal value of the objective function, reaching an optimal solution within the specified bounds."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
