{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1IiUuIxAAL7"
   },
   "source": [
    "# __Adadelta__\n",
    "- A limitation of gradient descent is that it uses the same step size (learning rate) for each input variable.\n",
    "- AdaGrad and RMSProp are extensions to gradient descent that add a self-adaptive learning rate for each parameter for the objective function.\n",
    "\n",
    "- Adadelta extends AdaGrad and RMSProp by introducing consistent unit calculations for the step size, eliminating the need for an initial learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulPQTQEREVWd"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing the required libraries\n",
    "2. Defining and plotting the objective function\n",
    "3. Implementing the Adadelta algorithm\n",
    "4. Optimizing the Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0-vzs4_AAL8"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- Import NumPy packages\n",
    "- Import the Python package matplotlib, which sets the padding between and around the subplots as well as the figure size\n",
    "- Import all other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcRsKOpVAAL9"
   },
   "outputs": [],
   "source": [
    "from numpy import arange\n",
    "from numpy import meshgrid\n",
    "from matplotlib import pyplot\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQFSXRRSJIvb"
   },
   "source": [
    "### Step 2: Defining and Plotting the Objective Function\n",
    "- Define the objective function as the sum of squares of x and y\n",
    "- Set the bounds for input variables\n",
    "- Generate arrays of x and y values within the specified bounds at 0.1 increments\n",
    "- Create a mesh grid from the x and y arrays\n",
    "- Compute the objective function values for each combination of x and y in the mesh grid\n",
    "- Create a filled contour plot with 50 contour levels and 'jet' color scheme\n",
    "- Display the plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "LPGtKNGgAAL_",
    "outputId": "b08f3b23-4ba0-47dc-cc6f-8c633af37380"
   },
   "outputs": [],
   "source": [
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0\n",
    "\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "\n",
    "xaxis = arange(bounds[0,0], bounds[0,1], 0.1)\n",
    "yaxis = arange(bounds[1,0], bounds[1,1], 0.1)\n",
    "\n",
    "x, y = meshgrid(xaxis, yaxis)\n",
    "\n",
    "results = objective(x, y)\n",
    "\n",
    "pyplot.contourf(x, y, results, levels=50, cmap='jet')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5wkv9NkAAMA"
   },
   "source": [
    "Observation:\n",
    "- The observation for the given code is that the contour plot shows a symmetric bowl-shaped pattern, indicating that the objective function has a minimum point at or near the origin (x = 0, y = 0).\n",
    "- The color intensity in the plot represents the function's values, with darker regions indicating lower values and lighter regions indicating higher values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xh_epzy8A5bi"
   },
   "source": [
    "### Step 3: Implementing Adadelta Algorithm\n",
    "\n",
    "- Initialization and Setup:\n",
    "\n",
    "  - **solution**: Initializes the solution with random values within the given bounds.\n",
    " - **sq_grad_avg**: Keeps track of the moving average of the squared gradients.\n",
    " - **sq_para_avg**: Keeps track of the moving average of the squared parameter updates.\n",
    " - **n_iter**, **rho**, and **ep** are hyperparameters controlling the number of iterations, decay factor, and a small constant to prevent division by zero, respectively.\n",
    "\n",
    "\n",
    "- Gradient Computation and Update:\n",
    "\n",
    " - In each iteration, the gradient of the objective function with respect to the current solution is computed.\n",
    " - For each component of the gradient, the moving average of the squared gradient is updated with the decay factor rho.\n",
    " - Then, a new solution is computed based on the ratio of the square root of the moving averages of the squared parameter updates and the squared gradients. The current gradient is also used to calculate the change in parameters.\n",
    "\n",
    "- Build a new solution\n",
    " - Iteratively update each variable in the solution by considering the gradient and step size specific to that variable.\n",
    " - Calculate the new position of each variable by subtracting the product of the step size and gradient from the current value and store the updated variables in a new solution\n",
    "\n",
    "- Evaluate the solution\n",
    "  - Evaluate the objective function for the candidate point defined by the new solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuxSY1UiAAMC"
   },
   "outputs": [],
   "source": [
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])\n",
    "\n",
    "def adadelta(objective, derivative, bounds, n_iter, rho, ep=1e-3):\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\n",
    "    sq_grad_avg = [0.0 for _ in range(bounds.shape[0])]\n",
    "\n",
    "    sq_para_avg = [0.0 for _ in range(bounds.shape[0])]\n",
    "\n",
    "    for it in range(n_iter):\n",
    "\n",
    "        gradient = derivative(solution[0], solution[1])\n",
    "\n",
    "        for i in range(gradient.shape[0]):\n",
    "\n",
    "            sg = gradient[i]**2.0\n",
    "\n",
    "            sq_grad_avg[i] = (sq_grad_avg[i] * rho) + (sg * (1.0-rho))\n",
    "\n",
    "        new_solution = list()\n",
    "        for i in range(solution.shape[0]):\n",
    "\n",
    "            alpha = (ep + sqrt(sq_para_avg[i])) / (ep + sqrt(sq_grad_avg[i]))\n",
    "            change = alpha * gradient[i]\n",
    "            sq_para_avg[i] = (sq_para_avg[i] * rho) + (change**2.0 * (1.0-rho))\n",
    "            value = solution[i] - change\n",
    "\n",
    "            new_solution.append(value)\n",
    "\n",
    "        solution = asarray(new_solution)\n",
    "        solution_eval = objective(solution[0], solution[1])\n",
    "\n",
    "        print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n",
    "    return [solution, solution_eval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl0CXpRIbAnv"
   },
   "source": [
    "## Step 4: Optimizing the Adadelta\n",
    "- Set the random seed to ensure reproducibility\n",
    "- Define the bounds for the input variables\n",
    "- Specify the number of iterations\n",
    "- Set the value of rho for the Adadelta algorithm\n",
    "- Apply the Adadelta algorithm to find the best solution and its corresponding score\n",
    "- Print messages indicating the completion of the optimization process\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxbS0HDOAAMC",
    "outputId": "0918ba2d-d403-4343-d6bf-ece8ab620bc0"
   },
   "outputs": [],
   "source": [
    "seed(1)\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "n_iter = 120\n",
    "rho = 0.99\n",
    "\n",
    "best, score = adadelta(objective, derivative, bounds, n_iter, rho)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6-sHKlyBZHD"
   },
   "source": [
    "**Observation:**\n",
    "- The Adadelta algorithm is applied with 120 iterations, a momentum value of 0.99, and seed 1, resulting in a convergence to the best solution and its corresponding score."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
