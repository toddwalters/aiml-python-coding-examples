{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXtVvLabkN3P",
    "tags": []
   },
   "source": [
    "# __Gradient Descent with AdaGrad__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCV46fWskN3S"
   },
   "source": [
    "Problem with the gradient descent algorithm:\n",
    "\n",
    "- It means that the step size (learning rate) is the same for each variable or dimension in the search space.\n",
    "- Better performance can be achieved using a step size that is tailored to each variable, allowing larger movements in dimensions with a consistently steep gradient and smaller movements in dimensions with less steep gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0lP3VQYkN3S"
   },
   "source": [
    "AdaGrad\n",
    "\n",
    "- It is an extension of the gradient descent optimization algorithm.\n",
    "- It is designed to accelerate the optimization process.\n",
    "- It is designed to specifically explore the idea of automatically tailoring the step size for each dimension in the search space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NeRcE8koOvL"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "\n",
    "1. Importing the required libraries\n",
    "2. Defining the objective function\n",
    "3. Defining the AdaGrad algorithm\n",
    "4. Plotting the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ol0zBVUJm17Y"
   },
   "source": [
    "###Step 1: Importing the Required Libraries\n",
    "\n",
    "- Import NumPy packages\n",
    "- Import the Python package matplotlib, which sets the padding between and around the subplots as well as the figure size\n",
    "- Import all other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlkO7UBXkN3T"
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from numpy import arange\n",
    "from numpy import meshgrid\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7lTuJotm-Nz"
   },
   "source": [
    "### Step 2: Defining the Objective Function\n",
    "- The objective function is a mathematical function that takes two variables, x and y, as inputs\n",
    "- It returns the sum of the squares of x and y, representing a measure of the quality or fitness of a particular solution in an optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohfA_XTYtXK5"
   },
   "outputs": [],
   "source": [
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEbqpLx4qKx6"
   },
   "source": [
    "### Step 3: Defining the AdaGrad Algorithm\n",
    "- AdaGrad is an optimization algorithm that adjusts the step size for each variable, based on the sum of the squared gradients.\n",
    "- It aims to accelerate the convergence of the gradient descent algorithm by adaptively scaling the step size for each variable.\n",
    "- AdaGrad keeps track of the sum of the squared partial derivatives for each variable, and the step size is inversely proportional to the square root of this sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bhbv2dPDkN3V",
    "outputId": "ff4364ef-ed24-46a0-a3e7-c8b23bb19872"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# derivative of the objective function\n",
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])\n",
    "\n",
    "# gradient descent algorithm with AdaGrad\n",
    "\n",
    "def adagrad(objective, derivative, bounds, n_iter, step_size):\n",
    "    # generate an initial point\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    # list of the sum square gradients for each variable\n",
    "    sq_grad_sums = [0.0 for _ in range(bounds.shape[0])]\n",
    "    # run the gradient descent\n",
    "    for it in range(n_iter):\n",
    "        # calculate gradient\n",
    "        gradient = derivative(solution[0], solution[1])\n",
    "        # update the sum of the squared partial derivatives\n",
    "        for i in range(gradient.shape[0]):\n",
    "            sq_grad_sums[i] += gradient[i]**2.0\n",
    "        # build a solution one variable at a time\n",
    "        new_solution = list()\n",
    "        for i in range(solution.shape[0]):\n",
    "            # calculate the step size for this variable\n",
    "            alpha = step_size / (1e-8 + sqrt(sq_grad_sums[i]))\n",
    "            # calculate the new position in this variable\n",
    "            value = solution[i] - alpha * gradient[i]\n",
    "            # store this variable\n",
    "            new_solution.append(value)\n",
    "        # evaluate the candidate point\n",
    "        solution = asarray(new_solution)\n",
    "        solution_eval = objective(solution[0], solution[1])\n",
    "        # report progress\n",
    "        print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n",
    "    return [solution, solution_eval]\n",
    "\n",
    "# seed the pseudo-random number generator\n",
    "seed(1)\n",
    "# define the range for input\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 50\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# perform the gradient descent search with AdaGrad\n",
    "best, score = adagrad(objective, derivative, bounds, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lLbUDfqqv1a"
   },
   "source": [
    "**Obseravtion:**\n",
    "- AdaGrad optimizes the objective function iteratively and displays the corresponding f value during each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScQ0fhLInlgP"
   },
   "source": [
    "### Step 4: Plotting the Objective Function\n",
    "- It defines the bounds for the x and y axes, then creates a meshgrid x, y using meshgrid. This meshgrid represents a grid of values covering the specified range ([-1.0, 1.0] for both x and y), with a step size of 0.1.\n",
    "- It generates a filled contour plot to visualize the variations of the objective function across a specified range of x and y values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "c8FCcmb-kN3U",
    "outputId": "a8f64bc6-6138-473f-bb71-672a3b76c469"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "\n",
    "xaxis = arange(bounds[0,0], bounds[0,1], 0.1)\n",
    "yaxis = arange(bounds[1,0], bounds[1,1], 0.1)\n",
    "\n",
    "x, y = meshgrid(xaxis, yaxis)\n",
    "\n",
    "results = objective(x, y)\n",
    "\n",
    "pyplot.contourf(x, y, results, levels=50, cmap='jet')\n",
    "\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T25MuRC4oL96"
   },
   "source": [
    "**Observation**\n",
    "- The observation for the given code is that the contour plot shows a symmetric bowl-shaped pattern, indicating that the objective function has a minimum point at or near the origin (x = 0, y = 0).\n",
    "- The color intensity in the plot represents the function's values, with darker regions indicating lower values and lighter regions indicating higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
