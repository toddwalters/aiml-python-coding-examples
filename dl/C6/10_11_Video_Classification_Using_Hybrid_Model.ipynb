{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/C6/10_11_Video_Classification_Using_Hybrid_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4CNzLHQWbPg"
   },
   "source": [
    "#__Video Classification Using Hybrid Model__\n",
    "Let's see how to classify the video using transfer learning and a recurrent model on the UCF101 dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktbWX3ddWxa9"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Downloading data and importing the required libraries\n",
    "2. Reading the data from datasets and printing the ten rows\n",
    "3. Defining the functions for cropping and loading video frames\n",
    "4. Building a feature extraction model using InceptionV3 architecture\n",
    "5. Creating a string lookup table for labels and printing the vocabulary of the label processor\n",
    "6. Preparing video data for training and testing by extracting frame features\n",
    "7. Defining and training a sequence model using GRU layers\n",
    "8. Loading a test video, extracting frame features, and making predictions using the sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvjlPz3dW5Bs"
   },
   "source": [
    "### Step 1: Downloading Data and Importing the Required Libraries\n",
    "- Download the dataset\n",
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t8MHucxPGBU"
   },
   "outputs": [],
   "source": [
    "!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101.rar\n",
    "!wget -q --no-check-certificate https://www.crcv.ucf.edu/data/UCF101/UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FufAi4ZnPJ7S"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!unrar e UCF101.rar data/\n",
    "!unzip -qq UCF101TrainTestSplits-RecognitionTask.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DT8IxSDPJ-U"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdj2UXjiFr0h"
   },
   "source": [
    "Open the __.txt__ file which has the names of the training videos\n",
    "\n",
    "Create a dataframe having video names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "BoFiu5HpPKBt",
    "outputId": "aebcf49b-1878-4b05-f2c5-624442f267e1"
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"ucfTrainTestlist/trainlist01.txt\", \"r\")\n",
    "temp = f.read()\n",
    "videos = temp.split('\\n')\n",
    "\n",
    "train = pd.DataFrame()\n",
    "train['video_name'] = videos\n",
    "train = train[:-1]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F6_vxikF3zd"
   },
   "source": [
    "Open the __.txt__ file which has the names of the test videos\n",
    "\n",
    "Create a DataFrame having video names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fGH6nO23PKFO",
    "outputId": "1ca5dea6-6067-476c-c233-55852ebb0860"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"ucfTrainTestlist/testlist01.txt\", \"r\") as f:\n",
    "    temp = f.read()\n",
    "videos = temp.split(\"\\n\")\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test[\"video_name\"] = videos\n",
    "test = test[:-1]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwJeWtfLHQs-"
   },
   "source": [
    "- Define the __extract_tag__ function that extracts a tag from the video path. This is done by splitting the video path by or and returning the first part.\n",
    "- Define the __separate_video_name function, which separates the video name from the video path. This is achieved by splitting the video name by / and returning the second part.\n",
    "- Define the __rectify_video_name__ function to rectify the video name by splitting the video name by \" \" and returning the first part.\n",
    "- Define the __move_videos__ function:\n",
    "   - Check if the output directory exists. If not, create the directory using __os.mkdir__.\n",
    "   - Iterate over the DataFrame, __df__, using a progress bar from the __tqdm__ library.\n",
    "   - For each row in the DataFrame, extract the video file name from the __video_name__ column, create its path, and then copy the video file to the output directory using __shutil.copy2__.\n",
    "   - After the loop ends, print the total number of videos in the output directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yi9ooC9XPfYe"
   },
   "outputs": [],
   "source": [
    "def extract_tag(video_path):\n",
    "    return video_path.split(\"/\")[0]\n",
    "\n",
    "def separate_video_name(video_name):\n",
    "    return video_name.split(\"/\")[1]\n",
    "\n",
    "def rectify_video_name(video_name):\n",
    "    return video_name.split(\" \")[0]\n",
    "\n",
    "def move_videos(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.mkdir(output_dir)\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        videoFile = df['video_name'][i].split(\"/\")[-1]\n",
    "        videoPath = os.path.join(\"data\", videoFile)\n",
    "        shutil.copy2(videoPath, output_dir)\n",
    "    print()\n",
    "    print(f\"Total videos: {len(os.listdir(output_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55J4781CXO9b"
   },
   "source": [
    "### Step 2: Reading the Data from Datasets and Printing the Ten Rows\n",
    "\n",
    "- Define the values of **IMG_SIZE**, **BATCH_SIZE**, **EPOCHS**, **MAX_SEQ_LENGTH**, and **NUM_FEATURES**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwjxIY0xPlw7"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUlUP1OOXWc4"
   },
   "source": [
    "### Step 3: Defining the Functions for Cropping and Loading Video Frames\n",
    "- Define a function named **crop_center_square** that crops a frame to a square shape by determining the minimum dimension and calculating the starting coordinates\n",
    "- Define a function named __load_video__ that loads a video file, crops each frame to a square shape, resizes it, and converts the color channels.\n",
    "- Open the video file using **cv2.VideoCapture** and initialize an empty list called frames\n",
    "- Read frames from the video, crop them to a square shape, resize them, convert the color channels, and append them to the frames list\n",
    "- If the maximum number of frames is reached or the video ends, exit the loop.\n",
    "- Release the video capture.\n",
    "- Convert the frame list to a NumPy array\n",
    "- Return the array of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HzfusTJPpPV"
   },
   "outputs": [],
   "source": [
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j2lXMm4Xd8V"
   },
   "source": [
    "__Observation:__\n",
    "- The code defines two functions, **crop_center_square** and __load_video__, which can be used to crop frames from videos and load videos as arrays of frames, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14OJjHbUXjrh"
   },
   "source": [
    "### Step 4: Building a Feature Extraction Model Using InceptionV3 Architecture\n",
    "\n",
    "- Create a feature extractor using the InceptionV3 model from keras.applications with specific configurations.\n",
    "- Assign the preprocess_input function from **keras.applications.inception_v3** to the v**ariable preprocess_input**.\n",
    "- Create an input layer with the shape __(IMG_SIZE, IMG_SIZE, 3)__ using **keras.Input**.\n",
    "- Preprocess the input using the **preprocess_input function**.\n",
    "- Pass the preprocessed input through the feature extractor to obtain the outputs.\n",
    "- Create a model with the inputs and outputs using **keras.Model** and assign it to the variable **feature_extractor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rjl2yoPmPtoW",
    "outputId": "83552d83-dbf2-4994-f2cb-09e0839863de"
   },
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rN_SBAOYG8B"
   },
   "source": [
    "__Observations:__\n",
    "- The code defines a function **build_feature_extractor** that creates a feature extractor model using InceptionV3 architecture.\n",
    "- The model takes inputs of size __(IMG_SIZE, IMG_SIZE, 3)__, preprocesses the inputs, and produces the outputs.\n",
    "- The created feature extractor model is assigned to the variable **feature_extractor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XMkKyQQ6QCMV",
    "outputId": "4adba7a4-7502-45c7-ec12-1085e8f9ce3a"
   },
   "outputs": [],
   "source": [
    "train[\"tag\"] = train[\"video_name\"].apply(extract_tag)\n",
    "train[\"video_name\"] = train[\"video_name\"].apply(separate_video_name)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ialV1htPQGlg",
    "outputId": "d0094838-f820-4790-95a5-9cb7c3797e53"
   },
   "outputs": [],
   "source": [
    "train[\"video_name\"] = train[\"video_name\"].apply(rectify_video_name)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a07ssIhrQL_H",
    "outputId": "538c0579-1d13-4171-bc59-3cda6f7c0fde"
   },
   "outputs": [],
   "source": [
    "test[\"tag\"] = test[\"video_name\"].apply(extract_tag)\n",
    "test[\"video_name\"] = test[\"video_name\"].apply(separate_video_name)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "CycyhCCB2HTh",
    "outputId": "9e614f98-9836-4895-ac95-e0b1463be460"
   },
   "outputs": [],
   "source": [
    "train[\"tag\"].value_counts().nlargest(n).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6zxubqKQQav",
    "outputId": "c46d9c4f-75fb-4551-a95f-e284dcfd7483"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "topNActs = train[\"tag\"].value_counts().nlargest(n).reset_index()[\"tag\"].tolist()\n",
    "train_new = train[train[\"tag\"].isin(topNActs)]\n",
    "test_new = test[test[\"tag\"].isin(topNActs)]\n",
    "train_new.shape, test_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Q2oQQC5KQZc"
   },
   "source": [
    "**Observation:**\n",
    "- The output **((1171, 2), (459, 2))** is a tuple showing the shapes of **train_new** and **test_new**. The **train_new** DataFrame has 1171 rows and 2 columns, and the **test_new** DataFrame has 459 rows and 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBPvTP0RQUGb"
   },
   "outputs": [],
   "source": [
    "train_new = train_new.reset_index(drop=True)\n",
    "test_new = test_new.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "C1xkFe562Sra",
    "outputId": "c48e65d0-ff18-4249-8377-3867bf3c5be8"
   },
   "outputs": [],
   "source": [
    "train_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNw7O0LdXzFV"
   },
   "source": [
    "### Step 5: Creating a String Lookup Table for Labels and Prints the Vocabulary of the Label Processor\n",
    "- Create a label processor using **keras.layers.StringLookup**\n",
    "- Set the number of out-of-vocabulary (OOV) indices to **0**\n",
    "- Set the vocabulary of the label processor to the unique values from the **tag** column of the **train_df** DataFrame\n",
    "- Retrieve the vocabulary of the label processor using **label_processor.get_vocabulary()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHg5adMbTlyr",
    "outputId": "5bcbf156-839b-48fa-a21f-0f88571b340e"
   },
   "outputs": [],
   "source": [
    "label_processor = keras.layers.StringLookup(\n",
    "    num_oov_indices=0, vocabulary=np.unique(train[\"tag\"])\n",
    ")\n",
    "print(label_processor.get_vocabulary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF_te3XFYQku"
   },
   "source": [
    "__Observations:__\n",
    "- The code creates a label processor that maps labels from text to integer indices.\n",
    "- It uses the unique values from the **tag** column of the **train_df** DataFrame as the vocabulary for the label processor.\n",
    "- The output is the vocabulary of the label processor, which is a list of unique labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsh5ULu7YU36"
   },
   "source": [
    "### Step 6: Preparing Video Data for Training and Testing by Extracting Frame Features\n",
    "- Define a function named **prepare_all_videos** that inputs a DataFrame (df) and a root directory **(root_dir)**.\n",
    "- Retrieve the video paths and labels from the DataFrame and encode the labels using **label_processor**.\n",
    "- Initialize arrays to store frame masks and frame features for each video.\n",
    "- Iterate over each video in the dataset, load the frames, and extract features using the **feature_extractor** model.\n",
    "- Update the arrays with the extracted features and masks for each video.\n",
    "- Call the **prepare_all_videos** function on the train and test DataFrames, storing the returned values in **train_data**, **train_labels**, **test_data**, and __test_labels__.\n",
    "- Finally, print the shape of the frame features in the train set and the shape of the frame masks in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARvHX2VATt36",
    "outputId": "66de96fc-cdc3-4e9a-d94f-43d6e233412d"
   },
   "outputs": [],
   "source": [
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
    "    frame_features = np.zeros(\n",
    "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohCZ1_qtYa8e"
   },
   "source": [
    "__Observations:__\n",
    "- The code processes videos by extracting frame features and creating frame masks.\n",
    "- It then returns the frame features, frame masks, and labels for the train and test sets.\n",
    "- The output is the shape of the frame features in the train set and the shape of the frame masks in the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztRcM2knYgPA"
   },
   "source": [
    "### Step 7: Defining and Training a Sequence Model Using GRU Layers\n",
    "- Define a function named **get_sequence_model** that creates a sequence model for video classification\n",
    "- Create input layers for frame features and masks\n",
    "- Apply two GRU layers to the frame features input, with the second GRU layer returning only the last output\n",
    "- Add a dropout layer, a dense layer with ReLU activation, and a final dense layer with softmax activation for the output\n",
    "- Compile the model with sparse categorical cross-entropy loss, Adam optimizer, and accuracy metric\n",
    "- Define a function named __run_experiment__ for running the training and evaluation\n",
    "- Set up a checkpoint to save the best model during training\n",
    "- Create the sequence model using **get_sequence_model**\n",
    "- Train the model on the training data with a validation split, specified number of epochs, and the checkpoint callback\n",
    "- Load the best weights saved during training\n",
    "- Evaluate the model on the test data and print the test accuracy\n",
    "- Return the history object and the trained sequence model\n",
    "- Call the **run_experiment** function and store the returned values in __(history)__ and **sequence_model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAgMUDWKT1Ba",
    "outputId": "e5c70b19-63e7-4ed6-d665-7a240b06d832"
   },
   "outputs": [],
   "source": [
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(\n",
    "        frame_features_input, mask=mask_input\n",
    "    )\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "\n",
    "def run_experiment():\n",
    "    filepath = \"/tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint],\n",
    "    )\n",
    "\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdIBd6N_Ym1z"
   },
   "source": [
    "__Observations:__\n",
    "- Training progress and validation metrics will be displayed during the model training process.\n",
    "- After training, the model will be evaluated on the test data, and the test accuracy will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZShMzB7Yyqp"
   },
   "source": [
    "### Step 8: Loading a Test Video, Extracting Frame Features, and Making Predictions Using the Sequence Model\n",
    "- Load a random test video path.\n",
    "- Call the **sequence_prediction** function with the test video path.\n",
    "- Within the **sequence_prediction** function:\n",
    "\n",
    "  a. Get the vocabulary of the classes.\n",
    "\n",
    "  b. Load the frames of the video.\n",
    "\n",
    "  c. Prepare the frames for sequence prediction by extracting frame features and creating a frame mask.\n",
    "\n",
    "  d. Use the trained sequence model to predict the probabilities of each class for the video.\n",
    "\n",
    "  e. Print the predicted class probabilities in descending order.\n",
    "\n",
    "- Assign the frames of the test video to the variable test_frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xezv1Nq3UDoz",
    "outputId": "ce64cede-4a95-4185-c13a-e2d39a8e1c99"
   },
   "outputs": [],
   "source": [
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "\n",
    "\n",
    "test_video = np.random.choice(test[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a01It9XpY46J"
   },
   "source": [
    "__Observations:__\n",
    "- The test video path will be printed.\n",
    "- The predicted class probabilities for the test video will be printed, showing the class label and the corresponding probability.\n",
    "- The frames of the test video will be assigned to the **test_frames** variable."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
