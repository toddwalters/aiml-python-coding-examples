{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/C6/10_06_02_Text_Classification_Using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMGXXgNF7k61"
   },
   "source": [
    "# __Text Classification using LSTM__\n",
    "\n",
    "Let's see how to classify the text using LSTM (Long Short-Term Memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RME2Qywv_p7Z",
    "outputId": "4847175e-619b-417e-87a6-3a8a4a830bde"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuJT1hcu92er"
   },
   "source": [
    "## Steps to  Be Followed:\n",
    "\n",
    "1. Importing the libraries\n",
    "2. Defining the hyperparameter\n",
    "3. Preprocessing the data and printing the lengths of the labels and articles lists.\n",
    "4. Splitting the data into training and validation sets\n",
    "5. Initializing a tokenizer and fitting it on the training articles\n",
    "6. Converting the training articles into sequences using the tokenizer\n",
    "7. Padding the sequence\n",
    "8. Printing the length of validation Sequences and the shape of validation padded\n",
    "9. Training the model\n",
    "10. Compiling the model\n",
    "11. Plotting the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKxl5ljN-IKH"
   },
   "source": [
    "### Step 1: Importing the libraries\n",
    "- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLg1HxV27k64"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cvQu2uW_AF0"
   },
   "source": [
    "### Step 2: Defining the Hyperparameter\n",
    "- Set the value of __vocab_size__ to __5000__, representing the size of the vocabulary\n",
    "- Set the value of __embedding_dim__ to __64__, specifying the dimensionality of the word embeddings\n",
    "- Set the value of __max_length__ to __200__, indicating the maximum length of input sequences\n",
    "- Set the value of __padding_type__ to __post__, specifying that padding should be added at the end of sequences\n",
    "- Set the value of __trunc_type__ to __post__, indicating that truncation should be applied at the end of sequences\n",
    "- Set the value of __oov_tok__ to __OOV__, representing the token to be used for out-of-vocabulary words\n",
    "- Set the value of __training_portion__ to __0.8__, representing the proportion of data to be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BecOwkf37k66"
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "embedding_dim = 64\n",
    "max_length = 200\n",
    "padding_type = 'post'\n",
    "trunc_type = 'post'\n",
    "oov_tok = '<OOV>'\n",
    "training_portion = .8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5682GVs7k66"
   },
   "source": [
    "### Step 3: Preprocessing the Data and Printing the Lengths of the Labels and Articles Lists.\n",
    "\n",
    "- Define two empty lists, articles, and labels to store the articles and labels, respectively\n",
    "- Read the contents of the **bbc-text.csv** file using csv.reader and iterate through each row\n",
    "- Extract the label from the first column of each row and append it to the labels list\n",
    "- Process the article from the second column by removing stopwords and replacing consecutive spaces with a single space and then append it to the articles list\n",
    "- Print the lengths of the labels and articles lists to display the number of labels and processed articles, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YOrfsrUe7k67",
    "outputId": "f8b8a843-4f70-4265-d6bb-2f85dc934c5e"
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "labels = []\n",
    "\n",
    "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace(' ', ' ')\n",
    "        articles.append(article)\n",
    "print(len(labels))\n",
    "print(len(articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtk-qqt67k67"
   },
   "source": [
    "__Observations:__\n",
    "- There are only **2,225** articles in the data.\n",
    "- Then, we split into a training set and validation set, according to the parameter we set earlier, 80% for training, and 20% for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C72gvCwGSa0k"
   },
   "source": [
    "### Step 4: Splitting the Data into Training and Validation Sets\n",
    "- Calculate the **train_size** by multiplying the length of the articles list with __training_portion__ and converting it to an integer\n",
    "- Create **train_articles** by slicing the articles list from index **0** to **train_size**\n",
    "- Create **train_labels** by slicing the labels list from index** 0 **to **train_size**\n",
    "- Create validation_articles by slicing the articles list from **train_size** onward\n",
    "- Create **validation_labels** by slicing the labels list from **train_size** onward\n",
    "- Print the **train_size** to display the calculated value\n",
    "\n",
    "- The lengths of **train_articles**, **train_labels**, **validation_articles**, and **validation_labels** represent the number of items in each list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKMT9Fb37k68",
    "outputId": "2bcf554b-3dee-4b5a-fff6-c8eb3dea3e12"
   },
   "outputs": [],
   "source": [
    "train_size = int(len(articles) * training_portion)\n",
    "\n",
    "train_articles = articles[0: train_size]\n",
    "train_labels = labels[0: train_size]\n",
    "\n",
    "validation_articles = articles[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "\n",
    "print(train_size)\n",
    "print(len(train_articles))\n",
    "print(len(train_labels))\n",
    "print(len(validation_articles))\n",
    "print(len(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx4Pb61WT2F3"
   },
   "source": [
    "__Observations:__\n",
    "- The value of **train_size** is calculated based on the proportion of training data.\n",
    "- The lengths of **train_articles**, **train_labels**, **validation_articles**, and **validation_labels** representing the number of items in each list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSR2G2lrWrue"
   },
   "source": [
    "### Step 5: Initializing a Tokenizer and Fitting It on the Training Articles\n",
    "\n",
    "- Initialize a **Tokenizer** object named tokenizer with the specified parameters: **num_words** representing the vocabulary size and **oov_token** representing the out-of-vocabulary token\n",
    "- Fit the tokenizer on the training articles **(train_articles)** using the **fit_on_texts** method\n",
    "- This step updates the tokenizer's internal word index based on the words in the training articles\n",
    "- Assign the word index obtained from the tokenizer to the variable **word_index**\n",
    "- Extract the first 10 items from the word_index dictionary\n",
    "- Print the resulting dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JrnZlTL7k68"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_articles)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7lhT9_-f7k68",
    "outputId": "e4e22179-762b-4136-886d-267b69f182a6"
   },
   "outputs": [],
   "source": [
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ_XTmT_eU7b"
   },
   "source": [
    "__Observations:__\n",
    "- The code prints a dictionary containing the first 10 items from the word_index dictionary.\n",
    "- These items represent a subset of the word-to-index mappings generated by the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTDip0vPe30D"
   },
   "source": [
    "### Step 6: Converting the Training Articles into Sequences Using the Tokenizer\n",
    "- Convert the training articles **(train_articles)** into sequences using the **texts_to_sequences** method of the tokenizer object and assign the result to **train_sequences**\n",
    "- Print the sequence representation of the 11th training article (index 10) by accessing **train_sequences[10]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaNkkgzP7k69",
    "outputId": "f2133525-533e-4ce9-81f8-1ad9192bdeb1"
   },
   "outputs": [],
   "source": [
    "train_sequences  = tokenizer.texts_to_sequences(train_articles)\n",
    "\n",
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__Jgr_cdfR_6"
   },
   "source": [
    "__Observation:__\n",
    "- The code prints the sequence representation of the 11th training article (index 10) in the **train_sequences** list.\n",
    "- The output is a list of integers, where each integer represents the index of a word in the tokenizer's word index vocabulary that corresponds to a word in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E60-EoRMh58g"
   },
   "source": [
    "### Step 7: Padding the Sequence\n",
    "- Pad the sequences in **train_sequences** using the **pad_sequences** function\n",
    "- Set the maximum length of the padded sequences to **max_length**\n",
    "- Specify the padding type as **padding_type** and the truncation type as **trunc_type**\n",
    "- Assign the padded sequences to the variable **train_padded**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msiF0l767k69"
   },
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKwGCkXf7k69",
    "outputId": "57e04b39-50e0-4db2-b4a4-39ff391a9228"
   },
   "outputs": [],
   "source": [
    "print(train_sequences[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2TGYE5IjhOI"
   },
   "source": [
    "__Observations:__\n",
    "- The code prints the sequence representation of the 11th training article (index 10) in the **train_sequences** list.\n",
    "- The output is a list of integers, where each integer represents the index of a word in the tokenizer's word index vocabulary that corresponds to a word in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQbLYMWl7k6-",
    "outputId": "30070a36-37cd-443a-a8a0-014026d342cb"
   },
   "outputs": [],
   "source": [
    "print(train_padded[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR1yorJLkMpI"
   },
   "source": [
    "__Observation:__\n",
    "- The code prints the padded sequence representation of the 11th training article.\n",
    "- The output is a list of integers representing the word indices of the corresponding words in the article, after applying padding to ensure a consistent length (max_length) for all sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QD6M00flHAm"
   },
   "source": [
    "### Step 8: Printing the Length of Validation Sequences and the Shape of Validation Padded\n",
    "- Convert the validation articles into sequences using the tokenizer and pad the sequences to a maximum length. Assign the result to **validation_padded**\n",
    "- Print the length of **validation_sequences** and the shape of **validation_padded**\n",
    "- Create a tokenizer for the labels and fit it on the labels list\n",
    "- Convert the training and validation labels into sequences using the label tokenizer and store the results in **training_label_seq** and **validation_label_seq** as NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpgc3k9y7k6-",
    "outputId": "ae2e898c-6b05-4c79-9e8d-7c8692927f62"
   },
   "outputs": [],
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(validation_articles)\n",
    "validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "print(len(validation_sequences))\n",
    "print(validation_padded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFFWDC12mo1e"
   },
   "source": [
    "__Observations:__\n",
    "- The length of **validation_sequences**, indicating the number of sequences in the validation set.\n",
    "- The shape of **validation_padded**, representing the dimensions of the padded validation sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Q3nLd8W7k6-",
    "outputId": "d8556e80-d97a-46f6-8ff4-ff30cefce9ad"
   },
   "outputs": [],
   "source": [
    "print(set(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0aPJkPCnkh3"
   },
   "source": [
    "__Observation:__\n",
    "- The output is a set containing the unique labels: 'business', 'tech', 'entertainment', 'politics', and 'sport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WcsrX7dP7k6-"
   },
   "outputs": [],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7VlgNsjpwKy"
   },
   "source": [
    "__Observations:__\n",
    "- The output of this code is the conversion of label sequences for the training and validation sets.\n",
    "- The **training_label_seq** and **validation_label_seq** are NumPy arrays containing the label sequences for the respective sets, based on the word index mapping generated by the **label_tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aefTdVG3ohC2"
   },
   "source": [
    "### Step 9: Training the Model\n",
    "- Create a sequential model using **tf.keras.Sequential()**\n",
    "- Add an embedding layer to the model with the specified vocabulary size **(vocab_size)** and embedding dimension **(embedding_dim)**\n",
    "- Add a bidirectional LSTM layer to the model with the same embedding dimension\n",
    "- Add a dense layer to the model with the embedding dimension as the number of units and **relu** activation function\n",
    "- Add a dense layer with 6 units and the **softmax** activation function to the model\n",
    "- Print a summary of the model's architecture using **model.summary()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2k0E1Cb7k6_",
    "outputId": "8264c4bd-d1c6-4778-8142-38daa8aaccaf"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RYZ5eRIqHSk"
   },
   "source": [
    "__Observation:__\n",
    "- The code outputs a summary of the model's architecture, including the number of parameters and the shape of each layer in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2es6RORPqq1M"
   },
   "source": [
    "### Step 10: Compiling the Model\n",
    "- Compile the model using **model.compile()** with the loss function set to **sparse_categorical_crossentropy**, the optimizer set to **adam**, and the metrics set to **accuracy**\n",
    "- Set the number of epochs to 10\n",
    "- Train the model using **model.fit()** with the training padded sequences **(train_padded)** and training label sequences **(training_label_seq)**\n",
    "- Specify the number of epochs as **num_epochs**, the validation data as the validation padded sequences **(validation_padded)** and validation label sequences **(validation_label_seq)**, and **verbose** mode as **2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOBn4QRv7k6_",
    "outputId": "1e042fb8-c5d1-4975-a839-4dd51c3d8ee6"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqnhF_pirkJL"
   },
   "source": [
    "__Observations:__\n",
    "- The code trains the model for the specified number of epochs and records the training and validation accuracy and loss metrics.\n",
    "- The output is an object named history that contains information about the training process, such as the loss and accuracy values at each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlqqXKGDtBhm"
   },
   "source": [
    "### Step 11: Plotting the Graph\n",
    "- Define a function named **plot_graphs** that takes history and string as inputs. Inside the function, plot the training and validation values of the given metric (string) from the history object using **plt.plot()**\n",
    "- Set the x-axis label as **Epochs** and the y-axis label as the given metric (string)\n",
    "- Call the **plot_graphs** function twice, first with **history** and **accuracy** as arguments, and then with **history** and **loss** as arguments\n",
    "- Display the generated plots showing the training and validation values of the accuracy and loss metrics over the epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "ovA5JGsp7k6_",
    "outputId": "34e95bfd-659b-4f79-8eef-e8530076e815"
   },
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
