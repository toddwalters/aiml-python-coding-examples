{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/C7/Transformers_from_Scratch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "btxy_uFPT_zi",
    "outputId": "2e35cc03-02e3-41fe-8b5c-bb0e82904148"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zcLl_PPeHeE4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import (Add, Dense, Layer, Dropout, Embedding, LayerNormalization,\n",
    "                                     MultiHeadAttention, TextVectorization, StringLookup)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "import tensorflow_text as tf_text\n",
    "_AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qJrVGWbcILp6",
    "outputId": "811da5b4-871f-428a-f503-0c00083f7a94"
   },
   "outputs": [],
   "source": [
    "!wget https://www.manythings.org/anki/fra-eng.zip\n",
    "!unzip fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q5YXi_-IpaE"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "  DATA_FNAME = 'fra.txt'\n",
    "  BATCH_SIZE = 256\n",
    "\n",
    "  SOURCE_VOCAB_SIZE = 10000\n",
    "  TARGET_VOCAB_SIZE = 10000\n",
    "\n",
    "  MAX_POS_ENCODING = 256 # define the max positions in source and target\n",
    "\n",
    "  ENCODER_NUM_LAYERS = 2 # number of layers for encoders\n",
    "  DECODER_NUM_LAYERS = 2 # number of layers for encoders\n",
    "\n",
    "  #Define the dimensions of models\n",
    "  D_MODEL = 128\n",
    "\n",
    "  #Number of units in FFNN\n",
    "  DFF = 256\n",
    "\n",
    "  #define the number of heads\n",
    "  NUM_HEADS = 4\n",
    "  DROP_RATE = 0.1\n",
    "\n",
    "  #Number of epochs to train\n",
    "  EPOCHS = 25\n",
    "\n",
    "  #Define the output directory\n",
    "  OUTPUT_DIR = 'output'\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHIty9jBK2u0"
   },
   "outputs": [],
   "source": [
    "# exp1 = Config()\n",
    "# exp1.EPOCHS = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smu1E63qLOjP"
   },
   "source": [
    "# Create Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vl1AsjUELMjs"
   },
   "outputs": [],
   "source": [
    "class BaseAttention(Layer):\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().init()\n",
    "    # Layer Definitions\n",
    "    self.mha = MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = LayerNormalization()\n",
    "    self.add = Add()\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attentionOuputs = self.mha(query=x,\n",
    "                               key = x,\n",
    "                               value=x)\n",
    "    x = self.add([x,attentionOuputs])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "  def call(self,x,context):\n",
    "    (attentionOutputs,attentionScores) = self.mha(query=x,\n",
    "                                                  key=context,\n",
    "                                                  value=context,\n",
    "                                                  return_attention_scores=True)\n",
    "    x = self.add([x,attentionOutputs])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "class CausalAttention(BaseAttention): # Masked Attention\n",
    "  def call(self,x):\n",
    "    attentionOutputs = self.mha(query=x,\n",
    "                                key=x,\n",
    "                                value=x,\n",
    "                                use_causal_mask=True)\n",
    "    x = self.add([x,attentionOutputs])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aY9xEp2LA4E"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data(file_name):\n",
    "  with open(file_name, 'rt') as textFile:\n",
    "    lines = textFile.readlines()\n",
    "    pairs = [line.split(\"\\t\")[:-1] for line in lines]\n",
    "    random.shuffle(pairs)\n",
    "    source = [src for src,_ in pairs]\n",
    "    target = [trgt for _,trgt in pairs]\n",
    "  return (source,target)\n",
    "\n",
    "def splitting_dataset(source,target):\n",
    "  trainSize = int(len(source) * 0.8)\n",
    "  valsize = int(len(source) * 0.1)\n",
    "\n",
    "  (trainSource,trainTarget) = (source[:trainSize],target[:trainSize])\n",
    "  (valSource,valTarget) = (source[trainSize:trainSize+valsize], target[trainSize:trainSize+valsize])\n",
    "  (testSource,testTarget) = (source[trainSize+valsize :], target[trainSize+valsize :])\n",
    "\n",
    "  return (\n",
    "      (trainSource,trainTarget),\n",
    "      (valSource,valTarget),\n",
    "      (testSource,testTarget)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhDFNTXaQE7q"
   },
   "outputs": [],
   "source": [
    "source,target = load_data(file_name=config.DATA_FNAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Mu4vbUqRQoD",
    "outputId": "e98e10a9-228b-494f-9054-e9c923ac36d9"
   },
   "outputs": [],
   "source": [
    "source[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UK6bpW4MRaIZ",
    "outputId": "fdf9d801-d201-497f-9089-86b9d6f794b9"
   },
   "outputs": [],
   "source": [
    "target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b42zTmsrRoZQ"
   },
   "outputs": [],
   "source": [
    "def make_dataset(splits, batchSize,sourceTextProcessor, targetTextProcessor, train=False):\n",
    "  (source,target) = splits\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((source,target))\n",
    "\n",
    "  def prepare_batch(source,target):\n",
    "    source = sourceTextProcessor(source)\n",
    "    targetBuffer = targetTextProcessor(target)\n",
    "    targetInput = targetBuffer[:,:-1]\n",
    "    targetOutput = targetBuffer[:,1:]\n",
    "    return (source,targetInput), targetOutput\n",
    "\n",
    "  if train:\n",
    "    dataset = (\n",
    "        dataset.shuffle(dataset.cardinality().numpy())\n",
    "        .batch(batchSize)\n",
    "        .map(prepare_batch,_AUTO)\n",
    "        .prefetch(_AUTO)\n",
    "    )\n",
    "  else:\n",
    "    dataset = dataset.batch(batchSize).map(prepare_batch,_AUTO).prefetch(_AUTO)\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enPiomFTTgAA"
   },
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # split accented characters\n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "\n",
    "    # keep space, a to z, and selected punctuations\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,]\", \"\")\n",
    "\n",
    "    # add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,]\", r\" \\0 \")\n",
    "\n",
    "    # strip whitespace and add [START] and [END] tokens\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[START]\", text, \"[END]\"], separator=\" \")\n",
    "\n",
    "    # return the processed text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bvze20gUl6N"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "source,target = load_data(file_name=config.DATA_FNAME)\n",
    "\n",
    "# Split the dataset\n",
    "(train,val,test) = splitting_dataset(source=source,target=target)\n",
    "\n",
    "# Create Text processing layer\n",
    "\n",
    "sourceTextProcessor = TextVectorization(standardize = tf_lower_and_split_punct ,\n",
    "                                        max_tokens=config.SOURCE_VOCAB_SIZE)\n",
    "sourceTextProcessor.adapt(train[0]) # source text data\n",
    "\n",
    "TargetTextProcessor = TextVectorization(standardize = tf_lower_and_split_punct ,\n",
    "                                        max_tokens=config.TARGET_VOCAB_SIZE)\n",
    "TargetTextProcessor.adapt(train[1]) # target text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aM-tkZoCV0cG"
   },
   "outputs": [],
   "source": [
    "trainDs = make_dataset(\n",
    "    splits=train,\n",
    "    batchSize=config.BATCH_SIZE,\n",
    "    train=True,\n",
    "    sourceTextProcessor=sourceTextProcessor,\n",
    "    targetTextProcessor=TargetTextProcessor\n",
    ")\n",
    "\n",
    "validDs = make_dataset(\n",
    "    splits=val,\n",
    "    batchSize=config.BATCH_SIZE,\n",
    "    train=False,\n",
    "    sourceTextProcessor=sourceTextProcessor,\n",
    "    targetTextProcessor=TargetTextProcessor\n",
    ")\n",
    "\n",
    "testDs = make_dataset(\n",
    "    splits=test,\n",
    "    batchSize=config.BATCH_SIZE,\n",
    "    train=False,\n",
    "    sourceTextProcessor=sourceTextProcessor,\n",
    "    targetTextProcessor=TargetTextProcessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opRW6M4aWjFA",
    "outputId": "05fc7b8b-b357-4b8d-cdfc-6947d0207760"
   },
   "outputs": [],
   "source": [
    "trainDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP-B4nHBXd0n"
   },
   "outputs": [],
   "source": [
    "class FeedForward(Layer):\n",
    "  def __init__(self,dff, dModel, dropoutRate=0.1, **kwargs ):\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    #Sequential\n",
    "    self.seq = Sequential([\n",
    "        Dense(units=dff, activation='relu'),\n",
    "        Dense(units=dModel),\n",
    "        Dropout(rate=dropoutRate)\n",
    "    ])\n",
    "\n",
    "    self.add = Add()\n",
    "    self.layernorm = LayerNormalization()\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.add([x,self.seq(x)])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltV_8uTyfrNp"
   },
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yWwgNRFxfqih"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer):\n",
    "\n",
    "  def __init__(self, dModel, numHeads, dff, dropoutRate=0.1, **kwargs):\n",
    "\n",
    "    super.__init__(**kwargs)\n",
    "\n",
    "    self.globalSelfAttention = GlobalSelfAttention(\n",
    "        num_heads = numHeads,\n",
    "        key_dim = dModel//numHeads,\n",
    "        dropout = dropoutRate\n",
    "    )\n",
    "\n",
    "    self.ffn = FeedForward(dff=dff, dModel = dModel, dropoutRate = dropoutRate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.globalSelfAttention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x\n",
    "\n",
    "class Encoder(Layer):\n",
    "  def __init__(self,\n",
    "               numLayers,\n",
    "               dModel,\n",
    "               numHeads,\n",
    "               sourceVocabSize,\n",
    "               maximumPositionEncoding,\n",
    "               dff, dropoutRate, **kwargs):\n",
    "    super.__init__(**kwargs)\n",
    "    self.dModel = dModel\n",
    "    self.numLayers = numLayers\n",
    "\n",
    "    self.positionalEmbedding = PositionalEmbedding(vocabSize = sourceVocabSize, dModel = dModel,\n",
    "                                                   maximumPositionEncoding = maximumPositionEncoding)\n",
    "\n",
    "    self.encoderLayers = [\n",
    "        EncoderLayer(dModel = dModel, dff = dff, numHeads = numHeads, dropoutRate = dropoutRate) for _ in range(numLayers)\n",
    "    ]\n",
    "\n",
    "    self.dropout = Dropout(rate=dropoutRate)\n",
    "\n",
    "  def call(self,x):\n",
    "    x = self.positionalEmbedding(x)\n",
    "    x = self.dropout(x)\n",
    "    for encoderLayer in self.encoderLayers:\n",
    "      x = encoderLayer(x=x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzwDRhWjhDo5"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(length,depth):\n",
    "  depth = depth/2\n",
    "  positions = np.arange(length)[:,np.newaxis]\n",
    "  depths = np.arange(depth)[np.newaxis,:]/depth\n",
    "\n",
    "  angleRates = 1/(10000**depths)\n",
    "  angleRads = positions * angleRates\n",
    "  posEncoding = np.concatenate([np.sin(angleRads), np.cos(angleRads)],axis=-1)\n",
    "  return tf.cast(posEncoding,dtype=tf.float32)\n",
    "\n",
    "class PositionalEmbedding(Layer):\n",
    "  def __init__(self,vocabSize,dModel, maximomPositonEncoding,**kwargs):\n",
    "    super.__init__(**kwargs)\n",
    "\n",
    "    self.embedding = Embedding(\n",
    "        input_dim = vocabSize, output_dim = dModel,mask_zero=True\n",
    "    )\n",
    "\n",
    "    self.posEncoding = position_encoding(length = maximumPositionEncoding,\n",
    "                                         depth=dModel)\n",
    "    self.dModel = dModel\n",
    "\n",
    "  def call(self,x):\n",
    "    seqLen = tf.shape(x)[-1]\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    x *= tf.math.sqrt(tf.cast(self.DModel,tf.float32))\n",
    "    x += self.posEncoding[tf.newaxis, :seqLen, : ]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQcUz4WJlJ83"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
