{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6uj1-TEDS54"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets.cifar10 import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94KACx6HE2Hc",
    "outputId": "d36defbf-3bc9-4a3a-b99a-f008a9e7b6d7"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OZdXuGzdFHtJ",
    "outputId": "f56a1f47-5079-40ab-85ec-d8d1949029a4"
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXUTAX0RFN9_",
    "outputId": "901083ff-8687-42e5-8ac1-b5c6cb1e4692"
   },
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvWg2mxiFSFx",
    "outputId": "3551cf4a-771d-4a67-9295-8df4a37567e6"
   },
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQeBggSbFS67"
   },
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "cifar10_classes = [\"airplane\",\"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "Ki8bVTuPFkf-",
    "outputId": "b30ed4da-79a4-4f00-df9b-66a5a117f44e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Declaring the number of rows\n",
    "rows = 3\n",
    "\n",
    "# Declaring the number of columns\n",
    "cols = 4\n",
    "\n",
    "fig = plt.figure(figsize = (10, 8))\n",
    "\n",
    "for i in range(cols):\n",
    "\n",
    "    for j in range(rows):\n",
    "\n",
    "        random_index = np.random.randint(0, len(y_train))\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i * rows + j + 1)\n",
    "\n",
    "        ax.imshow(x_train[random_index, :])\n",
    "\n",
    "        ax.set_title(cifar10_classes[y_train[random_index, 0]])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J9woE60JFvLd",
    "outputId": "a1a4ae33-1437-409b-fc64-5ed8104fae17"
   },
   "outputs": [],
   "source": [
    "# Data Preperation\n",
    "# normalize the data\n",
    "# transform the target data - one hot representation\n",
    "\n",
    "np.max(x_train),np.min(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy2mH0ohGQqn"
   },
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "x_train_normalized = x_train/255.0\n",
    "x_test_normalized = x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fT5HO5PVGVtE"
   },
   "outputs": [],
   "source": [
    "# one hot encoding of target\n",
    "\n",
    "y_train_encoded = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_encoded = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-JbNstluGcN_",
    "outputId": "0f2f000c-9be3-4b8c-cd30-a5f44e834a3c"
   },
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXMwFkZKGeqb",
    "outputId": "dc26ead6-2752-41f9-c494-acbe69867354"
   },
   "outputs": [],
   "source": [
    "y_train_encoded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n0_EcMTqGgm8"
   },
   "outputs": [],
   "source": [
    "# Model Building\n",
    "# Sending the data for CNN - 4 dim (batch_size,h,w,number of channels)\n",
    "# 60000,32,32,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK8ttSwHHDLa",
    "outputId": "2ef4fb49-5f3a-4f4b-8bc8-cf835f4ba34a"
   },
   "outputs": [],
   "source": [
    "model_0 = Sequential()\n",
    "model_0.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UmmkMF3HHMyY",
    "outputId": "7232bdfa-8679-470b-cfaf-56ec316892fb"
   },
   "outputs": [],
   "source": [
    "model_0 = Sequential()\n",
    "model_0.add(Conv2D(filters=1, kernel_size=(3,3), activation='relu', input_shape=(32,32,3),padding=\"same\"))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRSNNVtSHn7s",
    "outputId": "ecacdaa7-d0b4-4695-a7e7-132b6cab5aa7"
   },
   "outputs": [],
   "source": [
    "model_0 = Sequential()\n",
    "model_0.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(32,32,3),padding=\"same\"))\n",
    "model_0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QXEqvr34IUah"
   },
   "outputs": [],
   "source": [
    "# if we make of indices instead of one-hot encoding - loss function should be - sparse_categorical_crossentropy\n",
    "# if we make of one hot representation - loss function should be categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkqM2HxUJzf5",
    "outputId": "dbaf6228-cd7c-4b1e-d414-89dea14dad16"
   },
   "outputs": [],
   "source": [
    "#initialize a new model\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=(32,32,3),padding=\"same\"))\n",
    "model_1.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "model_1.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu',padding=\"same\"))\n",
    "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(128, activation='relu'))\n",
    "model_1.add(Dense(10, activation='softmax'))\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgUskbIeKyS7"
   },
   "outputs": [],
   "source": [
    "example = x_train_normalized[0].reshape(1,32,32,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IHryrhULMO6"
   },
   "outputs": [],
   "source": [
    "output = model_1.layers[0](example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6wl6rv4MnQa",
    "outputId": "7e5423a6-9232-48cd-a783-462bda11b9a5"
   },
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oqmd7bApM6R7"
   },
   "outputs": [],
   "source": [
    "one_sample = output[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "utrCOKvuNGIQ",
    "outputId": "deecf11b-ce9b-4b24-e379-8d9967dd15f1"
   },
   "outputs": [],
   "source": [
    "plt.imshow(x_train_normalized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "9Rv5Qo21NBa6",
    "outputId": "9b5ef491-28f3-4d74-f0f2-7ec7f23458a1"
   },
   "outputs": [],
   "source": [
    "plt.imshow(one_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAHpGs40NCmR"
   },
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2QxNN-vNhNe",
    "outputId": "238e8038-45e4-4247-b751-cee85e7098a2"
   },
   "outputs": [],
   "source": [
    "history = model_1.fit(x_train_normalized, y_train_encoded,\n",
    "                      validation_data=(x_test_normalized, y_test_encoded),\n",
    "                      batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "nxN-VaxHNneT",
    "outputId": "b501d9b2-77a1-40e4-bb13-126f0a12dcf2"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58ciGEPsObI2"
   },
   "outputs": [],
   "source": [
    "# Overfit\n",
    "# Dropout - Reguralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c2V2vQTNq7v",
    "outputId": "a34044db-f2ea-46a4-ac91-8f1a190f8035"
   },
   "outputs": [],
   "source": [
    "32*32*3*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P8IpumiROGI3",
    "outputId": "c22d232f-c917-4787-9363-d66e8bb4679b"
   },
   "outputs": [],
   "source": [
    "9*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "el1WufStOJFP"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LeakyReLU\n",
    "# Initialized a sequential model\n",
    "model_2 = Sequential()\n",
    "\n",
    "# Adding the first convolutional layer with 16 filters and the kernel size of 3x3, and 'same' padding\n",
    "\n",
    "# The input_shape denotes the input dimension of CIFAR images\n",
    "model_2.add(Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", input_shape = (32, 32, 3)))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_2.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding dropout to randomly switch off 20% neurons to reduce overfitting\n",
    "model_2.add(Dropout(0.2))\n",
    "\n",
    "# Adding the second convolutional layer with 32 filters and the kernel size of 3x3\n",
    "model_2.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_2.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding dropout to randomly switch off 20% neurons to reduce overfitting\n",
    "model_2.add(Dropout(0.2))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of second convolutional layer\n",
    "model_2.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening the 3-d output of the convolutional layer after max pooling to make it ready for creating dense connections\n",
    "model_2.add(Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 256 neurons\n",
    "model_2.add(Dense(256))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_2.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding dropout to randomly switch off 50% neurons to reduce overfitting\n",
    "model_2.add(Dropout(rate=0.5))\n",
    "\n",
    "# Adding the output layer with 10 neurons and 'softmax'  activation function since this is a multi-class classification problem\n",
    "model_2.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMqHvapDSOM2"
   },
   "outputs": [],
   "source": [
    "model_2.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "isA3vJIVSXLD",
    "outputId": "cb443020-0119-4e15-eb50-29bbbcfe4ca8"
   },
   "outputs": [],
   "source": [
    "history_2 = model_2.fit(x_train_normalized, y_train_encoded,\n",
    "                      validation_data=(x_test_normalized, y_test_encoded),\n",
    "                      batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPPOV6LoSbHc"
   },
   "outputs": [],
   "source": [
    "plt.plot(history_2.history['accuracy'])\n",
    "\n",
    "plt.plot(history_2.history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZdCiU3ZSe0a"
   },
   "outputs": [],
   "source": [
    "# Increase the layers\n",
    "\n",
    "# Initialized a sequential model\n",
    "model_3 = Sequential()\n",
    "\n",
    "# Adding the first convolutional layer with 16 filters and the kernel size of 3x3, and 'same' padding\n",
    "\n",
    "# The input_shape denotes input dimension of CIFAR images\n",
    "model_3.add(Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", input_shape = (32, 32, 3)))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding the second convolutional layer with 32 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of the second convolutional layer\n",
    "model_3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding dropout to randomly switch off 25% of the network to reduce overfitting\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Adding the third convolutional layer with 32 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding the fourth convolutional layer with 64 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of the fourth convolutional layer\n",
    "model_3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding dropout to randomly switch off 25% of the network to reduce overfitting\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Flattening the 3-d output of the convolutional layer after max pooling to make it ready for creating dense connections\n",
    "model_3.add(Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 256 neurons\n",
    "model_3.add(Dense(256))\n",
    "\n",
    "# Adding LeakyRelu activation function with negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding dropout to randomly switch off 50% of dense layer neurons to reduce overfitting\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "# Adding the output layer with 10 neurons and 'softmax' activation function since this is a multi-class classification problem\n",
    "model_3.add(Dense(10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFwmDbK_Uevp"
   },
   "outputs": [],
   "source": [
    "model_3.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGS_56alUiZF"
   },
   "outputs": [],
   "source": [
    "history_3 = model_3.fit(x_train_normalized, y_train_encoded,\n",
    "                      validation_data=(x_test_normalized, y_test_encoded),\n",
    "                      batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZNteWCQUlVg"
   },
   "outputs": [],
   "source": [
    "plt.plot(history_3.history['accuracy'])\n",
    "\n",
    "plt.plot(history_3.history['val_accuracy'])\n",
    "\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ge058IeEUnm_"
   },
   "outputs": [],
   "source": [
    "# Regulaization\n",
    "# L1 Regularization\n",
    "# L2 Regularization\n",
    "\n",
    "# Initialized a sequential model\n",
    "model_3 = Sequential()\n",
    "\n",
    "# Adding the first convolutional layer with 16 filters and the kernel size of 3x3, and 'same' padding\n",
    "\n",
    "# The input_shape denotes input dimension of CIFAR images\n",
    "model_3.add(Conv2D(filters = 16, kernel_size = (3, 3), padding = \"same\", input_shape = (32, 32, 3)))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding the second convolutional layer with 32 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of the second convolutional layer\n",
    "model_3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding dropout to randomly switch off 25% of the network to reduce overfitting\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Adding the third convolutional layer with 32 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding the fourth convolutional layer with 64 filters and the kernel size of 3x3\n",
    "model_3.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same'))\n",
    "\n",
    "# Adding LeakyRelu activation function with a negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding max pooling to reduce the size of output of the fourth convolutional layer\n",
    "model_3.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding dropout to randomly switch off 25% of the network to reduce overfitting\n",
    "model_3.add(Dropout(0.25))\n",
    "\n",
    "# Flattening the 3-d output of the convolutional layer after max pooling to make it ready for creating dense connections\n",
    "model_3.add(Flatten())\n",
    "\n",
    "# Adding a fully connected dense layer with 256 neurons\n",
    "model_3.add(Dense(256, kernel_regularizer=tf.keras.regularizers.L2(0.01)))\n",
    "\n",
    "# Adding LeakyRelu activation function with negative slope of 0.1\n",
    "model_3.add(LeakyReLU(0.1))\n",
    "\n",
    "# Adding dropout to randomly switch off 50% of dense layer neurons to reduce overfitting\n",
    "model_3.add(Dropout(0.5))\n",
    "\n",
    "# Adding the output layer with 10 neurons and 'softmax' activation function since this is a multi-class classification problem\n",
    "model_3.add(Dense(10, activation = 'softmax', kernel_regularizer=tf.keras.regularizers.L2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd3KsewbcQQP"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(Dense, kernel_regularizer=tf.keras.regularizers.L2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TMW9tYbche_"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    RegularizedDense(256, input_shape=(32, 32, 3)),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.5),\n",
    "    RegularizedDense(128),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    Dropout(0.5),\n",
    "    RegularizedDense(10, activation='softmax')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
