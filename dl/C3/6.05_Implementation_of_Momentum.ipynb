{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wv4Gvmgwa4gn"
   },
   "source": [
    "## __Gradient Descent with Momentum__\n",
    "\n",
    "Problem with the gradient descent algorithm:\n",
    "\n",
    "- The progression of the search can fluctuate within the search space based on the gradient.\n",
    "- This behavior can impede the progress of the search, particularly in optimization problems where the overall trend or shape of the search space is more valuable than specific gradients encountered along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dY3mU0P6a4gr"
   },
   "source": [
    "Momentum\n",
    "- It serves as an extension to the gradient descent optimization algorithm, aiming to expedite the optimization process by incorporating historical information into the parameter update equation.\n",
    "- This is achieved by considering the gradients encountered in previous updates.\n",
    "- In this approach, an additional hyperparameter is introduced to govern the degree of historical momentum included in the update equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K16_Lct9EsT1"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1.  Importing the required libraries\n",
    "2.  Defining the objective function\n",
    "3.  Defining the gradient descent algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIaxwF3UfKAs"
   },
   "source": [
    "### Step 1: Importing the Required Libraries\n",
    "\n",
    "- Import **numpy.asarray** to convert input data into an array\n",
    "- Import **numpy.random.rand** to generate random numbers from a uniform distribution\n",
    "- Import **numpy.random.seed** to set the seed for reproducible random number generation\n",
    "- Import **numpy.arange** to create an array of values within a specified range\n",
    "- Import **matplotlib.pyplot** that provides functions for creating plots and visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-8vf8Haa4gr"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy.random import rand\n",
    "from numpy.random import seed\n",
    "from numpy import arange\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UK7yPHdGfdnc"
   },
   "source": [
    "### Step 2: Defining the Objective Function\n",
    "- The function **objective** takes a single input parameter **x**.\n",
    "- It returns the square of the input value as the output, representing the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4PzQibNa4gr"
   },
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    return x**2.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu6svRFFg8nM"
   },
   "source": [
    "### Step 3: Defining the Gradient Descent Algorithm\n",
    "\n",
    "- You must calculate the square of **x**, representing the objective function.\n",
    "- The derivative(x) function computes the derivative of x with respect to the objective function.\n",
    "- The **gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)** function implements the gradient descent algorithm. It initializes a solution within the specified bounds and iteratively updates it based on the objective and derivative functions. The function also tracks and stores the solutions and their corresponding scores.\n",
    "- The random seed is set to 4 using seed(4) to ensure reproducibility.\n",
    "- The bounds variable defines the lower and upper bounds for the solution space.\n",
    "- Parameters such as the number of iterations (n_iter), step size (step_size), and momentum (momentum) are specified.\n",
    "- The **gradient_descent** function is called with the provided arguments, and the resulting solutions and scores are stored.\n",
    "- An array of input values (inputs) is generated using **arange** within the defined bounds.\n",
    "- The objective function values (results) are computed for the input values.\n",
    "- The objective function curve is plotted using **pyplot.plot** with inputs on the x-axis and results on the y-axis.\n",
    "- The optimization path is visualized by plotting the solutions and scores as red dots connected by lines using pyplot.plot.\n",
    "- Finally, **pyplot.show()** is called to display the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gsJsF4XYa4gs",
    "outputId": "3d642763-3612-4ddd-fbec-063359ec3462"
   },
   "outputs": [],
   "source": [
    "\n",
    "def derivative(x):\n",
    "    return x * 2.0\n",
    "\n",
    "def gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum):\n",
    "    solutions, scores = list(), list()\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "\n",
    "    change = 0.0\n",
    "\n",
    "    for i in range(n_iter):\n",
    "\n",
    "        gradient = derivative(solution)\n",
    "\n",
    "        new_change = step_size * gradient + momentum * change\n",
    "\n",
    "        solution = solution - new_change\n",
    "\n",
    "        change = new_change\n",
    "\n",
    "        solution_eval = objective(solution)\n",
    "\n",
    "        solutions.append(solution)\n",
    "        scores.append(solution_eval)\n",
    "\n",
    "        print('>%d f(%s) = %.5f' % (i, solution, solution_eval))\n",
    "    return [solutions, scores]\n",
    "\n",
    "\n",
    "seed(4)\n",
    "\n",
    "bounds = asarray([[-1.0, 1.0]])\n",
    "\n",
    "n_iter = 30\n",
    "\n",
    "step_size = 0.1\n",
    "momentum = 0.3\n",
    "\n",
    "inputs = arange(bounds[0,0], bounds[0,1] + 0.1, 0.1)\n",
    "results = objective(inputs)\n",
    "pyplot.plot(inputs, results)\n",
    "pyplot.show()\n",
    "\n",
    "solutions, scores = gradient_descent(objective, derivative, bounds, n_iter, step_size, momentum)\n",
    "pyplot.plot(inputs, results)\n",
    "pyplot.plot(solutions, scores, '.-', color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ex2cInyQjU8J"
   },
   "source": [
    "**Observation**\n",
    "- The code snippet visualizes the convergence of the gradient descent algorithm by plotting the objective function and the solutions found at each iteration, providing a graphical representation of the optimization process."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
