{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/deep-learning/C8/11_05_Introduction_to_BERT__V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C93-SkWJoY5b"
   },
   "source": [
    "# __Introduction to BERT and Transformers Library__\n",
    "- BERT stands for Bidirectional Encoder Representations from Transformers.\n",
    "- BERT is pre-trained on a large corpus of unlabeled text, including the entire Wikipedia (that's 2,500 million words!) and the Book Corpus (800 million words).\n",
    "- BERT is based on the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YQ_il2txL-2"
   },
   "source": [
    "## Steps to Be Followed:\n",
    "1. Importing required libraries\n",
    "2. Analyzing sentiment using transformer pipeline\n",
    "3. Creating text generation\n",
    "4. Creating named entity recognition (NER)\n",
    "5. Generating masked language model using a model and a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scQJ3cH8m61w"
   },
   "source": [
    "### Step 1: Importing Required Libraries\n",
    "- The code from the transformers import pipeline allows for easy access to pre-trained models and simplified execution of NLP tasks using the transformers library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k897DME3x3Wn"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bd9WmpIFx4pS"
   },
   "source": [
    "###Step 2: Analyzing Sentiment Using Transformer Pipeline\n",
    "\n",
    "- Import the pipeline function from the Transformers library, which enables easy access to pre-trained NLP models\n",
    "- The snippet creates a sentiment analysis pipeline using the pre-trained model and uses it to classify the sentiment of the input text **I hate you**\n",
    "- The result, including the sentiment label and score, is then printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruMCIipSm61x",
    "outputId": "ddb16688-ee97-4b4c-f5d2-1d041b0666d7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = classifier(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-frtliQRqC9r"
   },
   "source": [
    "- Perform sentiment analysis on the text **I love you**.\n",
    "- Print the sentiment analysis result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "phseGtlum61y",
    "outputId": "f263c9d2-d21f-4459-980c-10727689ecd9"
   },
   "outputs": [],
   "source": [
    "result = classifier(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS6lz9OAq-ES"
   },
   "source": [
    "**Observation**\n",
    "- The sentiment analysis model is highly confident that the sentiment of the text **I love you** is positive, with a score of 0.9999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNEoZm3wrGc_"
   },
   "source": [
    "### Step 3: Creating Text Generation\n",
    "- It creates a text generation pipeline using the pipeline function from the Transformers library.\n",
    "- It generates text starting with the provided prompt **As far as I am concerned, I will** using the text generation pipeline, with a maximum length of 50 tokens and without sampling, which is deterministic output.\n",
    "- The generated text is then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351,
     "referenced_widgets": [
      "3ee7ae13c13c4e08b7f7140ac5001d9e",
      "ebb9febfb38b492f992479ed8e203d09",
      "be46451521b34fb3a142709e1fde7536",
      "5d25ecd0ce144cd2892030c041241d54",
      "bb070803cd89460fbc3fbbd190d06d9e",
      "30b67d7f13174079a87143d54213611e",
      "80914a7ddec9428ab780ce7336643be5",
      "dda4aa7f907a4f4ba0e1257bfbf7e26f",
      "7d4d02889c624fd1b6799b96e09a3484",
      "4b3a68df696f44abbee31f13680247b1",
      "fa635022131041df9a22fafe431bd7aa",
      "e55e35ace58b4c23ac09dcaa8af21bcb",
      "724cde3e540145229bb9e548215acfe6",
      "2db2be4480094c0ab77bac465454fbb8",
      "72398eeaa4804b67ad05cb5048ae92dc",
      "e0a30a2232834e16a11c9a4557a4de06",
      "f38d5299d4e44da7996d5370e640ace2",
      "73b1184bd8634efb80160459417badc2",
      "32f87114955d4180bbe61ddea0547292",
      "7f50e1aef2634cfdae74bc8d918ce60d",
      "37542e0505c74c26a31d4b60c8fd785b",
      "baae23972740421cbf19af2064456e30",
      "ad5aa666c93d401a81a0e5196713253f",
      "1c94a6e034544298b4c81997870d3a59",
      "85a8f3ec6d7c4352a5ebe1687d2dd806",
      "122367c84ed34ff69d8a40519e3b1ca6",
      "894cf081d20640f68e9c811bec33386e",
      "88a427c18bf44e8aa5b9ac9f47a9357b",
      "33649b6aed264d4aaef868820ca78563",
      "8564be1f3d4348c69e5640268adbf2f1",
      "ac66132ce4714e89a8f3db3f84796053",
      "0eeeb16ad3f946d9922a688b4606c180",
      "4e293bcb03624aeabaccd0b2182dfdb5",
      "2eed48789d124aa18c3cd2caa1ec4570",
      "f307c4c84ad94cfebc2f5326e0ac7a0c",
      "0c5d9b6d754d4145bbcc84e4080d1576",
      "c90746e1eb8441fca6e7fe0becb3bdcb",
      "8f0266e57a284c62be9491fcb399ec0b",
      "5a71ca6fb19545988fda89fd70bff6cc",
      "fc1fa4ad9def45678eb0564f42d58c19",
      "3ac75eb614d6457eaccc39f1c5df61af",
      "4a7615d6260748cba6d883dcc6e9a4b0",
      "3cd0fe79f1554409a79335d1c4186878",
      "56ac5aec77624a7899e37765c3027a04",
      "c3014ee6ee3944c2b8fa60a3f5031856",
      "02aac68a87444397be252ca2636a1c25",
      "0a4ef47d18d941b699d8f153e09cb808",
      "f10f6c75928e4656b03df50b48dc60f4",
      "cf3a3c61936d4863bb54030db7e97287",
      "072d5b1cdab24c06ba7ff0fa16013fbe",
      "8721f0dd7dc3479fb52e3acf111af974",
      "ceea0872ccd84ba09b741f0d1939d24f",
      "cba2151553cb43ebb79011eee2bfd68c",
      "29047f1f83bd45768f8584a7f13f2705",
      "19e6ec13a7be4318b7a59d7d5ba415b4",
      "8bb9e0a728d4439381bd3e6fd0a0ca83",
      "07aed670b1294a12bbe70dc9aa62e75f",
      "100a45cd8df54ce799bb908245607a6e",
      "5491d44f64ee47c38b272c4bad32401d",
      "28d2c25d86d746bfb5df6f0350d71340",
      "0b81db3be19e4ebd9f7fb35b4fbe3bb0",
      "fb3a334588c144be83871d37c7feff68",
      "6e8b6ff3f7c1415a90dd019386b08048",
      "a15af5b363144088b9a65ab75dff6272",
      "85511d0cf4aa40f0b005d55d0777ecfb",
      "9f0e814f60e341ae9108244cdd3af510",
      "7a5b1850e1524273836167bd337f7b3c",
      "ebe4b797918144bda3085c44598d3232",
      "9bb9212bf9c24dab85463d94deea2533",
      "b36e62fd40c24017bf9499f527549d02",
      "6021857726874b15baab655e82dfbc19",
      "b25bb762b81347689351c036276bfe9a",
      "af67b34ca2ac4f49b8b76228ff0dc2da",
      "862efcb525ef4098a54f0741cb5856cf",
      "a53acab683c5481391367896d21f9fa3",
      "c2ea93c0ae094ef092405dd7606504fc",
      "cffc4c9b35744b0698a7d88dbb4deea2"
     ]
    },
    "id": "hjNtv9zVm61y",
    "outputId": "ceeed947-d672-45c5-bc9b-211b02f7d8c6"
   },
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"As far as I am concerned, I will\", max_length=50, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkrObi9WrcIL"
   },
   "source": [
    "### Step 4: Creating Named Entity Recognition (NER)\n",
    "- It creates a NER pipeline using the pipeline function from the Transformers library.\n",
    "\n",
    "- It applies the NER pipeline to the provided sequence, which is a text containing named entities. The pipeline identifies and extracts named entities such as organization names **Hugging Face Inc.**, locations **New York City**, and others. The extracted entities are then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zi0w6ZBm61y",
    "outputId": "4751bad8-a3df-4864-b0ea-17ea1b6849a3"
   },
   "outputs": [],
   "source": [
    "ner_pipe = pipeline(\"ner\")\n",
    "sequence = \"\"\"Hugging Face Inc. is a company based in New york city. Manhattan bridge is visible from the window.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y7-t5pnr0lj"
   },
   "source": [
    "- Print the Entities after Performing Named Entity Recognition on the Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eph1vgCSm61y",
    "outputId": "955a79f0-88b6-45a9-8a1d-83b0349bc338"
   },
   "outputs": [],
   "source": [
    "for entity in ner_pipe(sequence):\n",
    "    print(entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "TMl5ZcIsTbsg",
    "outputId": "1015da35-fc54-44d2-be51-d9ecc68069ca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(sequence)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Imd2QcVEm61z"
   },
   "source": [
    "\n",
    "### Step 5: Generating Masked Language Model Using a Model and a Tokenizer\n",
    "\n",
    "- Masked Language Modeling Using a Model and a Tokenizer\n",
    "  - Masked language modeling is a task where a model fills in masked tokens in a sequence, improving its understanding of language. It involves predicting missing tokens by considering the context of surrounding words.\n",
    "\n",
    "- The process includes the following steps:\n",
    "  - Instantiate a tokenizer and a model from the checkpoint name.\n",
    "  - Define a sequence with a masked token, placing the tokenizer.mask_token instead of a word.\n",
    "  - Encode that sequence into a list of IDs and find the position of the masked token in that list.\n",
    "  - Retrieve the predictions at the index of the masked token\n",
    "  - Retrieve the top 5 tokens using the PyTorch topk or TensorFlow top_k methods\n",
    "  - Replace the masked token with the tokens and print the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrNd1zdTvI8K"
   },
   "source": [
    " ### Masked Langauge Modeling\n",
    "- Import the necessary modules from the transformers library and torch\n",
    "- Load the pre-trained tokenizer and model\n",
    "- Define the input sequence with a masked token\n",
    "- Tokenize the input sequence and convert to tensors\n",
    "- Find the index of the masked token and generate token predictions using the model\n",
    "- Get the indices of the top 5 predicted tokens and print them in the sequence\n",
    "- Load the pre-trained tokenizer and model\n",
    "- Define the input sequence with a masked token\n",
    "- Tokenize the input sequence and convert to tensors\n",
    "- Find the index of the masked token and generate token predictions using the model\n",
    "- Get the indices of the top 5 predicted tokens and print them in the sequence\n",
    "- Print the top 5 predicted tokens in the masked position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323,
     "referenced_widgets": [
      "5a0cc406977e43debff205d1891d4a8c",
      "b457781386f54120b81cc82ee2e0be42",
      "80b1846ddef44b9491a0f99e23d9e8a3",
      "b999f688f11640f8b896288260015d02",
      "ebc46163279f4e2abf8dba6e56d6f5e5",
      "f5323fdf58f6429dac288171a525bb8f",
      "2690371bbf64478ebd93819218f27b7e",
      "66a11cfd6cbf454bac49eca91050c0be",
      "7bc2be13b17e4a46a2cb88cac335f0ca",
      "ae12faf157b545e38c5633ff95135c23",
      "6251c4a35566441a8ea43c964f600924",
      "7b9d8ec761944dca9feedd23365b62cd",
      "d3183f60673546c8abf7c39ee9338a47",
      "51aef0563e7e41d8a9866cbc6ca20c91",
      "e43445caae5c47f6820b75ecf9bbc496",
      "dbb327bcf0e148248335b1a1d63dfd19",
      "8d0bc8541a904e7e8afc3ddba4e443b5",
      "08f60a28f5e047d78c46526b98e8ea39",
      "5d62ab39fea34ca4989e14c392d80601",
      "f4068182850c49ec96ff967fb3a288cf",
      "6a3a75f739914a32ab1525a7f9e6c2cc",
      "a1fa24e8e5284905a046ba0710a3fea5",
      "4e65591e4b27437095437381363a015c",
      "f54e4ac2472c46818cee8d14b78bb615",
      "c3322ec14b774015b263da7927a48363",
      "5162520b2de642df9d8bf3aa69ddaf86",
      "4462b6b5700044b5a0970582d0a82460",
      "ef88001aeb564ee388e4b3c5e2a69890",
      "bc762dd7f0714c4aaa86ac08adb92be2",
      "f1a05636190c4608ab2376f21e9fd0b7",
      "92f00d76058641aeb3875721fe47121b",
      "87ce067caedb4c5a9a5caac77936418e",
      "d1914fc4067c4358a8148444d3b8fe32",
      "2eb667dbce7d4a288f0a8aab6ea6390a",
      "29c5a4cacace446599840d16404690f9",
      "8c3f031059c54c45ae3a4ac13ef950df",
      "0efea048a5454a34a68c4d0263f98f0e",
      "798504b406bb46c298f208de2b7ebe44",
      "32c68e1652e54b889b3830f1a311218c",
      "45e54bf1a90d4e0aaff269c486bb78a3",
      "108d6940aee24ba7a6700b3cd4c74502",
      "cdf5543b273e4be5a48361c37b72583d",
      "7244392a67504e9b93a25d6559c50df9",
      "074d5a9b267d4c29aa4ba4de3b3c1e47",
      "b737e63ce0054640873d940438e89447",
      "a77506bbb5754d0f832be28e62450005",
      "e51a2b8a5a864729983ce582e382d736",
      "b18f320be08f462f8ab7f9ad0bc77773",
      "1044d4e4cd38425b86bfa0c4ef49c5c6",
      "43d7e1e048ba4515b4ec938cbf0d5621",
      "dee9db2297df4cba9517f6ddc5c9c186",
      "198c5ce4a7634c99ae1702d3f378b524",
      "774ad08d9ac74a42b9687e42afc06bcb",
      "a8c719b8714d4355b7e4498c9dd84698",
      "4f43802b44204268b9fc00412a41b2dc"
     ]
    },
    "id": "fI7dzV5Tm61z",
    "outputId": "fac2e8dd-9a06-476a-977a-b4409974fec5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "sequence = (\n",
    "    \"Distilled models are smaller than the models they mimic. Using them instead of the large \"\n",
    "    f\"versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(sequence, return_tensors=\"pt\") # \"tf\"\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1] # np.where\n",
    "\n",
    "token_logits = model(**inputs).logits\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tfBK1kmSXfAB",
    "outputId": "22897c9e-86ff-425e-96b4-95936563d361"
   },
   "outputs": [],
   "source": [
    "#first word\n",
    "first_word = token_logits[0, 0, :]\n",
    "top_5_tokens = torch.topk(first_word, 5, dim=0).indices.tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtziyGkzXwxY",
    "outputId": "068b2d89-d9d2-44f4-b74f-1cb719aa95b6"
   },
   "outputs": [],
   "source": [
    "#first word\n",
    "first_word = token_logits[0, 1, :]\n",
    "top_5_tokens = torch.topk(first_word, 5, dim=0).indices.tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2tCG83UvYBZp",
    "outputId": "21edce42-2b59-4403-e76f-f2119db74abf"
   },
   "outputs": [],
   "source": [
    "#first word\n",
    "first_word = token_logits[0, 2, :]\n",
    "top_5_tokens = torch.topk(first_word, 5, dim=0).indices.tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eDBXMJ8mYDcD",
    "outputId": "af8210f5-23fa-4c9d-b174-ac8972b14aa5"
   },
   "outputs": [],
   "source": [
    "#first word\n",
    "first_word = token_logits[0, 3, :]\n",
    "top_5_tokens = torch.topk(first_word, 5, dim=0).indices.tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kZBbqqYYFV9",
    "outputId": "0e451192-89b5-403f-c936-04245868cab7"
   },
   "outputs": [],
   "source": [
    "#first word\n",
    "first_word = token_logits[0, 4, :]\n",
    "top_5_tokens = torch.topk(first_word, 5, dim=0).indices.tolist()\n",
    "for token in top_5_tokens:\n",
    "    print(tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wtY6dQZ0Wfu6",
    "outputId": "6cd801b8-7759-404a-ba43-e1c7b0479fc0"
   },
   "outputs": [],
   "source": [
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA8zRejtW1Ca"
   },
   "outputs": [],
   "source": [
    "mask_token_logits = token_logits[0, mask_token_index, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4H9yhewW1ur",
    "outputId": "19d1b8fa-5ae1-437a-f75f-214ff8069c2d"
   },
   "outputs": [],
   "source": [
    "mask_token_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cB7sDnvh_qhp"
   },
   "source": [
    "**Observation**\n",
    "- The output provides alternative sentence suggestions by replacing the masked token with different predicted tokens, demonstrating how using distilled models instead of larger ones can impact the carbon footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3NLlv_qm610"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
