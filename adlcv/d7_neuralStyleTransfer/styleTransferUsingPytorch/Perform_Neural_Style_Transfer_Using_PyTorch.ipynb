{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/toddwalters/pgaiml-python-coding-examples/blob/main/advanced-deep-learning-computer-vision/d7/Perform_Neural_Style_Transfer_Using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDcs09xc9F_S"
   },
   "source": [
    "# Perform Neural Style Transfer Using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEAsebmO9F_V"
   },
   "source": [
    "## Step 1: Import the Necessary Libraries\n",
    "- Install the **torch** package\n",
    "- Install **torchvision** package which provides utility functions and datasets for working with computer vision tasks in conjunction with PyTorch\n",
    "\n",
    "**Note:** Install these packages only when using a local machine, not the Simplilearn lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FnCSoHqxGkPy"
   },
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqoY4dxMGkQH"
   },
   "outputs": [],
   "source": [
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EsJq1Ga7GBW"
   },
   "source": [
    "- Import the torch module for working with PyTorch\n",
    "- Import specific modules from torchvision\n",
    "- Import the Image module from PIL (Python Imaging Library)\n",
    "- Import the pyplot module from matplotlib\n",
    "- Import the numpy module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTfyTxtQyIqc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms , models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-m-0ZTW9F_c"
   },
   "source": [
    "## Step 2: Check If Cuda Is Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itAgIyA2yIqm"
   },
   "outputs": [],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns34L8Vd9F_h"
   },
   "source": [
    "## Step 3: Load a VGG19 Model\n",
    "- The VGG19 model is loaded with pretrained weights.\n",
    "- The parameters of the model are set to have **requires_grad** as False, making them non-trainable.\n",
    "- The model is then moved to the specified device, preparing it for training or inference on that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQi0hj0VyIqs"
   },
   "outputs": [],
   "source": [
    "model = models.vgg19(pretrained=True).features\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15uTWVBr9F_n"
   },
   "source": [
    "## Step 4: Choose Layers for Style and Content Loss\n",
    "- Add a batch dimension to the input tensor\n",
    "- Pass the input through the layer\n",
    "- Store the output of specific layers in the features dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1LK32VEyIqy"
   },
   "outputs": [],
   "source": [
    "def model_activations(input,model):\n",
    "    layers = {\n",
    "    '0' : 'conv1_1',\n",
    "    '5' : 'conv2_1',\n",
    "    '10': 'conv3_1',\n",
    "    '19': 'conv4_1',\n",
    "    '21': 'conv4_2',\n",
    "    '28': 'conv5_1'\n",
    "    }\n",
    "    features = {}\n",
    "    x = input\n",
    "    x = x.unsqueeze(0)\n",
    "    for name,layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jPTTgvu9F_s"
   },
   "source": [
    "## Step 5: Transform the Images\n",
    "- Use (0.5, 0.5, 0.5) for both mean and sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZEz0sgSyIq4"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(300),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrySCQa0LzeM"
   },
   "source": [
    "## Step 6: Open the Content Image and Convert It to RGB Format\n",
    "- Apply transformations to the content image and move it to the specified device\n",
    "- Print the shape of the content image tensor\n",
    "- Open the style image and convert it to RGB format\n",
    "- Apply transformations to the style image and move it to the specified device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kuFBEMUiyIq8"
   },
   "outputs": [],
   "source": [
    "content = Image.open(\"content.jpg\").convert(\"RGB\")\n",
    "content = transform(content).to(device)\n",
    "print(\"Content shape => \", content.shape)\n",
    "style = Image.open(\"style.jpg\").convert(\"RGB\")\n",
    "style = transform(style).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERDJEePUlfJ4"
   },
   "source": [
    "## Step 7: Function to Convert the Image\n",
    "- Convert the image tensor to a numpy array on the CPU, and remove singleton dimensions\n",
    "- Transpose the dimensions to bring the channel dimension to the last axis\n",
    "- Scale the values and shift them to the range [0, 1]\n",
    "- Clip the values to the range [0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etAOtTQZyIrD"
   },
   "outputs": [],
   "source": [
    "def imcnvt(image):\n",
    "    x = image.to(\"cpu\").clone().detach().numpy().squeeze()\n",
    "    x = x.transpose(1,2,0)\n",
    "    x = x*np.array((0.5,0.5,0.5)) + np.array((0.5,0.5,0.5))\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9g_dKCc9F_5"
   },
   "source": [
    "## Step 8: Print the Image\n",
    "- Create a figure and two subplots\n",
    "- Display the content image on the first subplot using imshow\n",
    "- Display the style image on the second subplot using imshow\n",
    "- Show the figure with the subplots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-JFUloRyIrL"
   },
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "ax1.imshow(imcnvt(content),label = \"Content\")\n",
    "ax2.imshow(imcnvt(style),label = \"Style\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD-3w1epMhVn"
   },
   "source": [
    "## Step 9: Get the Dimensions of the Image Feature Tensor\n",
    "- Reshape the tensor to have dimensions (d, h * w)\n",
    "- Compute the Gram matrix by matrix multiplication\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-j7g3hxyIrR"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(imgfeature):\n",
    "    _,d,h,w = imgfeature.size()\n",
    "    imgfeature = imgfeature.view(d,h*w)\n",
    "    gram_mat = torch.mm(imgfeature,imgfeature.t())\n",
    "\n",
    "    return gram_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6z3f5ikMvFv"
   },
   "source": [
    "## Step 10: Set Device to Cuda If Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQhIt49FyIrX"
   },
   "outputs": [],
   "source": [
    "target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "print(\"device = \",device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPOUHT2QM2eT"
   },
   "source": [
    "## Step 11: Extract the Style and Content Features Using the Model Activations\n",
    "- The style features are extracted using the **model_activations** function on the style image.\n",
    "- The content features are extracted using the **model_activations** function on the content image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpUTsfJ_yIre"
   },
   "outputs": [],
   "source": [
    "style_features = model_activations(style,model)\n",
    "content_features = model_activations(content,model)\n",
    "\n",
    "style_wt_meas = {\"conv1_1\" : 1.0,\n",
    "                 \"conv2_1\" : 0.8,\n",
    "                 \"conv3_1\" : 0.4,\n",
    "                 \"conv4_1\" : 0.2,\n",
    "                 \"conv5_1\" : 0.1}\n",
    "\n",
    "style_grams = {layer:gram_matrix(style_features[layer]) for layer in style_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXjuYKnONSNZ"
   },
   "source": [
    "## Step 12: Extract the Features of the Target Image Using the Model Activations\n",
    "- Get the style Gram matrix for the layer\n",
    "- Get the target Gram matrix for the layer\n",
    "- Compute the Gram matrix for the target\n",
    "- Compute the style loss for the layer\n",
    "- Compute the total loss and print the total loss for every tenth epoch\n",
    "- Clear the gradients\n",
    "- Backpropagate the total loss\n",
    "- Update the target image using the optimizer\n",
    "- Display the target image after a certain number of epochs\n",
    "- Save the target image as a PNG file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keUbjUf4yIrl"
   },
   "outputs": [],
   "source": [
    "content_wt = 100\n",
    "style_wt = 1e8\n",
    "\n",
    "print_after = 500\n",
    "epochs = 2000\n",
    "optimizer = torch.optim.Adam([target],lr=0.007)\n",
    "\n",
    "for i in range(1,epochs+1):\n",
    "    target_features = model_activations(target,model)\n",
    "    content_loss = torch.mean((content_features['conv4_2']-target_features['conv4_2'])**2)\n",
    "\n",
    "    style_loss = 0\n",
    "    for layer in style_wt_meas:\n",
    "        style_gram = style_grams[layer]\n",
    "        target_gram = target_features[layer]\n",
    "        _,d,w,h = target_gram.shape\n",
    "        target_gram = gram_matrix(target_gram)\n",
    "\n",
    "        style_loss += (style_wt_meas[layer]*torch.mean((target_gram-style_gram)**2))/d*w*h\n",
    "\n",
    "    total_loss = content_wt*content_loss + style_wt*style_loss\n",
    "\n",
    "    if i%10==0:\n",
    "        print(\"epoch \",i,\" \", total_loss)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i%print_after == 0:\n",
    "        plt.imshow(imcnvt(target),label=\"Epoch \"+str(i))\n",
    "        plt.show()\n",
    "        plt.imsave(str(i)+'.png',imcnvt(target),format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSVdt6c-7GBY"
   },
   "source": [
    "**Observation:**\n",
    "- The code performs neural style transfer using a VGG19 model, displaying the stylized image every 500 epochs and saving it with the respective epoch number as the filename."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
