logging:
  log_dir: 'logs'
  log_file: '1697032566-training.log'            # Directory for saving logs
  log_level: 'INFO'      # Logging level

model:
  name: 'VGG16'                            # Model name
  preprocessing_function: 'vgg_preprocess'   # Function for preprocessing images
  use_pretrained_weights: True                 # Use pre-trained weights or not
  input_shape: [224, 224, 3]                    # Input image dimensions
  dense_units: 128                              # Units in the dense layer
  dropout_rate: 0.3                             # Dropout rate
  initial_learning_rate: 0.001                  # Initial learning rate
  decay_steps: 100                              # Learning rate decay steps
  decay_rate: 0.96                              # Learning rate decay rate
  fine_tune_learning_rate: 1e-5                 # Learning rate for fine-tuning
  additional_metrics:                           # List of metrics to track
    - Precision
    - Recall
    - AUC
    - F1Score


augmentation:
  enabled: True                      # Toggle to enable/disable data augmentation
  rotation_range: 10                 # Rotation in degrees (<= 360)
  width_shift_range: 0.2             # Width shift fraction (<= 1.0)
  height_shift_range: 0.2            # Height shift fraction (<= 1.0)
  horizontal_flip: True              # Enable horizontal flipping
  vertical_flip: False               # Enable vertical flipping
  zoom_range: 0.2                    # Zoom range (<= 1.0)
  brightness_range: 0.6              # Brightness adjustment
  # contrast_range: 0.1              # adjust as needed
  # saturation_range: 0.1
  # hue_range: 0.05
  # gaussian_blur: 0.5
  # shear_range: 0.2
  # cutout_size: [10, 10]

data:
  pre_split: False                            # Pre-split dataset directories or not
  test_val_size: 0.4                          # Test+Val dataset size in percentage of total dataset
  test_val_split: 0.5                         # Test and Validation split ratio of test_val_size (0.5 = 50%)
  dataset_dir: 'att_faces'                    # Root dataset directory (used if pre_split=True)
  train_dir: 'train_dataset'                  # Directory for training data
  test_dir: 'test_dataset'                    # Directory for test data
  validation_dir: 'val_dataset'               # Directory for validation data
  color_mode: 'rgb'                     # Image color mode 'rgb' or 'grayscale'
  batch_size: 16                              # Batch size for training
  input_shape: [224, 224, 3]                   # Input image dimensions
  target_size: [224, 224]                      # Image resizing dimensions

tuning:
  perform_tuning: False              # Toggle for performing hyperparameter tuning
  max_trials: 2                      # Maximum number of tuning trials
  executions_per_trial: 1            # Number of executions per tuning trial

training:
  override: False                                # Use custom model training configuration
  initial_epochs: 2                          # Number of initial epochs
  fine_tune_epochs: 0                           # Number of fine-tuning epochs
  patience: 2                                 # Early stopping patience
  target_accuracy: 0.99                         # Desired target accuracy
  find_lr: False                                # Enable learning rate finder
  model_checkpoint_path: 'checkpoints/checkpoint.h5.keras'  # Path for saving the model checkpoint

hyperparameters:
  pretrained_model:                  # Hyperparameters for pre-trained models
    num_dense_layers:
      min: 1
      max: 3
      default: 2
    dense_units:
      min: 128
      max: 1024
      step: 128
      default: 512
    dropout_rate:
      min: 0.0
      max: 0.7
      step: 0.1
      default: 0.5
    use_batch_norm:
      default: False
    optimizer:
      choices: ['adam', 'sgd']
      default: 'adam'
    learning_rate:
      min: 1e-5
      max: 1e-3
      default: 0.001

  scratch_model:
    num_conv_layers:
      min: 1
      max: 5
      default: 3
    conv_filters_scratch:
      min: 32
      max: 256
      step: 32
      default: 32
    conv_kernel_size_scratch:
      choices: [3, 5]
      default: 3
    use_conv_batch_norm_scratch:
      default: False
    conv_dropout_rate_scratch:
      min: 0.0
      max: 0.5
      step: 0.1
      default: 0.0
    num_dense_layers_scratch:
      min: 1
      max: 4
      default: 2
    dense_units_scratch:
      min: 64
      max: 512
      step: 64
      default: 128
    use_dense_batch_norm_scratch:
      default: False
    dropout_rate_scratch:
      min: 0.0
      max: 0.5
      step: 0.1
      default: 0.3
    optimizer_scratch:
      choices: ['adam', 'sgd']
      default: 'adam'
    learning_rate_scratch:
      min: 1e-5
      max: 1e-2
      default: 0.001

reduce_lr_on_plateau:
  monitor: 'val_loss'                # Metric to monitor for reducing LR
  factor: 0.2                        # Factor to reduce LR by
  patience: 20                       # Patience before reducing LR
  min_lr: 1e-6                       # Minimum learning rate allowed
  verbose: 1                         # Verbosity level for logging LR changes

gpu:
  memory_growth: true                # Allow GPU memory growth
  allow_growth: true                 # Allow dynamic GPU memory allocation

# ARCHITECTURES = {
#   'ResNet50V2': ResNet50V2,
#   'VGG16': VGG16,
#   'InceptionV3': InceptionV3,
#   'MobileNetV2': MobileNetV2,
#   'EfficientNetB0': EfficientNetB0
# }
#
# ARCHITECTURE_INPUT_SHAPES = {
#   'ResNet50V2': (128, 128, 3),
#   'VGG16': (224, 224, 3),
#   'InceptionV3': (299, 299, 3),
#   'MobileNetV2': (224, 224, 3),
#   'EfficientNetB0': (224, 224, 3)
# }
#
# PREPROCESSING_FUNCTIONS = {
#   'ResNet50V2': resnet_preprocess,
#   'VGG16': vgg_preprocess,
#   'InceptionV3': inception_preprocess,
#   'MobileNetV2': mobilenet_preprocess,
#   'EfficientNetB0': efficientnet_preprocess
# }
