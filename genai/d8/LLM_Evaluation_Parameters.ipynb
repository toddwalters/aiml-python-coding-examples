{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4977522e",
      "metadata": {
        "id": "4977522e"
      },
      "source": [
        "# LLM Evaluation Parameters and Techniques\n",
        "Evaluating large language models (LLMs) is crucial for understanding their performance, capabilities, and areas of improvement. This notebook provides an overview of evaluation parameters, their use cases, examples, and code implementations for different tasks.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d88868e",
      "metadata": {
        "id": "2d88868e"
      },
      "source": [
        "## Evaluation Parameters\n",
        "\n",
        "### 1. **Perplexity**\n",
        "- **What it is**: A measure of how well a probability model predicts a sample. Lower perplexity indicates better performance.\n",
        "- **When to use**: Suitable for language modeling tasks where the goal is to predict sequences of text.\n",
        "- **Formula**:\n",
        "  $$\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(x_i)}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e17b58",
      "metadata": {
        "id": "27e17b58"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def calculate_perplexity(probabilities):\n",
        "    n = len(probabilities)\n",
        "    log_prob_sum = sum(math.log2(p) for p in probabilities)\n",
        "    perplexity = 2 ** (-log_prob_sum / n)\n",
        "    return perplexity\n",
        "\n",
        "# Example usage\n",
        "probabilities = [0.1, 0.2, 0.3, 0.4]\n",
        "print(\"Perplexity:\", calculate_perplexity(probabilities))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7294dbb2",
      "metadata": {
        "id": "7294dbb2"
      },
      "source": [
        "### 2. **BLEU (Bilingual Evaluation Understudy)**\n",
        "- **What it is**: A metric for comparing a generated sequence against reference sequences using n-grams.\n",
        "- **When to use**: Commonly used in machine translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd0e847",
      "metadata": {
        "id": "cdd0e847"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "reference = [['this', 'is', 'a', 'test']]\n",
        "candidate = ['this', 'is', 'test']\n",
        "score = sentence_bleu(reference, candidate)\n",
        "print(\"BLEU score:\", score)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a370ca01",
      "metadata": {
        "id": "a370ca01"
      },
      "source": [
        "### 3. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
        "- **What it is**: Measures overlap between generated and reference texts.\n",
        "- **When to use**: Useful for summarization tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "lF_h_4zSvIMr"
      },
      "id": "lF_h_4zSvIMr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbddbce8",
      "metadata": {
        "id": "fbddbce8"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "hypothesis = \"The quick brown fox jumps over the lazy dog.\"\n",
        "reference = \"The fast brown fox leaps over the lazy dog.\"\n",
        "scores = rouge.get_scores(hypothesis, reference)\n",
        "print(\"ROUGE scores:\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25441a23",
      "metadata": {
        "id": "25441a23"
      },
      "source": [
        "### 4. **Accuracy**\n",
        "- **What it is**: Percentage of correct predictions.\n",
        "- **When to use**: Suitable for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d742e358",
      "metadata": {
        "id": "d742e358"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_true = [1, 0, 1, 1]\n",
        "y_pred = [1, 0, 0, 1]\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a9e7f6",
      "metadata": {
        "id": "74a9e7f6"
      },
      "source": [
        "### 5. **F1 Score**\n",
        "- **What it is**: Harmonic mean of precision and recall.\n",
        "- **When to use**: Appropriate for imbalanced datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77bcc000",
      "metadata": {
        "id": "77bcc000"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_true = [1, 0, 1, 1]\n",
        "y_pred = [1, 0, 0, 1]\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cc4e11",
      "metadata": {
        "id": "59cc4e11"
      },
      "source": [
        "### 6. **Exact Match (EM)**\n",
        "- **What it is**: Measures whether the generated output matches the reference exactly.\n",
        "- **When to use**: Best for tasks like question answering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c640b30d",
      "metadata": {
        "id": "c640b30d"
      },
      "outputs": [],
      "source": [
        "def exact_match_score(reference, prediction):\n",
        "    return int(reference == prediction)\n",
        "\n",
        "# Example usage\n",
        "reference = \"What is the capital of France?\"\n",
        "prediction = \"What is the capital of France?\"\n",
        "print(\"Exact Match:\", exact_match_score(reference, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338829da",
      "metadata": {
        "id": "338829da"
      },
      "source": [
        "### 7. **Human Evaluation**\n",
        "- **What it is**: Involves subjective feedback from humans.\n",
        "- **When to use**: Ideal for tasks where automated metrics fail to capture nuances, such as creative writing or conversational AI.\n",
        "- **Drawback**: Time-consuming and subjective."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "714bbd2f",
      "metadata": {
        "id": "714bbd2f"
      },
      "source": [
        "## Task-Based Evaluation\n",
        "\n",
        "### Task: **Text Generation**\n",
        "- **Evaluation Metric**: Perplexity, BLEU, Human Evaluation\n",
        "\n",
        "### Task: **Summarization**\n",
        "- **Evaluation Metric**: ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadafe2a",
      "metadata": {
        "id": "cadafe2a"
      },
      "outputs": [],
      "source": [
        "hypothesis = \"The quick brown fox.\"\n",
        "reference = \"The fast brown fox jumps.\"\n",
        "scores = rouge.get_scores(hypothesis, reference)\n",
        "print(\"ROUGE for Summarization:\", scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65250520",
      "metadata": {
        "id": "65250520"
      },
      "source": [
        "### Task: **Classification**\n",
        "- **Evaluation Metric**: Accuracy, F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c315815",
      "metadata": {
        "id": "0c315815"
      },
      "outputs": [],
      "source": [
        "y_true = [1, 0, 1, 1]\n",
        "y_pred = [1, 0, 0, 1]\n",
        "print(\"Accuracy for Classification:\", accuracy_score(y_true, y_pred))\n",
        "print(\"F1 Score for Classification:\", f1_score(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a77e8046",
      "metadata": {
        "id": "a77e8046"
      },
      "source": [
        "### Task: **Machine Translation**\n",
        "- **Evaluation Metric**: BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddc5747",
      "metadata": {
        "id": "9ddc5747"
      },
      "outputs": [],
      "source": [
        "print(\"BLEU score for Translation:\", sentence_bleu(reference, candidate))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef5588b",
      "metadata": {
        "id": "0ef5588b"
      },
      "source": [
        "### Task: **Question Answering**\n",
        "- **Evaluation Metric**: Exact Match, F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d85e0e",
      "metadata": {
        "id": "76d85e0e"
      },
      "outputs": [],
      "source": [
        "reference = \"Paris\"\n",
        "prediction = \"Paris\"\n",
        "print(\"Exact Match for QA:\", exact_match_score(reference, prediction))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "740995be",
      "metadata": {
        "id": "740995be"
      },
      "source": [
        "## Conclusion\n",
        "Selecting the right evaluation metric is essential for measuring the effectiveness of LLMs. This notebook provides examples and code for various tasks, helping you choose the most appropriate evaluation technique for your specific needs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}