{"cells":[{"cell_type":"markdown","id":"0d18b16f-fb05-48ab-a26c-ad1f2b644083","metadata":{"id":"0d18b16f-fb05-48ab-a26c-ad1f2b644083"},"source":["#**Demo: Data preparation**"]},{"cell_type":"markdown","source":["# **Description**\n","In this tutorial, you will walk through the process of preparing data for fine-tuning a LLM."],"metadata":{"id":"Qv6gIwj_toH8"},"id":"Qv6gIwj_toH8"},{"cell_type":"markdown","source":["# **Steps to perform:**\n","\n","1. Import necessary libraries\n","2. Load and prepare the dataset\n","3. Tokenize a single example\n","4. Handle long sequences\n","5. Tokenize the instruction dataset\n","6. Tokenize the entire dataset\n","7. Add labels\n","8. Prepare test/train splits\n","\n"],"metadata":{"id":"eIXu77jIqDmI"},"id":"eIXu77jIqDmI"},{"cell_type":"markdown","source":["# **Step 1: Import necessary libraries**\n"],"metadata":{"id":"0lEgedNVtubZ"},"id":"0lEgedNVtubZ"},{"cell_type":"code","execution_count":null,"id":"1fdcedcd-0918-463b-8ad0-631a3ccc56cb","metadata":{"id":"1fdcedcd-0918-463b-8ad0-631a3ccc56cb"},"outputs":[],"source":["import pandas as pd\n","#import datasets\n","\n","from pprint import pprint\n","from transformers import AutoTokenizer"]},{"cell_type":"code","execution_count":null,"id":"fdd939ab-c653-4cc6-abec-9efe7d2e6ea6","metadata":{"id":"fdd939ab-c653-4cc6-abec-9efe7d2e6ea6"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"]},{"cell_type":"markdown","id":"b75e2381-205d-4d72-a47f-a0e875fe52aa","metadata":{"id":"b75e2381-205d-4d72-a47f-a0e875fe52aa"},"source":["# **Step 2: Load and prepare the dataset**\n","\n"]},{"cell_type":"code","execution_count":null,"id":"eaf43db6-cd07-41c3-a3ba-f78f1ff6b5d7","metadata":{"id":"eaf43db6-cd07-41c3-a3ba-f78f1ff6b5d7","outputId":"63c2597c-61f5-463a-c8d4-c90c5c2dc475"},"outputs":[{"name":"stdout","output_type":"stream","text":["One datapoint in the finetuning dataset:\n","{'question': \"### Question:\\nWhat are the different types of documents available in the repository (e.g., installation guide, API documentation, developer's guide)?\\n\\n### Answer:\", 'answer': 'Lamini has documentation on Getting Started, Authentication, Question Answer Model, Python Library, Batching, Error Handling, Advanced topics, and class documentation on LLM Engine available at https://lamini-ai.github.io/.'}\n"]}],"source":["import pandas as pd\n","\n","data = \"lamini_docs.jsonl\"\n","instruction_dataset = pd.read_json(data, lines=True)\n","examples = instruction_dataset.to_dict()\n","\n","if \"question\" in examples and \"answer\" in examples:\n","  text = examples[\"question\"][0] + examples[\"answer\"][0]\n","elif \"instruction\" in examples and \"response\" in examples:\n","  text = examples[\"instruction\"][0] + examples[\"response\"][0]\n","elif \"input\" in examples and \"output\" in examples:\n","  text = examples[\"input\"][0] + examples[\"output\"][0]\n","else:\n","  text = examples[\"text\"][0]\n","\n","prompt_template = \"\"\"### Question:\n","{question}\n","\n","### Answer:\"\"\"\n","\n","num_examples = len(examples[\"question\"])\n","finetuning_data = []\n","for i in range(num_examples):\n","  question = examples[\"question\"][i]\n","  answer = examples[\"answer\"][i]\n","  text_with_prompt_template = prompt_template.format(question=question)\n","  finetuning_data.append({\"question\": text_with_prompt_template, \"answer\": answer})\n","\n","from pprint import pprint\n","print(\"One datapoint in the finetuning dataset:\")\n","print(finetuning_data[0])"]},{"cell_type":"markdown","id":"57d68dd2-e193-4ba3-b443-d49e5cb7792f","metadata":{"id":"57d68dd2-e193-4ba3-b443-d49e5cb7792f"},"source":["# **Step 3: Tokenize a single example**\n","\n","\n","*   Before tokenizing the entire dataset, first tokenize a single example to understand the process. Use the Pythia-70m tokenizer for this.\n"]},{"cell_type":"code","execution_count":null,"id":"180bbea0-0bbb-48bf-a824-004e5903e711","metadata":{"id":"180bbea0-0bbb-48bf-a824-004e5903e711","outputId":"83f6adf1-0d23-4f70-e366-331560f9347a"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 4118 19782    27   187  1276   403   253  1027  3510   273  7177  2130\n","    275   253 18491   313    70    15    72   904 12692  7102    13  8990\n","  10097    13 13722   434  7102  6177   187   187  4118 37741    27    45\n","   4988    74   556 10097   327 27669 11075   264    13  5271 23058    13\n","  19782 37741 10031    13 13814 11397    13   378 16464    13 11759 10535\n","   1981    13 21798 12989    13   285   966 10097   327 21708    46 10797\n","   2130   387  5987  1358    77  4988    74    14  2284    15  7280    15\n","    900 14206]]\n"]}],"source":["tokenizer.pad_token = tokenizer.eos_token\n","\n","text = finetuning_data[0][\"question\"] + finetuning_data[0][\"answer\"]\n","tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    padding=True\n",")\n","print(tokenized_inputs[\"input_ids\"])"]},{"cell_type":"markdown","source":["# **Step 4: Handle long sequences**\n","\n","\n","*   If the tokenized input is longer than the modelâ€™s maximum sequence length, you need to truncate it.\n","\n"],"metadata":{"id":"QpHz8gDG2x_9"},"id":"QpHz8gDG2x_9"},{"cell_type":"code","execution_count":null,"id":"be030ad2-933f-4abd-9fec-d45df984890b","metadata":{"id":"be030ad2-933f-4abd-9fec-d45df984890b"},"outputs":[],"source":["max_length = 2048\n","max_length = min(\n","    tokenized_inputs[\"input_ids\"].shape[1],\n","    max_length,\n",")"]},{"cell_type":"code","execution_count":null,"id":"e6b92295-9f1b-4358-a5e6-d3522fb61fbc","metadata":{"id":"e6b92295-9f1b-4358-a5e6-d3522fb61fbc"},"outputs":[],"source":["tokenized_inputs = tokenizer(\n","    text,\n","    return_tensors=\"np\",\n","    truncation=True,\n","    max_length=max_length\n",")"]},{"cell_type":"code","execution_count":null,"id":"7f103d80-de85-4c56-82a9-55bee16c5a56","metadata":{"id":"7f103d80-de85-4c56-82a9-55bee16c5a56","outputId":"c40f2f44-9940-4abf-a488-6ea9339f11b3"},"outputs":[{"data":{"text/plain":["array([[ 4118, 19782,    27,   187,  1276,   403,   253,  1027,  3510,\n","          273,  7177,  2130,   275,   253, 18491,   313,    70,    15,\n","           72,   904, 12692,  7102,    13,  8990, 10097,    13, 13722,\n","          434,  7102,  6177,   187,   187,  4118, 37741,    27,    45,\n","         4988,    74,   556, 10097,   327, 27669, 11075,   264,    13,\n","         5271, 23058,    13, 19782, 37741, 10031,    13, 13814, 11397,\n","           13,   378, 16464,    13, 11759, 10535,  1981,    13, 21798,\n","        12989,    13,   285,   966, 10097,   327, 21708,    46, 10797,\n","         2130,   387,  5987,  1358,    77,  4988,    74,    14,  2284,\n","           15,  7280,    15,   900, 14206]])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["tokenized_inputs[\"input_ids\"]"]},{"cell_type":"markdown","id":"84847e29-0870-4086-9074-17f376e87b4e","metadata":{"id":"84847e29-0870-4086-9074-17f376e87b4e"},"source":["# **Step 5: Tokenize the instruction dataset**\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"6a4f1144-88f2-4cf7-9120-676db70748a4","metadata":{"id":"6a4f1144-88f2-4cf7-9120-676db70748a4"},"outputs":[],"source":["def tokenize_function(examples):\n","    if \"question\" in examples and \"answer\" in examples:\n","      text = examples[\"question\"][0] + examples[\"answer\"][0]\n","    elif \"input\" in examples and \"output\" in examples:\n","      text = examples[\"input\"][0] + examples[\"output\"][0]\n","    else:\n","      text = examples[\"text\"][0]\n","\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        padding=True,\n","    )\n","\n","    max_length = min(\n","        tokenized_inputs[\"input_ids\"].shape[1],\n","        2048\n","    )\n","    tokenizer.truncation_side = \"left\"\n","    tokenized_inputs = tokenizer(\n","        text,\n","        return_tensors=\"np\",\n","        truncation=True,\n","        max_length=max_length\n","    )\n","\n","    return tokenized_inputs"]},{"cell_type":"markdown","source":["# **Step 6: Tokenize the entire dataset**\n","\n"],"metadata":{"id":"LEozM9XU3Ee8"},"id":"LEozM9XU3Ee8"},{"cell_type":"code","execution_count":null,"id":"91374faf-4e08-4223-a3a9-d8ccafc77679","metadata":{"colab":{"referenced_widgets":["363acfe48b404bc48a3613ce67739d7d"]},"id":"91374faf-4e08-4223-a3a9-d8ccafc77679","outputId":"31f84dde-d40e-4243-db4c-cf9062f66a90"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"363acfe48b404bc48a3613ce67739d7d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1400 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n","    num_rows: 1400\n","})\n"]}],"source":["from datasets import load_dataset\n","\n","finetuning_data = load_dataset(\"json\", data_files=data, split=\"train\")\n","\n","tokenized_dataset = finetuning_data.map(\n","    tokenize_function,\n","    batched=True,\n","    batch_size=1,\n","    drop_last_batch=True\n",")\n","\n","print(tokenized_dataset)"]},{"cell_type":"markdown","source":["# **Step 8: Add labels**\n","\n"],"metadata":{"id":"S7lIOSGi3OxQ"},"id":"S7lIOSGi3OxQ"},{"cell_type":"code","execution_count":null,"id":"1d505bc8-dbdf-40f1-81a7-af7cf0352511","metadata":{"id":"1d505bc8-dbdf-40f1-81a7-af7cf0352511"},"outputs":[],"source":["tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])"]},{"cell_type":"markdown","id":"3c32c16a-8a5d-470d-a114-c7e3a32e9aa1","metadata":{"id":"3c32c16a-8a5d-470d-a114-c7e3a32e9aa1"},"source":["# **Step 9: Prepare test/train splits**\n","\n"]},{"cell_type":"code","execution_count":null,"id":"11623660-5adc-48c7-be13-fadad0eb2ab3","metadata":{"id":"11623660-5adc-48c7-be13-fadad0eb2ab3","outputId":"33b2b7f3-f0b2-4177-d6f9-5b7ee787c926"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 1260\n","    })\n","    test: Dataset({\n","        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 140\n","    })\n","})\n"]}],"source":["split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n","print(split_dataset)"]},{"cell_type":"markdown","source":["\n","\n","# **Conclusion:**\n","This concludes the data preparation process for fine-tuning a Language Learning Model. The next steps would involve setting up the model, fine-tuning it on the training data, and evaluating its performance on the test data.\n"],"metadata":{"id":"f7Z1DqTC3fD5"},"id":"f7Z1DqTC3fD5"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}