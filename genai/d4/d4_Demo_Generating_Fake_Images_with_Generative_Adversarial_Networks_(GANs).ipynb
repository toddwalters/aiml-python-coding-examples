{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFjrWwbwmm--"
   },
   "source": [
    "# __Demo: Generating Fake Images with Generative Adversarial Networks (GANs)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zAosTOTB-qP"
   },
   "source": [
    "# __Steps to Perform__\n",
    "\n",
    "Step 1: Import the Necessary Libraries\n",
    "\n",
    "Step 2: Load and Preprocess the Data\n",
    "\n",
    "Step 3: Build the Generator and Discriminator\n",
    "\n",
    "Step 4: Compile the Models\n",
    "\n",
    "Step 5: Train the Models\n",
    "\n",
    "Step 6: Execute the Training\n",
    "\n",
    "Step 7: Generate New Images and Evaluate the Model's Performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWSHe4DSf-eN"
   },
   "source": [
    "# __Step 1: Import the Necessary Libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MJUz4u4rdcOQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set TF log level to only errors (3) or warnings (2)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "import tensorrt\n",
    "import tensorrt_lean as trt\n",
    "import tensorrt_dispatch as trt_dis\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('LD_LIBRARY_PATH set to: \\n',os.environ.get('LD_LIBRARY_PATH'), '\\n')\n",
    "# print('PATH set to: \\n', os.environ.get('PATH'), '\\n')\n",
    "\n",
    "print(f'Tensorflow Verion: {tf.__version__}')   # Should be 2.13.0\n",
    "print(f'List of GPUs: {tf.config.list_physical_devices()}', '\\n')\n",
    "\n",
    "print(f'Keras Version: {keras.__version__}', '\\n')\n",
    "\n",
    "print('TensorRT Version: ', tensorrt.__version__)\n",
    "print('TensorRT Lean Version: ', trt.__version__)\n",
    "print('TensorRT Lean Version: ', trt_dis.__version__, '\\n')\n",
    "\n",
    "print(\"PyTorch CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN enabled:\", torch.backends.cudnn.enabled, '\\n')\n",
    "\n",
    "\n",
    "# !python -c \"import tensorflow as tf; import keras; import tensorrt; import tensorrt_lean as trt; import tensorrt_dispatch as trt_dis; print('Tensorflow Version: ', tf.__version__); print('List of GPUs: ', tf.config.list_physical_devices(\\\"GPU\\\")); print('Keras Version: ', keras.__version__); print('TensorRT Version: ', tensorrt.__version__); print('TensorRT Lean Version: ', trt.__version__); print('TensorRT Lean Version: ', trt_dis.__version__)\"\n",
    "# !python -c \"import torch; print('PyTorch CUDA available: ', torch.cuda.is_available()); print('cuDNN enabled: ', torch.backends.cudnn.enabled)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NlLSckbgHt0"
   },
   "source": [
    "# __Step 2: Load and Preprocess the Data__\n",
    "\n",
    "- Load the MNIST dataset and preprocess it.\n",
    "- Preprocessing involves normalizing the data that can improve models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gTjuO07Mdoe5"
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "# Normalize to between -1 and 1\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 2 Code Breakdown__\n",
    "\n",
    "The provided code snippet demonstrates how to load the MNIST dataset using TensorFlow and Keras. The MNIST dataset is a well-known dataset in the machine learning community, consisting of 70,000 grayscale images of handwritten digits (0-9), each of size 28x28 pixels. This dataset is commonly used for training and evaluating image classification models, as well as for generative tasks such as generating fake images with Generative Adversarial Networks (GANs).\n",
    "\n",
    "1. **Loading the MNIST Dataset:**\n",
    "   ```python\n",
    "   (X_train, _), (_, _) = mnist.load_data()\n",
    "   ```\n",
    "   \n",
    "   The `mnist.load_data()` function loads the MNIST dataset and returns two tuples: `(X_train, y_train)` and `(X_test, y_test)`. These tuples contain the training and testing data, respectively. The `X_train` and `X_test` variables contain the images, while the `y_train` and `y_test` variables contain the corresponding labels.\n",
    "\n",
    "2. **Ignoring Unnecessary Data:**\n",
    "   ```python\n",
    "   (X_train, _), (_, _)\n",
    "   ```\n",
    "\n",
    "   In this snippet, the labels (`y_train` and `y_test`) are not needed, so they are ignored using the underscore `_`. This is a common practice in Python when you want to ignore certain values returned by a function. The focus here is on the training images (`X_train`), which will be used for further processing or training a model.\n",
    "\n",
    "### __Example Context__\n",
    "\n",
    "Let's consider an example where you are working on a project to generate fake images using Generative Adversarial Networks (GANs). In this context, you only need the training images from the MNIST dataset to train the GAN. The labels are not required because the GAN focuses on generating images rather than classifying them.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Print the shape of the training data\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "```\n",
    "\n",
    "### __Output__\n",
    "\n",
    "The output of the above code will display the shape of the training data:\n",
    "\n",
    "```\n",
    "Training data shape: (60000, 28, 28)\n",
    "```\n",
    "\n",
    "This indicates that the `X_train` variable contains 60,000 training images, each of size 28x28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDoEhcHcgRmZ"
   },
   "source": [
    "# __Step 3: Build the Generator and Discriminator__\n",
    "\n",
    "- Define the generator and discriminator models.\n",
    "- Generator takes a random noise vector as input and outputs an image.\n",
    "- Discriminator takes an image as input and outputs the probability of the image being real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l0d5rZYeeUmP"
   },
   "outputs": [],
   "source": [
    "# Generator\n",
    "def create_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=100, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(784, activation='tanh'))\n",
    "    model.add(Reshape((28, 28, 1)))\n",
    "    return model\n",
    "\n",
    "# Discriminator\n",
    "def create_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 3 Code Breakdown__\n",
    "\n",
    "### __Generator Function Definition__\n",
    "\n",
    "The provided code snippet defines a function `create_generator()` that constructs and returns a generator model for a Generative Adversarial Network (GAN) using the Keras Sequential API. In the context of GANs, the generator is responsible for creating fake images that resemble real images from the training dataset. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "```python\n",
    "def create_generator():\n",
    "```\n",
    "\n",
    "The `create_generator()` function is defined to create and return a generator model. This function encapsulates the architecture of the generator, making it easy to instantiate and use in the GAN training process.\n",
    "\n",
    "#### __Model Initialization__\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "The generator model is initialized as a Sequential model. The Sequential API in Keras allows you to build a neural network layer by layer in a straightforward manner.\n",
    "\n",
    "#### __Adding Layers to the Generator__\n",
    "\n",
    "1. **First Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(256, input_dim=100, activation='relu'))\n",
    "   ```\n",
    "   The first layer is a dense (fully connected) layer with 256 neurons. The `input_dim=100` parameter specifies that the input to this layer is a 100-dimensional vector, which is typically a random noise vector sampled from a standard normal distribution. The ReLU activation function is used to introduce non-linearity.\n",
    "\n",
    "2. **Second Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(512, activation='relu'))\n",
    "   ```\n",
    "   The second layer is another dense layer with 512 neurons and ReLU activation. This layer further processes the output of the previous layer, allowing the model to learn more complex features.\n",
    "\n",
    "3. **Third Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(1024, activation='relu'))\n",
    "   ```\n",
    "   The third layer is a dense layer with 1024 neurons and ReLU activation. This layer continues to increase the complexity and capacity of the model, enabling it to generate more detailed and realistic images.\n",
    "\n",
    "4. **Output Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(784, activation='tanh'))\n",
    "   ```\n",
    "   The output layer is a dense layer with 784 neurons and a tanh activation function. The number of neurons (784) corresponds to the total number of pixels in a 28x28 image (28 * 28 = 784). The tanh activation function is used to scale the output values to the range [-1, 1], which is suitable for image data.\n",
    "\n",
    "5. **Reshape Layer:**\n",
    "   ```python\n",
    "   model.add(Reshape((28, 28, 1)))\n",
    "   ```\n",
    "   The Reshape layer reshapes the 1D output vector (784) into a 2D image of shape (28, 28, 1). The last dimension (1) indicates that the image is grayscale. This reshaping is necessary to convert the output of the dense layers into the format of an image.\n",
    "\n",
    "#### __Returning the Model__\n",
    "\n",
    "```python\n",
    "return model\n",
    "```\n",
    "\n",
    "The `create_generator()` function returns the constructed generator model. This model can then be used in the GAN training process to generate fake images.\n",
    "\n",
    "#### __Summary__\n",
    "\n",
    "The `create_generator()` function constructs and returns a generator model for a Generative Adversarial Network (GAN) using the Keras Sequential API. The generator model consists of several dense layers with ReLU activation, followed by an output dense layer with tanh activation and a Reshape layer. The input to the generator is a 100-dimensional random noise vector, and the output is a 28x28 grayscale image. This architecture allows the generator to learn to produce realistic images that resemble the real images from the training dataset. The generator model is a crucial component of the GAN, working in tandem with the discriminator to improve the quality of the generated images through the adversarial training process.\n",
    "\n",
    "### __Discriminator Function Definition__\n",
    "\n",
    "The provided code snippet defines a function `create_discriminator()` that constructs and returns a discriminator model for a Generative Adversarial Network (GAN) using the Keras Sequential API. In the context of GANs, the discriminator is responsible for distinguishing between real images from the training dataset and fake images generated by the generator. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "```python\n",
    "def create_discriminator():\n",
    "```\n",
    "\n",
    "The `create_discriminator()` function is defined to create and return a discriminator model. This function encapsulates the architecture of the discriminator, making it easy to instantiate and use in the GAN training process.\n",
    "\n",
    "#### __Model Initialization__\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "\n",
    "The discriminator model is initialized as a Sequential model. The Sequential API in Keras allows you to build a neural network layer by layer in a straightforward manner.\n",
    "\n",
    "#### __Adding Layers to the Discriminator__\n",
    "\n",
    "1. **Flatten Layer:**\n",
    "   ```python\n",
    "   model.add(Flatten(input_shape=(28, 28, 1)))\n",
    "   ```\n",
    "   The first layer is a `Flatten` layer that reshapes the 2D input images (28x28 pixels with 1 channel) into 1D vectors. This transformation is necessary to feed the image data into the subsequent dense layers.\n",
    "\n",
    "2. **First Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(1024, activation='relu'))\n",
    "   ```\n",
    "   The first dense (fully connected) layer has 1024 neurons and uses the ReLU activation function. This layer learns to extract high-level features from the flattened input data.\n",
    "\n",
    "3. **Second Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(512, activation='relu'))\n",
    "   ```\n",
    "   The second dense layer has 512 neurons and also uses the ReLU activation function. This layer further processes the features extracted by the previous layer, allowing the model to learn more complex patterns.\n",
    "\n",
    "4. **Third Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(256, activation='relu'))\n",
    "   ```\n",
    "   The third dense layer has 256 neurons and uses the ReLU activation function. This layer continues to increase the complexity and capacity of the model, enabling it to learn more detailed features.\n",
    "\n",
    "5. **Output Dense Layer:**\n",
    "   ```python\n",
    "   model.add(Dense(1, activation='sigmoid'))\n",
    "   ```\n",
    "   The output layer is a dense layer with a single neuron and a sigmoid activation function. The sigmoid activation function outputs a value between 0 and 1, representing the probability that the input image is real. A value close to 1 indicates that the image is likely real, while a value close to 0 indicates that the image is likely fake.\n",
    "\n",
    "#### __Returning the Model__\n",
    "\n",
    "```python\n",
    "return model\n",
    "```\n",
    "\n",
    "The `create_discriminator()` function returns the constructed discriminator model. This model can then be used in the GAN training process to classify images as real or fake.\n",
    "\n",
    "#### __Summary__\n",
    "\n",
    "The `create_discriminator()` function constructs and returns a discriminator model for a Generative Adversarial Network (GAN) using the Keras Sequential API. The discriminator model consists of a Flatten layer to reshape the input images, followed by three dense layers with ReLU activation, and an output dense layer with a sigmoid activation function. The input to the discriminator is a 28x28 grayscale image, and the output is a single value representing the probability that the image is real. This architecture allows the discriminator to learn to distinguish between real images from the training dataset and fake images generated by the generator. The discriminator model is a crucial component of the GAN, working in tandem with the generator to improve the quality of the generated images through the adversarial training process.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPpH5pVQgYic"
   },
   "source": [
    "# __Step 4: Compile the Models__\n",
    "\n",
    "- Compile the models, which involves defining the loss function and the optimizer.\n",
    "- The loss function evaluates the model's performance, while the optimizer aims to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KY1zu283eg0C"
   },
   "outputs": [],
   "source": [
    "# Create and compile the discriminator\n",
    "discriminator = create_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Create and compile the generator\n",
    "generator = create_generator()\n",
    "# generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "# Create and compile the combined model\n",
    "discriminator.trainable = False\n",
    "gan_input = Input(shape=(100,))\n",
    "x = generator(gan_input)\n",
    "gan_output = discriminator(x)\n",
    "gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 4 Code Breakdown__\n",
    "\n",
    "The provided code snippet demonstrates how to create and compile the components of a Generative Adversarial Network (GAN) using TensorFlow and Keras. The GAN consists of a generator and a discriminator, which are trained together in an adversarial manner. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "1. **Importing Required Modules:**\n",
    "   ```python\n",
    "   from tensorflow.keras.models import Sequential, Model\n",
    "   from tensorflow.keras.layers import Input\n",
    "   ```\n",
    "\n",
    "   These lines import the necessary modules from TensorFlow and Keras. The `Sequential` and `Model` classes are used to define the generator and discriminator models, while the `Input` class is used to define the input layer for the combined GAN model.\n",
    "\n",
    "2. **Creating and Compiling the Discriminator:**\n",
    "   ```python\n",
    "   discriminator = create_discriminator()\n",
    "   discriminator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "   ```\n",
    "   \n",
    "   The `create_discriminator()` function is called to create the discriminator model. The discriminator is then compiled with the binary cross-entropy loss function and the Adam optimizer. The binary cross-entropy loss function is suitable for binary classification tasks, where the discriminator distinguishes between real and fake images.\n",
    "\n",
    "3. **Creating and Compiling the Generator:**\n",
    "   ```python\n",
    "   generator = create_generator()\n",
    "   # generator.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "   ```\n",
    "   \n",
    "   The `create_generator()` function is called to create the generator model. In the context of training a Generative Adversarial Network (GAN), you do not need to compile the generator model separately with a generator.compile statement. Instead, you compile the combined GAN model, which includes both the generator and the discriminator. The generator's weights are updated through the GAN model during training.\n",
    "\n",
    "4. **Creating and Compiling the Combined GAN Model:**\n",
    "   ```python\n",
    "   discriminator.trainable = False\n",
    "   gan_input = Input(shape=(100,))\n",
    "   x = generator(gan_input)\n",
    "   gan_output = discriminator(x)\n",
    "   gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "   gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "   ```\n",
    "   - **Freezing the Discriminator:** `discriminator.trainable = False`ensures that the discriminator's weights are not updated during the training of the combined GAN model. This is important because, during GAN training, only the generator's weights should be updated when training the combined model.\n",
    "   - **Defining the GAN Input:** `gan_input = Input(shape=(100,))` defines the input layer for the combined GAN model. The input is a 100-dimensional random noise vector.\n",
    "   - **Generating Fake Images:** `x = generator(gan_input)` passes the random noise vector through the generator to produce fake images.\n",
    "   - **Discriminating Fake Images:** `gan_output = discriminator(x)` passes the generated fake images through the discriminator to obtain the classification output.\n",
    "   - **Creating the GAN Model:** `gan = Model(inputs=gan_input, outputs=gan_output)` defines the combined GAN model, which takes the random noise vector as input and produces the discriminator's classification output.\n",
    "   - **Compiling the GAN Model:** `gan.compile(loss='binary_crossentropy', optimizer='adam')` compiles the combined GAN model with the binary cross-entropy loss function and the Adam optimizer. The loss function measures how well the discriminator classifies the generated images as fake.\n",
    "\n",
    "### __Summary__\n",
    "\n",
    "The provided code snippet demonstrates how to create and compile the components of a Generative Adversarial Network (GAN) using TensorFlow and Keras. The `create_discriminator()` and `create_generator()` functions define the architectures of the discriminator and generator models, respectively. The discriminator is compiled with the binary cross-entropy loss function and the Adam optimizer. The generator is also compiled, although its loss function is not used directly in the GAN training process. The combined GAN model is created by connecting the generator and discriminator, and it is compiled with the binary cross-entropy loss function and the Adam optimizer. This setup allows the GAN to be trained in an adversarial manner, where the generator learns to produce realistic images that can fool the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY3QwDoXgcPs"
   },
   "source": [
    "# __Step 5: Train the Models__\n",
    "\n",
    "- Train the model, which involves feeding data into the models and adjusting the weights of the models based on the output.\n",
    "- The primary aim is for the generator to create images indistinguishable from real images by the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IE9tbYPdenYp"
   },
   "outputs": [],
   "source": [
    "def train(epochs=1, batch_size=128):\n",
    "    # Load the data\n",
    "    (X_train, _), (_, _) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Labels for the batch size and the test size\n",
    "    y_train_ones = np.ones((batch_size, 1))\n",
    "    y_train_zeros = np.zeros((batch_size, 1))\n",
    "    y_test_ones = np.ones((100, 1))\n",
    "\n",
    "    # Start training\n",
    "    for e in range(epochs):\n",
    "        for i in range(X_train.shape[0] // batch_size):\n",
    "            # Train Discriminator weights\n",
    "            discriminator.trainable = True\n",
    "\n",
    "            # Real samples\n",
    "            X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "            d_loss_real = discriminator.train_on_batch(x=X_batch, y=y_train_ones * (1 - 0.1 * np.random.rand(batch_size, 1)))\n",
    "\n",
    "            # Fake Samples\n",
    "            z_noise = np.random.normal(loc=0, scale=1, size=(batch_size, 100))\n",
    "            X_fake = generator.predict_on_batch(z_noise)\n",
    "            d_loss_fake = discriminator.train_on_batch(x=X_fake, y=y_train_zeros)\n",
    "\n",
    "            # Discriminator loss\n",
    "            d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "\n",
    "            # Train Generator weights\n",
    "            discriminator.trainable = False\n",
    "            g_loss = gan.train_on_batch(x=z_noise, y=y_train_ones)\n",
    "\n",
    "            print(f'Epoch: {e+1}, Batch: {i}, D Loss: {d_loss}, G Loss: {g_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Step 5 Code Breakdown__\n",
    "\n",
    "The provided code snippet defines a `train` function that trains a Generative Adversarial Network (GAN) using the MNIST dataset. The GAN consists of a generator and a discriminator, which are trained together in an adversarial manner. The goal of the generator is to produce realistic fake images, while the discriminator aims to distinguish between real and fake images. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "### __Function Definition__\n",
    "\n",
    "```python\n",
    "def train(epochs=1, batch_size=128):\n",
    "```\n",
    "\n",
    "The `train` function is defined to train the GAN for a specified number of epochs and batch size. The default values are set to 1 epoch and a batch size of 128.\n",
    "\n",
    "### __Loading and Preprocessing the Data__\n",
    "\n",
    "```python\n",
    "# Load the data\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "```\n",
    "\n",
    "1. **Loading the Data:** The MNIST dataset is loaded using the `mnist.load_data()` function. The dataset is split into training and testing sets, but only the training images (`X_train`) are used in this function. The labels are ignored using the underscore `_`.\n",
    "\n",
    "2. **Normalizing the Data:** The pixel values of the training images are normalized to the range [-1, 1] by subtracting 127.5 and dividing by 127.5. This normalization helps improve the performance and convergence of the GAN.\n",
    "\n",
    "3. **Expanding Dimensions:** The training images are reshaped to add an extra dimension, making them suitable for the input shape expected by the discriminator. The new shape is (28, 28, 1), where the last dimension represents the single color channel (grayscale).\n",
    "\n",
    "### __Defining Labels for Training__\n",
    "\n",
    "```python\n",
    "# Labels for the batch size and the test size\n",
    "y_train_ones = np.ones((batch_size, 1))\n",
    "y_train_zeros = np.zeros((batch_size, 1))\n",
    "y_test_ones = np.ones((100, 1))\n",
    "```\n",
    "\n",
    "1. **Real Labels:** `y_train_ones` is a batch of labels with the value 1, representing real images. These labels are used when training the discriminator with real images.\n",
    "\n",
    "2. **Fake Labels:** `y_train_zeros` is a batch of labels with the value 0, representing fake images. These labels are used when training the discriminator with fake images generated by the generator.\n",
    "\n",
    "3. **Test Labels:** `y_test_ones` is a batch of labels with the value 1, used for testing the generator. This is not used in the provided code snippet but can be useful for evaluating the generator's performance.\n",
    "\n",
    "### __Training Loop__\n",
    "\n",
    "```python\n",
    "# Start training\n",
    "for e in range(epochs):\n",
    "    for i in range(X_train.shape[0] // batch_size):\n",
    "        # Train Discriminator weights\n",
    "        discriminator.trainable = True\n",
    "        # Real samples\n",
    "        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n",
    "        d_loss_real = discriminator.train_on_batch(x=X_batch, y=y_train_ones * (1 - 0.1 * np.random.rand(batch_size, 1)))\n",
    "        # Fake Samples\n",
    "        z_noise = np.random.normal(loc=0, scale=1, size=(batch_size, 100))\n",
    "        X_fake = generator.predict_on_batch(z_noise)\n",
    "        d_loss_fake = discriminator.train_on_batch(x=X_fake, y=y_train_zeros)\n",
    "        # Discriminator loss\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        # Train Generator weights\n",
    "        discriminator.trainable = False\n",
    "        g_loss = gan.train_on_batch(x=z_noise, y=y_train_ones)\n",
    "        print(f'Epoch: {e+1}, Batch: {i}, D Loss: {d_loss}, G Loss: {g_loss}')\n",
    "```\n",
    "\n",
    "1. **Epoch Loop:** The outer loop iterates over the specified number of epochs.\n",
    "\n",
    "2. **Batch Loop:** The inner loop iterates over the training data in batches. The batch size is specified by the `batch_size` parameter.\n",
    "\n",
    "3. **Training the Discriminator:**\n",
    "   - **Real Samples:** A batch of real images is selected from the training data. The discriminator is trained on these real images with labels `y_train_ones`, slightly perturbed by random noise to improve training stability.\n",
    "   - **Fake Samples:** A batch of random noise vectors is generated and passed through the generator to produce fake images. The discriminator is trained on these fake images with labels `y_train_zeros`.\n",
    "   - **Discriminator Loss:** The loss for the discriminator is calculated as the average of the losses on real and fake images.\n",
    "\n",
    "4. **Training the Generator:**\n",
    "   - **Freezing the Discriminator:** The discriminator's weights are frozen to ensure that only the generator's weights are updated during this step.\n",
    "   - **Generator Training:** The generator is trained to produce images that the discriminator classifies as real. This is done by training the GAN model with the random noise vectors and labels `y_train_ones`.\n",
    "\n",
    "5. **Logging the Losses:** The losses for the discriminator and generator are printed for each batch, providing insight into the training progress.\n",
    "\n",
    "### __Summary__\n",
    "\n",
    "The `train` function trains a Generative Adversarial Network (GAN) using the MNIST dataset. The function loads and preprocesses the data, defines labels for real and fake images, and iterates over the specified number of epochs and batches. During each iteration, the discriminator is trained on real and fake images, and the generator is trained to produce images that the discriminator classifies as real. The losses for the discriminator and generator are logged for each batch, providing insight into the training progress. This training process allows the GAN to learn to generate realistic images that resemble the real images from the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFwC_WhRgo8q"
   },
   "source": [
    "# __Step 6: Execute the Training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74Qf18jCes5z",
    "outputId": "7182133e-b227-4f5e-b03d-ad8289a1d0df"
   },
   "outputs": [],
   "source": [
    "# Call the train function\n",
    "train(epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I09QJRDIlZqo"
   },
   "source": [
    "**Notes:**\n",
    "- Epochs parameter determines how many times the learning algorithm will work through the entire training dataset.\n",
    "- The `batch_size` is the number of samples that will be propagated through the network at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtmQnlKeg9C8"
   },
   "source": [
    "# __Step 7: Generate New Images and Evaluate the Model's Performance__\n",
    "\n",
    "- Generate new images and evaluate the performance of the GAN.\n",
    "- Generate a random noise vector and feed it into the trained generator to create new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "oIlF3BEEe45G",
    "outputId": "7d69ba3f-0d68-4aa0-dfed-75515819180f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Generate random noise as an input to initialize the generator\n",
    "random_noise = np.random.normal(0,1, [100, 100])\n",
    "\n",
    "# Generate the images from the noise\n",
    "generated_images = generator.predict(random_noise)\n",
    "\n",
    "# Visualize the generated images+\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(generated_images.shape[0]):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzI9y3KSlwMp"
   },
   "source": [
    "- The resulting plot shows the images generated by the GAN model.\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "The output effectively demonstrates how model can generate images resembling handwritten digit 8. However, there is room for improving the quality of these images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ic6p2uAFJMDl"
   },
   "source": [
    "# __Conclusion__\n",
    "\n",
    "In this demo, you have successfully implemented a GAN to generate images resembling handwritten digits, focusing on the MNIST dataset. The process involved constructing and training a generator and a discriminator. The results were promising, showcasing the GAN's ability to create images similar to the digit **8**, but also highlighted the need for further improvements in image quality."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gan-genai-linux-20241209-v20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
