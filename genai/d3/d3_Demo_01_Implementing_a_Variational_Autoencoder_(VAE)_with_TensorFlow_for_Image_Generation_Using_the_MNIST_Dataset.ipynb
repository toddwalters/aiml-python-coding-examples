{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. <a id='toc1_'></a>[__Demo: Implementing a Variational Autoencoder (VAE) with TensorFlow for Image Generation Using the MNIST Dataset__](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- 1. [__Demo: Implementing a Variational Autoencoder (VAE) with TensorFlow for Image Generation Using the MNIST Dataset__](#toc1_)    \n",
    "- 2. [__Steps to Perform__](#toc2_)    \n",
    "- 3. [__Step 1: Import the Necessary Libraries__](#toc3_)    \n",
    "- 4. [__Step 2: Load the MNIST Dataset__](#toc4_)    \n",
    "  - 4.1. [__Step 2 Code Explanation__](#toc4_1_)    \n",
    "    - 4.1.1. [__Code Breakdown__](#toc4_1_1_)    \n",
    "    - 4.1.2. [__Summary__](#toc4_1_2_)    \n",
    "- 5. [__Step 3: Set Hyperparameters__](#toc5_)    \n",
    "  - 5.1. [__Step 3 Code Explanation__](#toc5_1_)    \n",
    "    - 5.1.1. [__Code Breakdown__](#toc5_1_1_)    \n",
    "    - 5.1.2. [__Summary__](#toc5_1_2_)    \n",
    "- 6. [__Step 4: Define Model Architecture__](#toc6_)    \n",
    "  - 6.1. [__Step 4 Explanation__](#toc6_1_)    \n",
    "    - 6.1.1. [__Code Breakdown__](#toc6_1_1_)    \n",
    "      - 6.1.1.1. [__Latent Space Dimension__](#toc6_1_1_1_)    \n",
    "      - 6.1.1.2. [__Encoder Definition__](#toc6_1_1_2_)    \n",
    "      - 6.1.1.3. [__Decoder Definition__](#toc6_1_1_3_)    \n",
    "    - 6.1.2. [__Summary__](#toc6_1_2_)    \n",
    "  - 6.2. [__When to Use a `Flatten` Layer__](#toc6_2_)    \n",
    "  - 6.3. [__Explanation of the Choice of Neurons and Activation Function__](#toc6_3_)    \n",
    "- 7. [__Step 5: Define the Sampling Function__](#toc7_)    \n",
    "  - 7.1. [__Step 5 Code Explanation__](#toc7_1_)    \n",
    "    - 7.1.1. [__Code Breakdown__](#toc7_1_1_)    \n",
    "    - 7.1.2. [__Summary__](#toc7_1_2_)    \n",
    "- 8. [__Step 6: Connect the Encoder and the Decoder__](#toc8_)    \n",
    "  - 8.1. [__Step 6 Code Explanation__](#toc8_1_)    \n",
    "    - 8.1.1. [__Code Breakdown__](#toc8_1_1_)    \n",
    "    - 8.1.2. [__Summary__](#toc8_1_2_)    \n",
    "- 9. [__Step 7: Define the Loss Function and Compile the Model__](#toc9_)    \n",
    "  - 9.1. [__Step 7 Code Explanation__](#toc9_1_)    \n",
    "    - 9.1.1. [__Summary__](#toc9_1_1_)    \n",
    "- 10. [__Step 8: Train the Model__](#toc10_)    \n",
    "- 11. [__Step 9: Generate a Manifold of Digits__](#toc11_)    \n",
    "- 12. [__Conclusion__](#toc12_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=true\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. <a id='toc2_'></a>[__Steps to Perform__](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17rpQjjgtnOu"
   },
   "source": [
    "- Step 1: Import the Necessary Libraries\n",
    "\n",
    "- Step 2: Load the MNIST Dataset\n",
    "\n",
    "- Step 3: Set Hyperparameters\n",
    "\n",
    "- Step 4: Define Model Architecture\n",
    "\n",
    "- Step 5: Define the Sampling Function\n",
    "\n",
    "- Step 6: Connect the Encoder and Decoder\n",
    "\n",
    "- Step 7: Define the Loss Function and Compile the Model\n",
    "\n",
    "- Step 8: Train the Model\n",
    "\n",
    "- Step 9: Generate a Manifold of Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. <a id='toc3_'></a>[__Step 1: Import the Necessary Libraries__](#toc0_)\n",
    "- Import numpy, matplotlib.pyplot, and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "OKSPmDg6EL4s",
    "outputId": "9c7b9755-ebfc-4518-f652-2fe9f8b8e46b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 18:03:33.145814: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-27 18:03:35.465891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:35.494702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:35.496444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOC-NOTICE: GPU memory for this assignment is capped at 2048MiB\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. <a id='toc4_'></a>[__Step 2: Load the MNIST Dataset__](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8XGexe6vFfi_"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. <a id='toc4_1_'></a>[__Step 2 Code Explanation__](#toc0_)\n",
    "\n",
    "### 4.1.1. <a id='toc4_1_1_'></a>[__Code Breakdown__](#toc0_)\n",
    "\n",
    "The provided code snippet demonstrates how to load and preprocess the MNIST dataset using TensorFlow and Keras. The MNIST dataset is a well-known dataset in the machine learning community, consisting of 70,000 grayscale images of handwritten digits (0-9), each of size 28x28 pixels. This dataset is commonly used for training and evaluating image classification models.\n",
    "\n",
    "1. **Loading the MNIST Dataset:**\n",
    "   ```python\n",
    "   mnist = tf.keras.datasets.mnist\n",
    "   ```\n",
    "   This line imports the MNIST dataset from the `tf.keras.datasets` module. TensorFlow provides easy access to several popular datasets, including MNIST, through the `tf.keras.datasets` module.\n",
    "\n",
    "2. **Splitting the Dataset:**\n",
    "   ```python\n",
    "   (x_train, _), (x_test, _) = mnist.load_data()\n",
    "   ```\n",
    "   The `mnist.load_data()` function loads the MNIST dataset and returns two tuples: `(x_train, y_train)` and `(x_test, y_test)`. These tuples contain the training and testing data, respectively. The `x_train` and `x_test` variables contain the images, while the `y_train` and `y_test` variables contain the corresponding labels. In this snippet, the labels are not needed, so they are ignored using the underscore `_`.\n",
    "\n",
    "3. **Normalizing the Data:**\n",
    "   ```python\n",
    "   x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "   ```\n",
    "   This line normalizes the pixel values of the images to the range [0, 1]. The original pixel values range from 0 to 255, so dividing by 255.0 scales the values to the desired range. Normalization is an important preprocessing step that helps improve the performance and convergence of neural network models by ensuring that the input data has a consistent scale.\n",
    "\n",
    "### 4.1.2. <a id='toc4_1_2_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The code snippet loads and preprocesses the MNIST dataset using TensorFlow and Keras. The MNIST dataset is imported from the `tf.keras.datasets` module, and the `load_data()` function is used to split the dataset into training and testing sets. The pixel values of the images are then normalized to the range [0, 1] by dividing by 255.0. This preprocessing step is essential for preparing the data for training neural network models, as it ensures that the input data has a consistent scale, which can improve model performance and convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. <a id='toc5_'></a>[__Step 3: Set Hyperparameters__](#toc0_)\n",
    "-  Define the `learning_rate`, `num_steps`, and `batch_size`.\n",
    "- `learning_rate` is the step size at each iteration while moving toward a minimum of a loss function.\n",
    "- `num_steps` is the number of steps you want to train the model.\n",
    "- `batch_size` is the number of samples that will be propagated through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_steps = 100\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. <a id='toc5_1_'></a>[__Step 3 Code Explanation__](#toc0_)\n",
    "\n",
    "The provided code snippet defines three important hyperparameters for training a neural network model using TensorFlow and Keras. These hyperparameters control various aspects of the training process, including the learning rate, the number of training steps, and the batch size. Let's break down each of these hyperparameters and understand their significance.\n",
    "\n",
    "### 5.1.1. <a id='toc5_1_1_'></a>[__Code Breakdown__](#toc0_)\n",
    "\n",
    "1. **Learning Rate:**\n",
    "   ```python\n",
    "   learning_rate = 0.001\n",
    "   ```\n",
    "   The learning rate is a crucial hyperparameter that determines the step size at each iteration while moving toward a minimum of the loss function. A smaller learning rate means the model will take smaller steps, which can lead to more precise convergence but may require more iterations. Conversely, a larger learning rate means the model will take larger steps, which can speed up convergence but may risk overshooting the minimum. In this snippet, the learning rate is set to 0.001, which is a common choice for many neural network models.\n",
    "\n",
    "2. **Number of Steps:**\n",
    "   ```python\n",
    "   num_steps = 100\n",
    "   ```\n",
    "   The number of steps (or iterations) specifies how many times the model will update its weights during training. Each step involves processing a batch of training data, computing the loss, and updating the model's weights based on the gradients. In this snippet, the number of steps is set to 100, meaning the model will perform 100 weight updates during training. This parameter is often used in conjunction with the number of epochs, which defines how many times the entire training dataset is passed through the model.\n",
    "\n",
    "3. **Batch Size:**\n",
    "   ```python\n",
    "   batch_size = 64\n",
    "   ```\n",
    "   The batch size is another important hyperparameter that determines the number of training samples used in one forward and backward pass. A smaller batch size means the model will update its weights more frequently, which can lead to faster convergence but may introduce more noise in the training process. A larger batch size means the model will update its weights less frequently, which can lead to more stable updates but may require more memory. In this snippet, the batch size is set to 64, meaning the model will process 64 training samples in each step.\n",
    "\n",
    "### 5.1.2. <a id='toc5_1_2_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The code snippet defines three key hyperparameters for training a neural network model using TensorFlow and Keras:\n",
    "\n",
    "- **Learning Rate (`learning_rate = 0.001`):** Controls the step size at each iteration while moving toward a minimum of the loss function. A learning rate of 0.001 is a common choice for many models.\n",
    "- **Number of Steps (`num_steps = 100`):** Specifies the number of weight updates the model will perform during training. In this case, the model will perform 100 updates.\n",
    "- **Batch Size (`batch_size = 64`):** Determines the number of training samples used in one forward and backward pass. A batch size of 64 means the model will process 64 samples in each step.\n",
    "\n",
    "These hyperparameters play a crucial role in the training process, affecting the model's convergence, stability, and overall performance. Properly tuning these hyperparameters is essential for achieving optimal results in neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. <a id='toc6_'></a>[__Step 4: Define Model Architecture__](#toc0_)\n",
    "- Define the architecture of the VAE, including the encoder and decoder networks and the loss function.\n",
    "- Construct the VAE using `tf.keras` that defines the encoder and decoder using keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 18:03:36.467050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:36.468948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:36.470569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:37.004647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:37.006616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:37.008121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-01-27 18:03:37.009602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2 # Example latent space dimension\n",
    "\n",
    "# Define the encoder part - functional approach\n",
    "encoder_inputs = tf.keras.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Flatten()(encoder_inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "\n",
    "# Define the decoder part - functional approach\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(latent_inputs)\n",
    "x = tf.keras.layers.Dense(784, activation='sigmoid')(x)\n",
    "decoder_outputs = tf.keras.layers.Reshape((28, 28))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. <a id='toc6_1_'></a>[__Step 4 Explanation__](#toc0_)\n",
    "\n",
    "The provided code snippet defines the encoder and decoder parts of a Variational Autoencoder (VAE) using TensorFlow and Keras. A VAE is a type of generative model that learns to encode input data into a latent space and then decode it back to the original data space. This model is particularly useful for tasks such as image generation, anomaly detection, and data compression. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "### 6.1.1. <a id='toc6_1_1_'></a>[__Code Breakdown__](#toc0_)\n",
    "\n",
    "#### 6.1.1.1. <a id='toc6_1_1_1_'></a>[__Latent Space Dimension__](#toc0_)\n",
    "\n",
    "```python\n",
    "latent_dim = 2 # Example latent space dimension\n",
    "```\n",
    "\n",
    "The `latent_dim` variable defines the dimensionality of the latent space. In this example, the latent space has 2 dimensions. The latent space is a lower-dimensional representation of the input data, where the VAE learns to encode the essential features of the data.\n",
    "\n",
    "#### 6.1.1.2. <a id='toc6_1_1_2_'></a>[__Encoder Definition__](#toc0_)\n",
    "\n",
    "```python\n",
    "# Define the encoder part - functional approach\n",
    "encoder_inputs = tf.keras.Input(shape=(28, 28))\n",
    "x = tf.keras.layers.Flatten()(encoder_inputs)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "```\n",
    "\n",
    "The encoder part of the VAE is defined using the functional API of Keras. The encoder maps the input data to the latent space.\n",
    "\n",
    "1. **Input Layer:**\n",
    "   ```python\n",
    "   encoder_inputs = tf.keras.Input(shape=(28, 28))\n",
    "   ```\n",
    "   The input layer expects images of shape (28, 28), which corresponds to the dimensions of the MNIST dataset images.\n",
    "\n",
    "2. **Flatten Layer:**\n",
    "   ```python\n",
    "   x = tf.keras.layers.Flatten()(encoder_inputs)\n",
    "   ```\n",
    "   The `Flatten` layer reshapes the 2D input images into 1D vectors, making them suitable for the subsequent dense layers.\n",
    "\n",
    "3. **Dense Layer:**\n",
    "   ```python\n",
    "   x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "   ```\n",
    "   A dense (fully connected) layer with 512 neurons and ReLU activation is applied to the flattened input. This layer learns to extract features from the input data.\n",
    "\n",
    "4. **Latent Variables:**\n",
    "   ```python\n",
    "   z_mean = tf.keras.layers.Dense(latent_dim)(x)\n",
    "   z_log_var = tf.keras.layers.Dense(latent_dim)(x)\n",
    "   ```\n",
    "   \n",
    "   Two dense layers are used to compute the mean (`z_mean`) and the logarithm of the variance (`z_log_var`) of the latent variables. These parameters define the distribution of the latent space.\n",
    "\n",
    "   **Key Points**\n",
    "\n",
    "   1. **Variable Names Are Arbitrary:** The names `z_mean` and `z_log_var` are arbitrary and chosen by the programmer for clarity. The Dense layers themselves do not have any inherent knowledge of what \"*mean*\" or \"*log variance*\" are. They simply perform a linear transformation followed by an optional activation function.  The actual computation performed by the `Dense` layers is the same as any other `Dense` layer, but the names `z_mean` and `z_log_var` make it clear that these variables are used to parameterize the Gaussian distribution in the latent space.  The roles of these variables as the *mean* and *log variance* of the latent variables are determined by how they are used in the subsequent steps of the VAE.\n",
    "\n",
    "   2. **Dense Layer Computation:** The Dense layer performs the following computation: `[ \\text{output} = W \\cdot x + b ]` where ( *W* ) is **the weight matrix**, ( *x* ) is **the input**, and ( *b* ) is **the bias vector**. This computation is independent of the variable names.\n",
    "   \n",
    "   3. **Role in VAE:** In the context of a VAE, the encoder network maps the input data to a latent space defined by a Gaussian distribution. This distribution is parameterized by its mean and variance. The `z_mean` and `z_log_var` variables are used to store these parameters.\n",
    "\n",
    "   4. **Context and Usage:** The context in which you use these variables determines their role. In a Variational Autoencoder (VAE), the outputs of these `Dense` layers are interpreted as the mean and log variance of the latent variables because of how they are used in the subsequent steps of the VAE.\n",
    "\n",
    "      The reason z_mean and z_log_var are able to calculate two different values, even though they are both defined using tf.keras.layers.Dense(latent_dim)(x), is because they are instantiated as two separate Dense layers with their own unique sets of weights and biases. Each Dense layer in Keras maintains its own parameters, which are learned independently during the training process.\n",
    "\n",
    "   **Detailed Explanation**\n",
    "   \n",
    "      1. **Separate Dense Layers:**\n",
    "\n",
    "         Although both lines use `tf.keras.layers.Dense(latent_dim)(x)`, they create two distinct Dense layers. Each Dense layer has its own set of weights and biases, which are initialized independently and updated separately during training.\n",
    "\n",
    "      2. **Unique Weights and Biases:**\n",
    "\n",
    "         - Weights (W): Each Dense layer has a weight matrix that is initialized randomly and updated during training. The weight matrix determines how the input features are combined to produce the output.\n",
    "         - Biases (b): Each Dense layer also has a bias vector that is added to the weighted sum of the input features.\n",
    "      \n",
    "         For `z_mean`: [ z_{\\text{mean}} = W_{\\text{mean}} \\cdot x + b_{\\text{mean}} ]\n",
    "\n",
    "         For `z_log_var`: [ z_{\\text{log_var}} = W_{\\text{log_var}} \\cdot x + b_{\\text{log_var}} ]\n",
    "\n",
    "         Here, ( W_{\\text{mean}} ) and ( b_{\\text{mean}} ) are the weights and biases for the `z_mean` layer, while ( W_{\\text{log_var}} ) and ( b_{\\text{log_var}} ) are the weights and biases for the `z_log_var` layer. These parameters are learned independently.\n",
    "\n",
    "      3. **Independent Learning:** During the training process, the weights and biases of z_mean and z_log_var are updated based on the gradients computed from the loss function. Since they are separate layers, their parameters are updated independently, allowing them to learn different transformations of the input data.\n",
    "\n",
    "\n",
    "#### 6.1.1.3. <a id='toc6_1_1_3_'></a>[__Decoder Definition__](#toc0_)\n",
    "\n",
    "```python\n",
    "# Define the decoder part - functional approach\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(latent_inputs)\n",
    "x = tf.keras.layers.Dense(784, activation='sigmoid')(x)\n",
    "decoder_outputs = tf.keras.layers.Reshape((28, 28))(x)\n",
    "```\n",
    "\n",
    "The decoder part of the VAE is also defined using the functional API of Keras. The decoder maps the latent space back to the original data space.\n",
    "\n",
    "1. **Latent Input Layer:**\n",
    "   ```python\n",
    "   latent_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "   ```\n",
    "   The input layer expects vectors of shape `(latent_dim,)`, which corresponds to the dimensionality of the latent space.\n",
    "\n",
    "2. **Dense Layer:**\n",
    "   ```python\n",
    "   x = tf.keras.layers.Dense(512, activation='relu')(latent_inputs)\n",
    "   ```\n",
    "   A dense layer with 512 neurons and ReLU activation is applied to the latent inputs. This layer learns to decode the latent representation back to the original data space.\n",
    "\n",
    "3. **Output Dense Layer:**\n",
    "   ```python\n",
    "   x = tf.keras.layers.Dense(784, activation='sigmoid')(x)\n",
    "   ```\n",
    "   Another dense layer with 784 neurons (28 * 28) and sigmoid activation is applied. The sigmoid activation ensures that the output values are in the range [0, 1], which is suitable for image data.\n",
    "\n",
    "4. **Reshape Layer:**\n",
    "   ```python\n",
    "   decoder_outputs = tf.keras.layers.Reshape((28, 28))(x)\n",
    "   ```\n",
    "   The `Reshape` layer reshapes the 1D output vector back into a 2D image of shape (28, 28), which corresponds to the original dimensions of the MNIST dataset images.\n",
    "\n",
    "### 6.1.2. <a id='toc6_1_2_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The code snippet defines the encoder and decoder parts of a Variational Autoencoder (VAE) using TensorFlow and Keras. The encoder maps the input images to a lower-dimensional latent space, while the decoder maps the latent space back to the original image space. The encoder consists of an input layer, a flatten layer, a dense layer, and two dense layers for the latent variables (`z_mean` and `z_log_var`). The decoder consists of an input layer for the latent space, two dense layers, and a reshape layer to reconstruct the images. This architecture allows the VAE to learn a meaningful latent representation of the input data, which can be used for various generative tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. <a id='toc6_2_'></a>[__When to Use a `Flatten` Layer__](#toc0_)\n",
    "\n",
    "A `Flatten` layer does not always need to be added prior to a `Dense` layer, but it is often necessary when transitioning from convolutional layers (or other multi-dimensional layers) to fully connected `Dense` layers. The `Flatten` layer reshapes the multi-dimensional output of convolutional layers into a 1D vector, which is the expected input format for `Dense` layers.\n",
    "\n",
    "1. **Transitioning from Convolutional Layers to Dense Layers:**\n",
    "   In convolutional neural networks (CNNs), the output of convolutional and pooling layers is typically a multi-dimensional tensor (e.g., 3D for images: height, width, and channels). To feed this output into a `Dense` layer, you need to flatten it into a 1D vector.\n",
    "\n",
    "   ```python\n",
    "   model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "       tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "       tf.keras.layers.Flatten(),\n",
    "       tf.keras.layers.Dense(128, activation='relu'),\n",
    "       tf.keras.layers.Dense(10, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "2. **When the Input is Already Flattened:**\n",
    "   If the input to the `Dense` layer is already a 1D vector, you do not need to add a `Flatten` layer. For example, if you are working with tabular data where each sample is represented as a 1D feature vector, you can directly use `Dense` layers.\n",
    "\n",
    "   ```python\n",
    "   model = tf.keras.Sequential([\n",
    "       tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
    "       tf.keras.layers.Dense(10, activation='softmax')\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "__Example Without `Flatten` Layer__\n",
    "\n",
    "If the input data is already in a 1D format, you can directly use `Dense` layers without a `Flatten` layer:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example with tabular data (1D input)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(100,)),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "__Example With `Flatten` Layer__\n",
    "\n",
    "When transitioning from convolutional layers to dense layers, you need to use a `Flatten` layer:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example with image data (2D input)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "__Summary__\n",
    "\n",
    "A `Flatten` layer is necessary when you need to convert a multi-dimensional tensor (e.g., the output of convolutional layers) into a 1D vector before feeding it into a `Dense` layer. However, if the input to the `Dense` layer is already a 1D vector, you do not need to add a `Flatten` layer. The use of the `Flatten` layer depends on the shape of the data being fed into the `Dense` layer.\n",
    "\n",
    "## 6.3. <a id='toc6_3_'></a>[__Explanation of the Choice of Neurons and Activation Function__](#toc0_)\n",
    "\n",
    "- __Complexity of the Data__\n",
    "\n",
    "    The MNIST dataset consists of 28x28 grayscale images, which are relatively simple compared to more complex datasets like CIFAR-10 or ImageNet. However, the VAE needs to learn a meaningful latent representation of these images, which requires a certain level of complexity in the network.\n",
    "\n",
    "- __Balancing Model Capacity and Overfitting__\n",
    "\n",
    "    Choosing 512 neurons for the Dense layer provides a balance between model capacity and the risk of overfitting. A higher number of neurons increases the model's capacity to learn complex patterns, but it also increases the risk of overfitting. Empirical results often show that 512 neurons work well for the MNIST dataset in terms of achieving good performance without overfitting.\n",
    "\n",
    "- __Empirical Results and Common Practices__\n",
    "\n",
    "    The choice of 512 neurons is also influenced by common practices and empirical results in the field of deep learning. Many successful architectures for similar tasks use 512 neurons in dense layers, making it a reasonable starting point for experimentation.\n",
    "\n",
    "- __ReLU Activation__\n",
    "\n",
    "    The use of the ReLU activation function helps mitigate the vanishing gradient problem and allows the network to learn more complex patterns. The combination of 512 neurons and ReLU activation is a common choice for achieving good performance.\n",
    "\n",
    "__Summary__\n",
    "The choice of 512 neurons for the Dense layer in the VAE implementation is influenced by several factors, including the complexity of the MNIST dataset, the need to balance model capacity and overfitting, empirical results, and common practices in the field of deep learning. This choice provides a good starting point for achieving meaningful latent representations while maintaining a manageable level of complexity in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. <a id='toc7_'></a>[__Step 5: Define the Sampling Function__](#toc0_)\n",
    "\n",
    "-  Create a custom Keras layer for the sampling function used in the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):  #z_mean = [32,10] - [batch_size, number_of_samples]\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. <a id='toc7_1_'></a>[__Step 5 Code Explanation__](#toc0_)\n",
    "\n",
    "The provided code snippet defines a custom Keras layer called `Sampling`, which is used in the context of a Variational Autoencoder (VAE). The `Sampling` layer is responsible for implementing the reparameterization trick, a key component of VAEs that allows for backpropagation through stochastic nodes. This trick enables the model to learn a meaningful latent space representation by sampling from a distribution defined by the encoder's output.\n",
    "\n",
    "### 7.1.1. <a id='toc7_1_1_'></a>[__Code Breakdown__](#toc0_)\n",
    "\n",
    "1. **Defining the Sampling Layer:**\n",
    "   ```python\n",
    "   class Sampling(tf.keras.layers.Layer):\n",
    "   ```\n",
    "\n",
    "   The `Sampling` class inherits from `tf.keras.layers.Layer`, making it a custom Keras layer. This layer will be used to sample latent variables from the distribution defined by the encoder's output.\n",
    "\n",
    "2. **Call Method:**\n",
    "   ```python\n",
    "   def call(self, inputs):  #z_mean = [32,10] - [batch_size, number_of_samples]\n",
    "   ```\n",
    "\n",
    "   The `call` method is where the layer's logic is implemented. It takes `inputs` as an argument, which in this case are the mean (`z_mean`) and log variance (`z_log_var`) of the latent variables produced by the encoder.\n",
    "\n",
    "3. **Extracting Batch Size and Dimension:**\n",
    "   ```python\n",
    "   z_mean, z_log_var = inputs\n",
    "   batch = tf.shape(z_mean)[0]\n",
    "   dim = tf.shape(z_mean)[1]\n",
    "   ```\n",
    "\n",
    "   The `inputs` are unpacked into `z_mean` and `z_log_var`. The batch size (`batch`) and the dimensionality of the latent space (`dim`) are extracted from the shape of `z_mean`. These values are used to generate random samples of the appropriate shape.\n",
    "\n",
    "4. **Generating Random Noise:**\n",
    "   ```python\n",
    "   epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "   ```\n",
    "\n",
    "   A random noise tensor `epsilon` is generated using a standard normal distribution. The shape of `epsilon` matches the batch size and the dimensionality of the latent space. This noise is used to introduce stochasticity into the sampling process.\n",
    "\n",
    "5. **Reparameterization Trick:**\n",
    "   ```python\n",
    "   return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "   ```\n",
    "\n",
    "   The reparameterization trick is applied to generate samples from the latent space. The formula `z_mean + tf.exp(0.5 * z_log_var) * epsilon` ensures that the samples are drawn from a distribution defined by the mean and log variance. The `tf.exp(0.5 * z_log_var)` term scales the random noise `epsilon` according to the standard deviation of the distribution.\n",
    "\n",
    "### 7.1.2. <a id='toc7_1_2_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The `Sampling` class defines a custom Keras layer that implements the reparameterization trick, a crucial component of Variational Autoencoders (VAEs). The `call` method takes the mean (`z_mean`) and log variance (`z_log_var`) of the latent variables as inputs, extracts the batch size and dimensionality, generates random noise, and applies the reparameterization trick to produce samples from the latent space. This process allows the VAE to learn a meaningful latent representation by enabling backpropagation through stochastic nodes, facilitating the training of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. <a id='toc8_'></a>[__Step 6: Connect the Encoder and the Decoder__](#toc0_)\n",
    "\n",
    "- Use keras functional API to connect the encoder and decoder parts of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = Sampling()([z_mean, z_log_var])\n",
    "encoder = tf.keras.Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, encoder_outputs])\n",
    "\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=decoder_outputs)\n",
    "vae_outputs = decoder(encoder(encoder_inputs)[2])\n",
    "vae = tf.keras.Model(inputs=encoder_inputs, outputs=vae_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. <a id='toc8_1_'></a>[__Step 6 Code Explanation__](#toc0_)\n",
    "\n",
    "The provided code snippet constructs the encoder and decoder models for a Variational Autoencoder (VAE) using TensorFlow and Keras. It then combines these models to create the complete VAE model. Let's break down the code step by step to understand its components and functionality.\n",
    "\n",
    "### 8.1.1. <a id='toc8_1_1_'></a>[__Code Breakdown__](#toc0_)\n",
    "\n",
    "1. **Sampling Layer Output:**\n",
    "   ```python\n",
    "   encoder_outputs = Sampling()([z_mean, z_log_var])\n",
    "   ```\n",
    "   \n",
    "   This line creates an instance of the `Sampling` layer and applies it to the mean (`z_mean`) and log variance (`z_log_var`) of the latent variables. The `Sampling` layer implements the reparameterization trick, which allows for backpropagation through stochastic nodes. The output of this layer, `encoder_outputs`, represents the sampled latent variables.\n",
    "\n",
    "2. **Defining the Encoder Model:**\n",
    "   ```python\n",
    "   encoder = tf.keras.Model(inputs=encoder_inputs, outputs=[z_mean, z_log_var, encoder_outputs])\n",
    "   ```\n",
    "\n",
    "   The encoder model is defined using the Keras functional API. It takes `encoder_inputs` (the input images) as input and produces three outputs: `z_mean`, `z_log_var`, and `encoder_outputs`. The `z_mean` and `z_log_var` represent the parameters of the latent distribution, while `encoder_outputs` represents the sampled latent variables.\n",
    "\n",
    "3. **Defining the Decoder Model:**\n",
    "   ```python\n",
    "   decoder = tf.keras.Model(inputs=latent_inputs, outputs=decoder_outputs)\n",
    "   ```\n",
    "   \n",
    "   The decoder model is also defined using the Keras functional API. It takes `latent_inputs` (the sampled latent variables) as input and produces `decoder_outputs` (the reconstructed images) as output. The decoder maps the latent space back to the original data space.\n",
    "\n",
    "4. **Combining Encoder and Decoder to Form the VAE:**\n",
    "   ```python\n",
    "   vae_outputs = decoder(encoder(encoder_inputs)[2])\n",
    "   vae = tf.keras.Model(inputs=encoder_inputs, outputs=vae_outputs)\n",
    "   ```\n",
    "\n",
    "   The complete VAE model is constructed by combining the encoder and decoder models. The encoder processes the `encoder_inputs` to produce the sampled latent variables (`encoder(encoder_inputs)[2]`). These latent variables are then passed to the decoder to generate the reconstructed images (`vae_outputs`). The final VAE model takes `encoder_inputs` as input and produces `vae_outputs` as output.\n",
    "\n",
    "### 8.1.2. <a id='toc8_1_2_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The code snippet constructs the encoder and decoder models for a Variational Autoencoder (VAE) using TensorFlow and Keras. The `Sampling` layer is used to implement the reparameterization trick, allowing for backpropagation through stochastic nodes. The encoder model maps the input images to a latent space, producing the mean and log variance of the latent variables, as well as the sampled latent variables. The decoder model maps the latent variables back to the original data space, reconstructing the images. The complete VAE model is formed by combining the encoder and decoder, taking input images and producing reconstructed images. This architecture allows the VAE to learn a meaningful latent representation of the input data, which can be used for various generative tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the next steps, you will define the loss function and train the model. After training, you can use the model to generate new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. <a id='toc9_'></a>[__Step 7: Define the Loss Function and Compile the Model__](#toc0_)\n",
    "- The loss function in VAE typically includes a reconstruction loss and a KL divergence loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the VAE loss within the VAE model class\n",
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        z_mean, z_log_var, z = self.encoder(x)\n",
    "        reconstructed = self.decoder(z)\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(x, reconstructed)\n",
    "        )\n",
    "        reconstruction_loss *= 28 * 28\n",
    "        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        kl_loss = tf.reduce_mean(kl_loss)\n",
    "        kl_loss *= -0.5\n",
    "        return reconstruction_loss + kl_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(data)\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return {'loss': loss}\n",
    "\n",
    "# Instantiate and compile the VAE\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1. <a id='toc9_1_'></a>[__Step 7 Code Explanation__](#toc0_)\n",
    "\n",
    "The provided code snippet defines a Variational Autoencoder (VAE) model using TensorFlow and Keras. A VAE is a type of generative model that learns to encode input data into a latent space and then decode it back to the original data space. This model is particularly useful for tasks such as image generation, anomaly detection, and data compression.\n",
    "\n",
    "1. **Defining the VAE Class:**\n",
    "   ```python\n",
    "   class VAE(tf.keras.Model):\n",
    "       def __init__(self, encoder, decoder, **kwargs):\n",
    "           super(VAE, self).__init__(**kwargs)\n",
    "           self.encoder = encoder\n",
    "           self.decoder = decoder\n",
    "   ```\n",
    "   The **VAE class** inherits from `tf.keras.Model`, making it a custom Keras model. The `__init__` method initializes the VAE with an encoder and a decoder, which are neural network models responsible for encoding the input data into a latent space and decoding it back to the original data space, respectively.\n",
    "\n",
    "1. **Computing the VAE Loss:**\n",
    "   ```python\n",
    "   def compute_loss(self, x):\n",
    "       z_mean, z_log_var, z = self.encoder(x)\n",
    "       reconstructed = self.decoder(z)\n",
    "       reconstruction_loss = tf.reduce_mean(\n",
    "           tf.keras.losses.binary_crossentropy(x, reconstructed)\n",
    "       )\n",
    "       reconstruction_loss *= 28 * 28\n",
    "       kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "       kl_loss = tf.reduce_mean(kl_loss)\n",
    "       kl_loss *= -0.5\n",
    "       return reconstruction_loss + kl_loss\n",
    "   ```\n",
    "   The `compute_loss` method calculates the VAE loss, which consists of two parts:\n",
    "   - **Reconstruction Loss:** Measures how well the decoder can reconstruct the input data from the latent space. It uses binary cross-entropy loss between the original input `x` and the reconstructed output `reconstructed`. The loss is scaled by the size of the input images (28 * 28 for MNIST).\n",
    "   - **KL Divergence Loss:** Measures the difference between the learned latent distribution and a standard normal distribution. This regularizes the latent space to ensure it follows a normal distribution. The KL divergence loss is calculated using the mean and log variance of the latent variables (`z_mean` and `z_log_var`).\n",
    "\n",
    "2. **Training Step:**\n",
    "   ```python\n",
    "   def train_step(self, data):\n",
    "       if isinstance(data, tuple):\n",
    "           data = data[0]\n",
    "       with tf.GradientTape() as tape:\n",
    "           loss = self.compute_loss(data)\n",
    "       grads = tape.gradient(loss, self.trainable_variables)\n",
    "       self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "       return {'loss': loss}\n",
    "   ```\n",
    "   The `train_step` method defines a custom training loop for the VAE. It performs the following steps:\n",
    "   - Checks if the input `data` is a tuple and extracts the first element if it is.\n",
    "   - Uses `tf.GradientTape` to record the operations for automatic differentiation.\n",
    "   - Computes the loss using the `compute_loss` method.\n",
    "   - Calculates the gradients of the loss with respect to the model's trainable variables.\n",
    "   - Applies the gradients to the model's variables using the optimizer.\n",
    "   - Returns the loss value for monitoring.\n",
    "\n",
    "4. **Instantiating and Compiling the VAE:**\n",
    "   ```python\n",
    "   vae = VAE(encoder, decoder)\n",
    "   vae.compile(optimizer='adam')\n",
    "   ```\n",
    "   The VAE model is instantiated with the encoder and decoder models. The `compile` method is called to configure the model for training, specifying the Adam optimizer.\n",
    "\n",
    "### 9.1.1. <a id='toc9_1_1_'></a>[__Summary__](#toc0_)\n",
    "\n",
    "The code snippet defines a Variational Autoencoder (VAE) model using TensorFlow and Keras. The `VAE` class inherits from `tf.keras.Model` and includes methods for computing the VAE loss and performing a training step. The VAE loss consists of a reconstruction loss and a KL divergence loss, which together ensure that the model learns a meaningful latent space representation. The `train_step` method defines a custom training loop, and the model is instantiated and compiled with an Adam optimizer. This VAE model can be used for tasks such as image generation, anomaly detection, and data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. <a id='toc10_'></a>[__Step 8: Train the Model__](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-27 18:03:39.503744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-01-27 18:03:39.795356: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fafc750d7f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-27 18:03:39.795389: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6\n",
      "2024-01-27 18:03:39.799452: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-27 18:03:41.134828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2024-01-27 18:03:41.251324: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 7s 3ms/step - loss: 181.7992\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 162.7491\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 159.5753\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 157.3615\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 155.5820\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 154.1689\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 152.9828\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 152.0396\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 151.2354\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 150.4547\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 149.8704\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 149.3017\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 148.8203\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 148.3222\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 147.9211\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 147.5298\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 147.1653\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 146.8348\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 146.5514\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 146.2268\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 145.9381\n",
      "Epoch 22/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 145.6703\n",
      "Epoch 23/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 145.3358\n",
      "Epoch 24/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 145.1758\n",
      "Epoch 25/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 144.9374\n",
      "Epoch 26/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 144.7603\n",
      "Epoch 27/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 144.5295\n",
      "Epoch 28/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 144.3438\n",
      "Epoch 29/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 144.1455\n",
      "Epoch 30/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.9359\n",
      "Epoch 31/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.7992\n",
      "Epoch 32/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.6760\n",
      "Epoch 33/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.4546\n",
      "Epoch 34/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.4115\n",
      "Epoch 35/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.1720\n",
      "Epoch 36/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 143.0523\n",
      "Epoch 37/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.9139\n",
      "Epoch 38/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.8120\n",
      "Epoch 39/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.7062\n",
      "Epoch 40/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.5738\n",
      "Epoch 41/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.4370\n",
      "Epoch 42/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.2985\n",
      "Epoch 43/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.2604\n",
      "Epoch 44/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 142.1368\n",
      "Epoch 45/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.9647\n",
      "Epoch 46/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.9620\n",
      "Epoch 47/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.8347\n",
      "Epoch 48/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.7604\n",
      "Epoch 49/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.6648\n",
      "Epoch 50/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.5706\n",
      "Epoch 51/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.4378\n",
      "Epoch 52/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.3722\n",
      "Epoch 53/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.3270\n",
      "Epoch 54/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.2220\n",
      "Epoch 55/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.1545\n",
      "Epoch 56/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 141.0764\n",
      "Epoch 57/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.9755\n",
      "Epoch 58/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.9155\n",
      "Epoch 59/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.8226\n",
      "Epoch 60/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.7680\n",
      "Epoch 61/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.6965\n",
      "Epoch 62/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.5856\n",
      "Epoch 63/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.5697\n",
      "Epoch 64/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.4470\n",
      "Epoch 65/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.4318\n",
      "Epoch 66/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.3602\n",
      "Epoch 67/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.2815\n",
      "Epoch 68/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.2290\n",
      "Epoch 69/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.1518\n",
      "Epoch 70/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.0923\n",
      "Epoch 71/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 140.0545\n",
      "Epoch 72/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.9527\n",
      "Epoch 73/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.9193\n",
      "Epoch 74/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.8488\n",
      "Epoch 75/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.7585\n",
      "Epoch 76/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.8167\n",
      "Epoch 77/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.6775\n",
      "Epoch 78/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.6050\n",
      "Epoch 79/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.5525\n",
      "Epoch 80/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.5171\n",
      "Epoch 81/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.5360\n",
      "Epoch 82/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.4340\n",
      "Epoch 83/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.3537\n",
      "Epoch 84/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.3540\n",
      "Epoch 85/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.3127\n",
      "Epoch 86/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.2436\n",
      "Epoch 87/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.2059\n",
      "Epoch 88/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.1809\n",
      "Epoch 89/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.1680\n",
      "Epoch 90/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.0984\n",
      "Epoch 91/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.0636\n",
      "Epoch 92/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 139.0299\n",
      "Epoch 93/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.9326\n",
      "Epoch 94/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.8741\n",
      "Epoch 95/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.8347\n",
      "Epoch 96/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.8110\n",
      "Epoch 97/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.8555\n",
      "Epoch 98/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.7623\n",
      "Epoch 99/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.6788\n",
      "Epoch 100/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 138.6793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fb1af913040>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x_train, x_train, epochs=num_steps, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. <a id='toc11_'></a>[__Step 9: Generate a Manifold of Digits__](#toc0_)\n",
    "- Generate a manifold of digits by creating a latent space grid.\n",
    "- Feed these grid values into the decoder to produce images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAIuCAYAAABdOBlOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAABQfElEQVR4nO3dSZCe1XmH/bsNCARoFprneUACxGAwYIydsk1il53Yi8SVcpJFKouss842y2yySlJZeOVUquyy4yEOBmywEGJGs4TQrEYSmpCYB32LfN9X3Nc59PNKertbffr67e4enh6e0+c59Z5/32fg0qVLIUmS1JovjPY3IEmSNBxc5EiSpCa5yJEkSU1ykSNJkprkIkeSJDXJRY4kSWrS9UO9c2BgwP8vb9SlS5cGRuprOY7aNVLjyDHULuei/vjCF/JrFp9++ukofSej4/PGka/kSJKkJrnIkSRJTRpyu0qSJPUXt5Z48sCtt95afM4tt9yS6gsXLqT6vffeG/Jr1L7O5b7/SgwMlLtI/N4++eSTvn/d//9rDduVJUmSRpGLHEmS1CQXOZIkqUlmciRJGkbMpVx/fX70TpkyJdWLFy8urrFgwYJU33TTTanetWtXqk+cOFFc4+LFi6n+4IMPUv3xxx+nejgyOhEj++/tvpIjSZKa5CJHkiQ1yUWOJElqkoscSZLUJIPHQ2BYjPV1113XeY3LDVj18vHDFQaTJPUfnx0TJkxI9YwZM1L94IMPFtdYu3Ztqt99991UnzlzJtXnz58vrvHhhx+mmk34+Py5koAwn0+j/bzylRxJktQkFzmSJKlJLnIkSVKTxm0mhweE3XDDDcXH3HzzzameNGnSkNfoZf+SX4eHrrE5U+1tbOjEg9q47xox+vuiY0HtIDnunfPgPI4JNvmKKMcJ788777yTah60V/ucrr1073fbOFY5xno5FJGZQta1eaRrnLEeyaZv1zL+7lnPnTs31Rs3biyusXz58lS/8cYbqeZcVNPVlLCX+3e5uZ3Rnot8JUeSJDXJRY4kSWqSixxJktSkZjM5XXuPzNvMnj27uMasWbNSPWfOnFRzD5vZiohy75Vfd9q0aalmviai/FmOHDmS6j179qT6zTffLK6h8n7deOONqeYheRERixYtSvU999yT6ocffjjVtXHEjA2zDnv37k31c889V1yD9/j06dOpZk+MWrbL3M7oqOVjhnp/7eM5dpnlmz59eqp5eGPtupwTmT9jH5Ya5gNZc5xGtD/uavevq0/OwoULU82MTu0a/F0PDg4O+f6I7mxfL2Oxazxfa/fXV3IkSVKTXORIkqQmuciRJElNajaTwz1sZmFWr16d6m9+85vFNdasWZPq999/P9Xc86ztVTIHcdttt6X65MmTqa7leohZkp07d3Z+znjUddYYcww8GyaizNw88sgjqd60aVOq2UcnIuLcuXOpZu6KuR/uz0dEPPXUU6nesWNHqpnrqfWu+Pjjj1PN/fhrbS99LOjlPDv+vbLmOGT2L6LskcKa2b6JEycW1+A45Nc9depUqmvjgVmfY8eOpfq3v/1tqmsZw1perCW9ZKr4PGIGp9bzhs+G119/PdXHjx9PNf/eI7r7a/WiK7fDerTnFV/JkSRJTXKRI0mSmuQiR5IkNclFjiRJalITweNa2I/Bu/Xr16f6r//6r1P9rW99q7gGA3KHDx9ONUPDbPwXUQa72BiOYWaGx2pf58CBA6lm067RDnpdK7qanzEkzDBnRMQDDzyQ6mXLlqWajRfZqDGiHDdsBMaxWgsv33777ak+c+ZMqg8ePJjqroZd6g/+zdfuHYPEd955Z6rvvffeVH/ta18rrsGQMEOlnCMYMI0o5x5+r12HbUaUQWJ+HQaTx+MBnb3Mv5wDJk+enGqG0yPKYPi2bdtS3Uug+3JDwr3MIwaPJUmSRoGLHEmS1CQXOZIkqUljMpPTdehlRNns72//9m9T/Wd/9mdDXjOizMcwW8FcRO0wOu6TduVraodrnj17NtXcW+/lID2Ve8VTp05N9apVq4rP4dh64403Ur1169ZUv/TSS8U1mLviwa9sQrh06dLiGmwWxgMae9k7H+298Rbw98xcy+LFi4vP+eIXv5jqb3/726n+0pe+lGrmNSLKeYNzE5vy1bJ9bDC3YMGCVDP3c8MNNxTXYPaLNb8Px1wdf9dsAMq5KaKcWz766KNU837xORFRjt/ac68Lc1jMOtbyYNRL/qtffCVHkiQ1yUWOJElqkoscSZLUpDGRyeE+InsxMOMQUe57P/roo6nmHuCuXbuKa/zkJz9J9WuvvZbq8+fPp5rZi9rbmJ9hjqd2Dfaa6KrHo1ompatPzpQpU1Jd+z2+/fbbqeaheDw48+jRo8U1uFfOsceDYGuHK/JwPo4T7r/XDuczH3H1mJfhgbu1wzWZuWGfHOb2mPuKiHj22WdT/cILL6Sa+UCO9Ygyg8PcB8cYrxlRZn12796das6JtXzGleRAxjrej9mzZ6eaObxaLou/+w8//HDIr1n7PbOnHD+G7+9lzuC8yaxQrX9P1+HA/ey1M/5GmyRJGhdc5EiSpCa5yJEkSU26JjM53I/jOR7cW/7qV79aXINnUTHnsGPHjlT/9Kc/La7x5JNPppq9Kph7qO2R8mO69ivN11yZXvZsu/aja/2WeP9OnDiRamYQavvP7KWyZMmSVHM/vpZj4Dk1+/btSzWzXrVrjGRvilZwjDDHsnLlylQ/9NBDxTU2btyYap7/xFzL888/X1zj6aefTjX7aXHszpgxo7gG80LMA7IfV63v1549e1LNn+VKMhytqeUDmcvj/WFGh/2GIsqsFjNUfP7UMjmci9g7iWrzCOdRznm9jAl+r2ZyJEmSLpOLHEmS1CQXOZIkqUkuciRJUpOuyeAxg008zIwH3n3nO98prjFv3rxUM0DFw84Y7IyIOHfuXKoZsGJDtl7CngyyGgYdPgyvdTUHZJPJiHIszpw5M9UMDNbCywwWr1+/PtUM1teawTEoz2AiA+21ceTYGlotMMqmbNOmTUv1hg0bUs2QcUQZAOU/MGzZsiXVPPQ1ogwacx5hoHTFihXFNXgQLBtdMlTPkHFEeaDwqVOnUs2mlONxzNXGEf/GOSfwcw4dOlRcg88j/q45zmrhc45fNkTlM47zX0Q51hiAZkNUzk0R5bgYzga3vpIjSZKa5CJHkiQ1yUWOJElq0qhncmoNi7gPziZW9957b6qXL19eXIPN/wYHB1PNfUI2GIwoG6xxL72XAzq5d849Tu499pKlGI/73F1q++D8PTEzxaZVHCMRZfO3+fPnp/r+++9PdW0MLFy4cMhrHjlyJNXPPPNMcY2dO3emmt976w3WRkJtDHU1A+R4YF4homyyx0Nema+qfR/8upyv7rvvvlTz4NCIMtfDZn88ALLWkI65Hc6RtYNhx5tesl18PjHXwmdLRNlAj1+Hjf1qzzQ+SydPnpxq5nxqPwszspzz+IzjXBVR/izMR/L5XPs+euUrOZIkqUkuciRJUpNc5EiSpCaNeianl54C/N/+uXPnDvnxEeWeHvcauZfOA9Qiyh4C7Btx8ODBVNf2UYl72MxS1Pa0zeB06+V3xEzOW2+9lerDhw8Xn7No0aIha+5xc285ohyL7IHxu9/9LtXM30REXLx4MdX8WRwjV6+WD+S8wMwNf++8TxHl/Wd/Gs5Fs2bNKq7B/kzss8LcQ63PCrMTzILt379/yPfXvo4ZnFKttwwzObwXXXVEORb5fGJdG0dr1qxJNcc8n2G1Q6fZG4w/G2v2Uooon4NcB3B+q/1Oex17vpIjSZKa5CJHkiQ1yUWOJElq0qhncmq4T8j9OPZzYP+aiHL/kvmLrvOxIsqzQtjvhNgPI6I8C4T73LXzrjQ8uIfbSyaHZ/XwTDSeB8T+FxHlWS48N43vZ/+SiHIP23EzMpj3Y7aPZ4rVzgvimFi1alWqed4VM4i172P37t2pfuWVV1JdG8ucz5jbOXPmTKpruRD7MZWYJ6nlO5ndYl8c/q5rZ+AxD8jnJJ9hq1evLq7B8clcKZ9PtfP8mMnhnMhMTm0s8hnOccXnc+38q17Hoq/kSJKkJrnIkSRJTXKRI0mSmuQiR5IkNWnEg8e9HLTFUCUbFPGAu1oTLwZ+Gbpj0IuHm0WUYc/FixenmgHCWsBq69atqWb4tZcDOtUf/F2z0VUt8Mvg8fr161PNcVZrTMnxecstt6SaB+nxa9a+165mgI6jy1ebm/g2BiBrcw8x7Mn7zSZubPwXUQaNeXgmQ/ScIyPKf9Bg40LOTY6h3jAkW5sDiCFvNoisHbDKgC/nBDbJrR3QyWcpx03XYZsR5d8Ag9YMzvNw2Ygy0MwgNv/urmYs+kqOJElqkoscSZLUJBc5kiSpScOeyeHeGvewa3vazBswK/Haa6+lmg2NIsqsBPefu3ISEeXeOPc8lyxZkurJkycX13j22WdTzT1Pm2sNn659XO431w6jY06BOa0TJ06kmk2uatfg4YocE/v27SuuMTg4mGozOCOD944HsLLRX20MMaPBRm/MX7DhYESZsWETwm3btqWa4yWiHLsertkfzKTUcix8DvJ5xAZ6y5cvL65RG1ufxedTrTEp8378XnkAbe35xPHJn4Xju5dnXNfH1Bosdv0+/j++kiNJkprkIkeSJDXJRY4kSWrSsGdymH3hnjb3ACPK/Tf+Xz5zEG+++WZxDWZyuI/IPcDawXpdPTD4ORcuXCg+hnue9qIYOV2/W2YlamORuSvevz/84Q+pZu4houyD8sADD6R606ZNqf7Nb35T/4Y/w3HTf7X+JvPnz081e5VwDqjdf/Yi4fzGOZEZh4iIZ555JtXPP/98qtk3p5brMf/XH8zXdOVOI8q/V+ZM+f7aYdCcn5gj5dzEnjgR3f22+P5a7yj2uGH2hz1/auOZ+HX4fV1NfsxXciRJUpNc5EiSpCa5yJEkSU3qeyaHe2vMpPCMqNmzZxfXYJ6Ge9ZU2wfnWRjca+Q1a19j2bJlqeZZVdOnT0917bwY7ovam2LkcJ+bY5N7y7XzYtgriWegvfDCC6lm/iIi4vbbb0/1N7/5zVQzC1I7u6iXM990eZidqPW5mjVrVqo5JzBf88YbbxTXOHLkSKo5HpiV4RlTtWsw08H5rZbZMsfVH/w98v7V5nj2wmKO9Pe//32q77jjjuIaixYtSjWzqkePHk31qVOnOr8PzivMD9bOdOQ8yZ/30KFDqebYjeger/3sA+YrOZIkqUkuciRJUpNc5EiSpCa5yJEkSU0a9uAxg3kMGrO5VkQZ5GIomKE7hqkiygZFPDSMIcMNGzYU13jkkUdSvW7dulSz4VYtdMjwlw25rh0cV2z8F1EeescDV3ft2pXqWuMrBgb5dadOndr1rWoYsLka71NEGbJkAHLnzp2prjUmZWM3zlcMjO7fv7+4xtmzZ1PN0GkvwXSDx8OD94L/9FL7GAZvf/azn6V62rRpxTW+973vpZrzCP/Jp9bcks/brqZ7/OeMiPIZzn+22LNnT6pr/xjEr8PfTz+fk76SI0mSmuQiR5IkNclFjiRJatKwH9DJvWLuE9b2wfk25np4GF0tC8MMzpw5c1LNpl733XdfcY0777yzeNtn/frXv071k08+WXwMmwG6Lz562PyNeZvVq1cXn8N9bx7CynwF98UjyoZybCLJHM/g4GBxDV29rvtfa8DGnBbzBcw01LIEXQcaMp9Qy3Uxs8CfpatBnYYPxwDvVe1jOG8wx/Pv//7vxTWY//uTP/mTVPO5uWLFiuIazOTwe2Uj3trPwmaVPDyWz+MzZ84U1+A8yr8BMzmSJEkdXORIkqQmuciRJElN6nsmh3vD3NNjRoV7gBFlDoJ9Jlh/5StfKa7B/+VnJof78eyZEVH2wdm8eXOqf/GLX6SaB5NF1Pc0NTK6DuTkQZiLFy8ursFxwSwXD+xkfiwi4tFHH001czv79u1L9dtvv11cwwM6rx5/75wjeABvRP3Q1s/i/a5dg/21mMninHj48OHiGuylw2wQMwxm/0ZP7YDOWlZvqM85ePBg8TEcJ/yYe+65J9W1vl8crxw37MdUO3SamRtmCHkN5m8iyuf+cI5XX8mRJElNcpEjSZKa5CJHkiQ1adgzOTyj4/jx46nesmVLcQ2e/8L/99+0aVOqmdGJKHuTsDcF+xbUzot59dVXU80+OFu3bk01+wdE2K9iNHEfnLkWjs3aXjrH1mOPPZbq+++/P9W1TM769etTzRwPs148Cyai/Lviz2IGo1vX/WcdETFp0qRUd513Vcte8G3sicK8Dc/DiijzGGZwxhY+b6hrbEaU5yAyu7d9+/ZUT5gwofPrMP/Kr1s7F5L4Mcyh1n72kXwu+kqOJElqkoscSZLUJBc5kiSpSS5yJElSk4b9gE4G4tgEiKG7iDJkxyDyiRMnUs2DyyLKRkgMYfGAuwMHDhTXYJDrhRdeSDWDYF3hMo0u3h82sfrd735XfM6CBQtSzRA8D4JlqDSiPKDut7/9bap/+ctfptoA+/Dg3zzvf+2gXzaIZNCYQWTOVRFl0JxN3F588cVU1+5/18GJjo+xrZfgOOcvNoRk3Q+1JqSX+08Pox2K95UcSZLUJBc5kiSpSS5yJElSk4Y9k0PcO64d0MmmbMw5HDt2LNU8eDGibMDFPWzWtSwFGyON9t6iPl8vB1hyXDH7wGxERJnjWLt2baqnTJmS6lqug9fdsWNHqplLq41F7sc7Fi8f/+YvXryY6lq2b+rUqalmto9zBA8njIg4cuRIqpn1Y/M/ZoVqX8f7r5FQG2djbez5So4kSWqSixxJktQkFzmSJKlJA0Ptrw0MDFyTm2+95C+6/pd/rO0r9tulS5e6f4l9cq2MI+a0eDgd8ze1j+kaV7WD9ZgF6hqLY2lsjtQ46scY4v3leKhl+6ZPn57qZcuWDfk5p0+fLq7B/iXMgrGPTm0MtWw8zkXqv88bR76SI0mSmuQiR5IkNclFjiRJatKYzOTo6rkPrn4YS5mcyjU7P4bzIz+HtWdIXT7nIvWDmRxJkjSuuMiRJElNcpEjSZKa5CJHkiQ1acQP6JSka8GVNF0cy40bpfHIV3IkSVKTXORIkqQmuciRJElNGrIZoCRJ0ljlKzmSJKlJLnIkSVKTXORIkqQmuciRJElNcpEjSZKa5CJHkiQ1yUWOJElqkoscSZLUJBc5kiSpSS5yJElSk1zkSJKkJrnIkSRJTXKRI0mSmuQiR5IkNclFjiRJapKLHEmS1CQXOZIkqUkuciRJUpNc5EiSpCa5yJEkSU1ykSNJkprkIkeSJDXJRY4kSWrS9UO9c2Bg4NJIfSMaWZcuXRoYqa/lOGrXSI0jx1C7nIvUD583jnwlR5IkNclFjiRJatKQ21XKBgbyq2GXLvnKp6SRx7moF85XGo98JUeSJDXJRY4kSWqSixxJktQkMzmX4YYbbhjy/R999FHxNvfBJQ2F+Zpbbrml+Jg5c+aket26danm3HTkyJHiGvv27Uv1O++8k2rOX85daoGv5EiSpCa5yJEkSU1ykSNJkprkIkeSJDXJ4PFl+PDDD4d8f61B1xe+kNeR11133ZCfY3hZahv/5idOnJjq5cuXF5/DoDFrXmPWrFnFNTh/HTt2LNUXL15M9QcffFBc49NPPy3eJl3LfCVHkiQ1yUWOJElqkoscSZLUJDM5w6zrID2+v5eD98zoSGNHVwZn6dKlqf7yl79cXOOxxx5L9cKFC1O9d+/eVO/fv7+4xvXXXz9kzYaCH3/8cXENzj3ORVeGWc0rOXCVeslL8eswI9pVR5T3/JNPPhmyrn1fI5nt8pUcSZLUJBc5kiSpSS5yJElSk8zkXIWuHjgR5R4o9yJZ1/Zm+baufXB7WfTmSvJQXXvptXtzufevH3r5Ghwn5itGBg/g3LRpU6q/9a1vFZ+zYcOGVL///vupfvvtt1N99uzZ4hr8HGZumKWoGY2xPNbU5hHmnaZOnZrq6dOnp5p5qYiyjxH7HvHA1VqmitedMmXKkN8H57uIchydP38+1RyL/Pia4Xxm+UqOJElqkoscSZLUJBc5kiSpSWZy/l+9ZGG4rzpjxoxUs/9FRMSZM2dSzT1s7lfWcj3cR73ppptSzT1RMzn/h/ePv8cJEyYM+f6I8nfNmnvac+bMKa4xefLkVPMe84wg3s+IMmPx1ltvpfrdd99Nde2cNY4LfkxtD19X78Ybb0z1HXfckeof/OAHqb7//vuLazAbsWXLllRv3rw51SdOnCiu8d5776W6Ky9o3ubK1J4lN998c6rnzZuX6rVr16Z60qRJxTWYdeH95BzQy1mKzIfxmVZz+vTpVO/bty/VnFdqWa+uMxv7OfZ8JUeSJDXJRY4kSWqSixxJktQkFzmSJKlJ4zZ4zNDpzJkzi4+ZNm3akDVDqLVrHD9+PNUMlbLBUy2kdeuttxZv+ywG0BjiGq+6AtsMA9ZCw+vWrUv1kiVLhvyc2hjgWGPAk8HjWvMsjqPdu3eneufOnalmODCiHGscJzZ6u3q1fxxYtGhRqn/4wx+m+sEHH0x17e/9ueeeS/WPfvSjVHeFP2tvY9C8l+CxY+LKMODLQ1nZ7JH/0BARce7cuVQfOXIk1fxnhNpzgP88M3fu3FRzfqs1Azxw4ECq+c81/CcJzm8R5XNuOOceX8mRJElNcpEjSZKa5CJHkiQ1qdlMDvcS2ZBr+fLlqeaeaO1juG/IxkmnTp0qrjF//vxUc//y4MGDxecQD147efJkqpnJGY9qja+YyWHWgfvijzzySHGN22+/PdXcw+becq2RH9/GzA2bSLJ5YO1jmC/qOnwxomwWpqvHcccDDiMivve976X6G9/4RqrZ+G3btm3FNf7pn/4p1S+88EKqmb+oZYP4tq4chJmcK1Obizj3MPuyYsWKzmsMDg6mmg0fWTODF1HOLcz+MKdYywZduHAh1Ry/nKtq3wcN57jylRxJktQkFzmSJKlJLnIkSVKTmsjk9JLHYL7m4YcfTvXf/d3fFddg1mXPnj2p5oFptZ4C7E3BXAR7HdRyE7wuP8Z98noGgX2NVq5cmWpmI2oHI06dOjXVzF0xP/H6668X12Amh/eLfVT4fUZEzJ49O9UcE4cOHer8Pq6kL4qGxqzfpk2bio/5zne+k2r2UmJPo3/9138trsEMDnumcPzXcl3s1VLryfVZjocrUzvolwdyrl+/PtXMbu7du7e4Bv+m9+/fn2r2p6n1SuIzi7lSzivM6ESUOZ2uPmC1cTaSh0j7So4kSWqSixxJktQkFzmSJKlJYzKTw33DWh5j2bJlqb733ntT/Td/8zepZmYnIuL3v/99qrdv357qN998M9WHDx8ursHeBcxnsK7tX/Ln5R4o39+1194C/szMG0RELFy4MNVf/epXh6zZeyai3AfnmNi1a1eqecZURHk/mPPh/eT+fUT58zEvwT45tf34rkyOunHcLViwINXf/e53i8/h+We8Nz/5yU9S/cQTTxTXYG8S3n+OIWYtIsoshf21hkdtHuEZd5ybeP+Yt4ko5yJmudiPptYri/lV1syYsedNRJnT4fzGs6rM5EiSJA0DFzmSJKlJLnIkSVKTXORIkqQmjYngMcN+N9xwQ6rZ9C0i4mtf+1qq/+Ef/iHVt912W6o3b95cXONHP/pRqtmQi0EvHpIXUYYMWTMcVmvAxXBYL5/TOo6BWtDyrrvuSjUP4GRTtloDLgaNn3vuuVQzfM4AcEQZ3mOzMAb5GGaNKBsGMoTIscjwX4Tjph8YAGcDyS996UvF53CsHjhwINU//elPU1076Jf3imOGYfWNGzcW12CDQI4Zft1acNUxU+LziQdWRpThcx4OzH9Y4D+sRJThcwbH+Wzp5YBVjk3+E0/t2cpDpjlO+Bwc7X9w8JUcSZLUJBc5kiSpSS5yJElSk67JTE5XwyJmHO6+++7iGjwUb/HixalmpmHnzp3FNQYHB1PNLAUbONXyGF37k/zZetlHvdz3t6DrAEI2f4yIuO+++1LNBlzcW67lsrZu3ZrqY8eOpZr74NzjjiizPzyMj40oV61aVVzj1ltvTTXH5ltvvZVq7t9HjP7e+FjEccfs15e//OVUz5o1q7gGD0X82c9+lmpmdGqHIrKBJMcQm52uWbOmuMY777yTao5/5kBq2aBa7vCzxsNcRMzk8O89osxIMdvFwzWZ9YvoLQP6WbWDQpkX4rzCcVbLF7FhYJfavDOS48RXciRJUpNc5EiSpCa5yJEkSU0a9UwOMyk13POcPn16qr/4xS8Wn7Ny5cpUc1+ce561/Uv2vGAOgocz1jI53NPnPio/p5f9y/GYreA+MDM5tQNWV6xYkWrmZZhJ4P2NKMceMxd8f61fD/NCzEts2LAh1cwORZR79swG8f21AzqplzzYeFKbizhmOM54L2uHBb/66qupfvHFF1PNscz8YETEkiVLUs35jTWzFhHlz8eDE9mr5d133y2uwawX56LxOIaYzWQPnNrbunqf1XoUMYvK3z3notpBofw+ONbYj6s2nzGnwwwZ/2b4fUV0P/f7OY58JUeSJDXJRY4kSWqSixxJktSkEc/kcH+ul7OauM/N/MycOXOKazD7wgwD95trvUnmzp075DXOnz+fau5x176Po0ePppq9K8Zj3qaGY4D73tyfru378nN4L5gvqI0jfh2ORe5Hz549u7jGlClTUs1cDzM4tXzY4cOHU80+T8wX1cYi/9bGY37iszjGamOI9479aHgv+fccUd475mV4xlptLuJ5ZswlduUzIsoeKMz58PwrZg5rxuMY4hzAjEotU8d5gnMT318bA8zYcD7jNXi/I8pnGscexzP7+dS+DmvmJ2vzGeen2nzVL76SI0mSmuQiR5IkNclFjiRJapKLHEmS1KRhDx4zAMfQVi1oy89hSOu2225Lda2x0MGDB1N9+vTpVB85ciTVtWZyDJStXr061Qxl7du3r7jGk08+mWqGxcZjcO9KdDVRPHfuXPE5DHxyHHU1eosom3JxbDJk18v95LhiWJkH8UWUDeV27NiRaja77CV4PN50HfxbC0gyqMnGjQwmMwAeUf6TAwPuDPzyMMeIMtzJr8MDWmuHfHIscxzy+6iFl/m24QyMXqs4TjgGamHd9957L9Vsssdg+UMPPVRcg/ecDT8ZNK4Fj3nPGTRmoJ1zZkR5z7v+WaaXZoDDOTf5So4kSWqSixxJktQkFzmSJKlJw57JYWbhSvbieA3uE9byGMz+sJEfr3HixIniGtxL5L4p92bZXK72vXFvdrznJD4Pfy/M5LAR4/79+4trMA/FRoy9HCTH/WbeP35ftTzNzJkzU33PPfekmuN7+/btxTWeeuqpVHM8c39+PGYlLhfnIt6HiDI/wwMMOU5rcxHvRdc1a9/HyZMnU83MIfMatcaWxPnrSg7bHA/zV1d2q5fnEZt38h4zP7N+/friGpzzmLFiFqiWMeP8xPtXmwOJhwFzbDIvWTtslH8TZnIkSZIuk4scSZLUJBc5kiSpScOeyWFmgTmI2oGGPMCO+4Q8AK2258eD8ni4GT+nlmFg/oJ7nMz91PrkcH9yJPciW8L7xb4wb7zxRvE53H/evXt3qidPnpzqWk8I4tflOGPPjIiIr3/966meP3/+kNf81a9+VVxj586dqWb+q5f+S461ofEw1oiyTw77z/DvmdmoiHLu6Tpokf2dIiL27NmTavb9Yqajlq3g987eOsz51PqfcAyNZL+Ta1XXPBNR5mWYa+E447Mlopxr+HziHMn+TBERb775ZqqZB2P/nlq/JWaOTp06lWrOZ7W/iSvJf10pX8mRJElNcpEjSZKa5CJHkiQ1adgzOcS9YvYQiSjP12DPAJ4hxX3FiHJ/klkg7iNyLzIiYt26danm3jlzIDxPqPZ1us75UB1/b9znrfWm4B4296O7Mla1t3VlzG6//fbiGg888ECqmf1g3mLr1q3FNZif4M/P8T4esxGXi3kSZmUiynHH3zPHQy3Xw7mF94b3stZrieOQGQ6eO8XcV0SZ42LOi+f5ffDBB8U1xuO4utyeXbVsJj9m7969qe7lzCiOV85vHDf8mhHlGLj77rtT/eCDD6a6lsnhM5xzby99ckZyHPlKjiRJapKLHEmS1CQXOZIkqUkuciRJUpOGPXjMgBGDmmwcFFEGj1euXJlqBo9roWGGkRmWOn78eKp7OViPByf+4he/SHUteMyGTeoPjqteDoGrNaX6rF4Op2P4j2N1w4YNxecwOM/g4jPPPJPqWjMxNvbqpfmfhsb7XRsfg4ODqWbDPIaV2fQtImLFihWp5r1jY7haY9Lly5enmg1TGVythZdfffXVVDN4zGA+A6S17208jruuQ3lrvzd+DA/Y5bzSS2NSPls4fmv3hv9swQB7LwdIc+xd6/8E4Ss5kiSpSS5yJElSk1zkSJKkJg17JqeriVutYREbW7HpEfc8aw32upp28f3cj44oD8p76qmnUv3444+nupYvsvnf6OFecC3r0PV+7pUzx8FMzp/+6Z8W12BDLTaRfPLJJ1N94sSJ4hpszDba+9wt4O+wlqVgE8ZXXnkl1by3nLsiIqZNm5Zq5hJ5UCzHVEQ5DpmtOHDgQKpfeuml4hrM5LAh3ZkzZ1LNOTPCcRfRnQeszSN87tWeFVf7fXCMsI6IuPHGG1PNMd9LNog5tF6yQKPJV3IkSVKTXORIkqQmuciRJElNGvE+Ob3kaV5++eVUc6946dKlqeZBmhERs2bNSjVzDszgcH86IuL06dOp5gF2vfQUUFuYhXj44YdTXeuTwz1r5iW2bduWag9GHB78HXLuqf3e2U/r6aefTjUPPKwd0DpnzpxUM4PDwzdreQ1mgzgXcf5iP5+IiKNHj6aa8yrnZsdcb/h7upZ/b105HmZ2aj8Lx0kvPX1Gk6/kSJKkJrnIkSRJTXKRI0mSmjTsmRziPnitNwUzDDxjhX1GtmzZ0vl1ud/e1b8noux/UDsfSe2o9ZXg23hO2g9/+MNUs4dERJmn+MMf/pDqU6dOpfpa3tNvSS9zAPt48fy6Q4cOpZqZnYiy7w1zXV1nAdXexjHCbBDPRooofxZ7L7WlK29Tw4/hmKidgTZx4sRUsw8Uc2y1vkEjOdZ8JUeSJDXJRY4kSWqSixxJktQkFzmSJKlJIx487gUDgax5YCfrCEN06g8eprh69epUL168ONVsEBlRHp64Y8eOVNeCphp+Xc0BI8p7w38+4P1mA9GIcgyx+R/rWlCT3xs/hv/A0cs11LbaM5BjgGF0NpFkKD6iDCuz8e7u3btT3XUw8nDzlRxJktQkFzmSJKlJLnIkSVKTrslMThfzNhoOteZZPLCOh8Eyb1HLZGzevDnV+/fvT/Vo71nr//SSWem6V7UxxNxOV5O22vzmnKd+4Phl08hf/epXqR4cHCyucfLkyVQz13OtHVjqKzmSJKlJLnIkSVKTXORIkqQmjclMjjQcanvH3MNmn5SjR4+m+ty5c8U1Hn/88VRzT3u096zVP73cS++3RkJtnHH+YubmJz/5Saqff/754ho8DJbz2bV2kLWv5EiSpCa5yJEkSU1ykSNJkpo0MNT+8MDAgJvHjbp06dLQzTr6aCyPowkTJqR63rx5qV65cmWquT8dEbF3795U85yhsZzRGKlxNJbHkIbmXKR++Lxx5Cs5kiSpSS5yJElSk1zkSJKkJrnIkSRJTTJ4PE4Z9lM/GDzW1XIuUj8YPJYkSeOKixxJktQkFzmSJKlJQ2ZyJEmSxipfyZEkSU1ykSNJkprkIkeSJDXJRY4kSWqSixxJktQkFzmSJKlJLnIkSVKTXORIkqQmuciRJElNcpEjSZKa5CJHkiQ1yUWOJElqkoscSZLUJBc5kiSpSS5yJElSk1zkSJKkJrnIkSRJTXKRI0mSmuQiR5IkNclFjiRJapKLHEmS1CQXOZIkqUkuciRJUpOuH+qdAwMDl0bqG9HIunTp0sBIfS3HUbtGahw5htrlXKR++Lxx5Cs5kiSpSS5yJElSk4bcrpIkScNrYCDvtHzhC92vP3z66aepvnTJnbgaX8mRJElNcpEjSZKa5CJHkiQ1yUyOJEl9UsvTXH99ftTecMMNqb7xxhtTzbxNRMTHH398WXUto9OV2+kl1zPWsj++kiNJkprkIkeSJDXJRY4kSWqSixxJktQkg8eXgQ2b+mGshbgkXR3OIwyl1oKrH330UaprwVRdGxgijoiYMWNGqmfOnJnqrhBxRMSHH3445Me8/fbbndf45JNPUs1x1PX+Gj7DOL5H+xnnKzmSJKlJLnIkSVKTXORIkqQmNZvJ4b4g90knTJiQ6ilTphTX4D7qypUrU33TTTel+s033yyuceDAgVSfO3cu1e+//36q33vvveIa3CfVtYP5iVqegm+77rrrLuv9tbexZq7jgw8+KK7BXEfXHr+5j2695A9uueWWVN91112pfvjhh1PNuSki4rXXXkv1Cy+8kOoTJ06kmvf68743XT029mMdUT5f+Ozg328N/x7ffffdVHMe4fsjynHBZ0vXnFD7Pq51vpIjSZKa5CJHkiQ1yUWOJElqUhOZnFr/mkmTJqV6+vTpqZ49e3aqFy1aVFzjvvvuS/W9996bauYijh07VlzjpZdeSjX31pnZqV2DuZ3aPqmGB+8x8xVz5sxJNcdZRMTEiRNTPXXq1CE/5+abby6uwUwZ99+Ztzh58mRxjXfeeSfVhw8fTvXBgwdTffr06eIa5sOGVstW8N5xzHzzm99M9W233VZcY8WKFalmpuN3v/tdqt96663iGl0HOJrZ6Q3/9nh/+fdee9uSJUtSzRxP7V4wT8PMza233ppq5j8jyswna84RvYwJZnSutXHkKzmSJKlJLnIkSVKTXORIkqQmjclMDjM4s2bNKj5m+fLlqV69enWq169fP2Rdexv3Vbn/zj46tWscPXo01f/5n/+ZamZ4IsqsBM8oudb2QMeKrjOEIsq8zNq1a1P99a9/PdX3339/cY1p06YN+XWZ++HeekSZy+LnDA4OpvrUqVPFNbj/vnPnzlT//Oc/T3VtT99MTtbLGOK8wfwFc1481ygiYtWqVanmvXv55ZdTzTkiosxOXOtZirGCmZxapmrdunWpZiaHY6R2jYsXL6b6woULqWZmpzYHsJcbM6HMctXGRK0H12ddSR+d4Rx7vpIjSZKa5CJHkiQ1yUWOJElqkoscSZLUpDEZPOZhZwx1RUQ88sgjqf7Sl76U6gULFqSazQEjyqZP58+fTzVDh7UmfQyU3X777almgLR2sB6v+/rrr6e6KwimOv7ua4FPBon/6q/+KtWPPvpoqidPnlxcg4Ffhvv4/lpzS44jhvsYVq4FTxlEPHPmzJDXHGsH8Y2GXhrqcR5ZuHBhqhlWrv3e2SCSwVReo/Z92PyvP/j3yb/N2rOEzRxZs0Eo/9Egojy4lWOA44bPmojyINdXX3011du3b0/1kSNHimtw3PCZ1ctBvyM59nwlR5IkNclFjiRJapKLHEmS1KQxkclhs6U777wz1X/xF39RfM6Xv/zlVHOflHuCbJIUUWYleGAh389mTbWvy71YNoX6/ve/X1yDmZuzZ8+m+vjx48XnqMRsBLNdd999d/E5HFvMejGDw3sTEbFt27ZUs5Eb97D5fUaUDS/5N8HGYLWDXp977rlU79u3L9VsVGkmp9uV5FzY/I8ZDzYLjCjHKjMcPLDTvE3/8P6w5u+6dtgzG4Ly/jHXwr/FiPJATua0unJ7EWVz0w0bNqSazT5rGVHioZ58XtUaiPJtw9lk1FdyJElSk1zkSJKkJrnIkSRJTbomMznsX8K+Et/5zndS/dhjjxXX6Oo7cPjw4VTv37+/uAZ7BDD7wr4jtX41PKCT++3z589PNfv3RJQ5nkOHDg35fen/cO+cORaOK+5PR0SsWbMm1dx/Z1+Jp59+urjGE088kWreL+6lc/8+ohwn7OnDvNju3buLa/Cg165+PWZyLl8tW8BxN2nSpFQzg8X5L6L7kE9eo9ZryT45/cHfLTNWfPZElJmqDz/8MNVvvPFGqpmXiyizexwnzAfya0aU2T5+zuLFi1Nd67fF8cx5hJ/DzE5E+fMP52GxvpIjSZKa5CJHkiQ1yUWOJElq0qhncmr7zzNmzEg1e8ew5sdHlP/fv2PHjlS//PLLqeaeaETEuXPnUj04OJhqZhxq2NuA+7esaz0y2NuAdVffhvGKOQXulTODc9dddxXXYP8RjqNf/vKXqd66dWtxDea9OOaZwamNgZMnT6aa44p9NWpjk2dVMUNmBufq1e4dzxXjxzBvU5sTiT1SOB5q+UDv75XhfMr5ln+/zFxFlM8j/n0yQ7d3797iGl1n3DGDs3Tp0uIafFZyTmRGh+dj1b4ucd6t9f1iTqerb87VPNN8JUeSJDXJRY4kSWqSixxJktQkFzmSJKlJIx48ZgipFmz68z//81T//d//farnzp2b6lrIjmFPNldiE7c9e/YU12DQmEFkBsFqASsGV9kckA2e2KwpojzEU6VaGI6/+3nz5qWa92LOnDnFNc6fP5/qXbt2pZqBQY6R2vfBICqDigyj167BRpQnTpxINb/viO6D9AymXj6OuwkTJhQf0xXuZBCZzdYiynmC95IN2YbzwMPxjiFYBscZCo8om9HyQGg+f2oH7LKBHucEBtb5fUVEXH99fuTzmcWANL9GRDlfMfDMr1H7ffCfIDhe+Wy9mvHsKzmSJKlJLnIkSVKTXORIkqQmjXgmh/uG3/jGN4qP+cEPfpBqZiW4D15r5Pe///u/qX7ttddSzaZutSwF97m7DhWrZXLOnj2bau4tMn9Ry0Xw8EWb/5VqDdSYbWDeiYdcci85IuLixYup5l4xcz6zZ88ursF97q7sC/MXEeUeftcheMwARIzsoXjjBf/ma80AiVkojrvaWObnsDkk77/3cuTwd13LiHLeZ+aGczybO9a+DrMuzH4xuxpRNpLl3MQ5szYn8mfh3wAzOr1kzPg7Y20mR5IkCVzkSJKkJrnIkSRJTRrxTA57kzz22GPFx6xZsybV3PM7dOhQqn/84x8X1/jv//7vVLOPCPMIFy5cKK7R1Uekl2xM12GM7JlS279kTwHu345H/N3X8lDsWcI9bP7ue8n1rFy5MtXLli3rvAZzPNyPZ++dI0eOFNfg3jkzGF3ZoYhyX9u+OFeP4652/7v6lzDHUxvLXX1WmLeq9Y0yp9MfXb/H2t8e8zJdPdZqPW44ThYuXJjqjRs3pnr+/Pmd3wefJfzZatk+Pis5r/D75DMvop47/Cz2+aplcnqdv3wlR5IkNclFjiRJapKLHEmS1KRhz+Tw/+zvu+++VDOjE1HmILg/9/TTT6eaPXEiytxD11kYzDxEdPcRYV3bj+d+JDMdPBuktlfJjMbRo0eLjxnveslD8X4yp1XrccNzWrgPXjvbhZipqvXA+Kza/jPHL/svsc8TMxoRZnCGA8dd7e+X8xfzF5wja/eJ9599kniN2nzWlSE0s9Mb/p54f2u/e/ZDY+83Zl9qGVE+S9auXZvqDRs2pHrp0qXFNbryrQcPHkw1x1lEObd05Ws4h0aUczNrZs5qc2KtH1GNr+RIkqQmuciRJElNcpEjSZKa5CJHkiQ1adiDx0uWLBmy5mFeEWWw6fXXX0/15s2bU11rnsbgFsN8XWGpiO4gHoN8DJdFRKxatSrV69atSzV//u3btxfX4GF8tTCYSgyrcUycOnUq1Qy7RZQh4cmTJ6eaDQdrzRzZqI+fs3z58lTXDos9fPhwqhl27GpcqeHRy+GMDJ5z3PFe1hrB8X7OmDEj1Tx48fjx453fq0Hj/uDfd+3Zwn9qYFCcjUk5JiLKsDKfpYsXLx7ya0aUY49zDecvzncRZQiewWKOzdo1+Dl79uxJNZ/pV/PM85UcSZLUJBc5kiSpSS5yJElSk4Y9k8N9xFmzZqW6tn/51ltvpZoNi3bs2JHq2n4dr8s97V4yC8zcsOaBjzwgLSLi+9//fqp5aBqbID311FPFNbhfWTsAbrzp5YBOYpMuNgOsqWVsPovNADkmIsoD65jD4t45D9GLKA/k7GpuqeFRO/jys2qN4Hjv2HCNua9e8oIcI5xXBwcHi88xp9UfzDLxb5EZnYgyZ3XHHXekmnMAm31GlPMInyV81jLnE1EeyDlz5sxUM9tV+1n4Mfw6/D6YP4ooMznMS/L9tUa7vfKVHEmS1CQXOZIkqUkuciRJUpOGPZPDfUT2kqkdcMj95L1796aaGZwr6XnTlbeJKPcBuU/IfdW//Mu/LK7x6KOPDnmN3bt3p/qJJ54orsFM0njMX3RlcGr9lrg3zLwM8xPMekWUfSRqB+d9FvsgRZSHet59992p5sF7vew/dx0eq+HR1Wumdh94r5jBYV+w2jWYpeAcwFxELaM2HueNkcDnT+0AXn4MMzjMVNUyObx/fJZyDNSyMJwn16xZk2pmEGs/C3tBcV7lNWrfB38+XoN/M7WsW698JUeSJDXJRY4kSWqSixxJktSkYc/kcC+N+3O1c1q6+kbw/TwLqIZfl3WtH8ptt92W6nvuuSfV3/3ud1PN/E1EmbdgD6B//ud/TvWzzz5bXKPWq2C8YSaH44Z72hHlvjczOtz3rZ33c/To0VR37UcvXbq0uMaCBQuGrDm+a9kJfl3zFaOjq09OrRcNMzfM5XH+qmUM2WuH8wozOLUcBL8PXZmu88p4ryK6z15ivxrWEeXYYnaP+dZaLosZ2a6eNrV5huOT44q/j9qZgMyZ8hnH8956yd1+Hl/JkSRJTXKRI0mSmuQiR5IkNclFjiRJatKwB48ZuGIoq9bkhweAMai5bNmyVNdCdgxp8UA7hv3mzp1bXOOP//iPU83mf4sWLUo1v++IMmj8H//xH6n+6U9/mupaQG08NnpjaI6BuRkzZqSaIfGI8n6wedaRI0dSXTuwsyusyXG1ZMmS4mNuv/32VHOs8cDGWlCPwbzxOCauBV3N/2phT447jmXWtTmA12ConnNgLSDN780DO69MV/CY/6wQUTYa5bODz6PaP1J0/YMNQ8K1McCGge+///5l1RHlXMT5iv/A8cYbbxTXOHz4cKq3bduWagaPr2a+85UcSZLUJBc5kiSpSS5yJElSk4Y9k8PGfcwf8ADEiHI/8uGHH0419yZre37MUjDHw7wG90gjIlasWJFq7oszT8R9xYiIf/mXf0k1MzjM7Ji1+D9sdMXmf2yW1cv9YyaBv/vaYbHMOjCD88ADD6T6G9/4RnGNO++8M9VsbPXyyy+neteuXcU1uO/vOLk21Q5XZQ6Ccw/HZS3XM2/evFR3HfJps8iRw999LZPzzDPPpJoH/XKOWL16dXEN5hB52CbveS1zxbnnvffeSzVziSdPniyuweft66+/nmr+/MzX1N7GHFrtYNAr5Ss5kiSpSS5yJElSk1zkSJKkJg17JufUqVOpfvzxx1O9du3a4nOYa2BPiK997Wupvv/++4trcF+b++LM9dQOCiX+LP/1X/+V6h//+MfF57z22muptt9Jb/h7YTaG+9HLly8vrsF9bR7Cyj4SPPSwhgdw/tEf/VGq16xZ03mNLVu2pPr3v/99qmv74FdzQJ2GD8dQrZcJ+yIxK8a5h9m/iHI+Y5aC+cCug0TVP8zCnD17tviY3bt3p5q9ZV588cVU1/p+MavKXCLnyFrGkOOGmVjmafjMq30Oe9/10guPOSbO98wT2SdHkiQJXORIkqQmuciRJElNGvZMDvfntm/fnup/+7d/Kz6HvUg2bNiQau5Z1/afuafHPUD2Kaj9Xz73STdv3pzqZ599NtXsFxBhf5MrxfvHmtkm7jVHlHvSzEbwnKna3jFNnjw51XPmzEk1x1VExJ49e1L9P//zP6l+5ZVXUl07u8hzhq5NvWRybr755lRzTmRvnVqfHI5v5j66Mg6f9zb1X20e6eoDw55dtWdJVz+lXvotMT/EMcH3187u48d09ee5krmrn2PVV3IkSVKTXORIkqQmuciRJElNcpEjSZKaNOzBY4aSGBh9/vnni8/5x3/8x1R/5StfSTUbCNYOZ2RjpOPHj6eaYdD9+/cX12D4i5/Dpki1Q/EM+12ZruA47+fOnTuLa7ABJIPIbCDIMRNR3j8GyQ8dOpTq2uGaDLD/+te/HvIaHq44djB4XAtqHj58ONVPPPFEqhlenzRpUnENXpcNJTmGat+Hc9HoYTNPzm+9NHO83AaPV3KNrqZ8tY/pqnv53q7kGr3ylRxJktQkFzmSJKlJLnIkSVKTBoba+xoYGOj7Ji734mp7hMxGsAEbD2esHUTGQ+/ef//9VJ8+fTrVtSZuzF9wX3Us73FfunRpxE7w68c4YsM0Nl1jA8mIiMWLF6f6i1/8Yqp52Ob06dOLa3CvnFmgI0eOpHrr1q3FNQ4ePJhqjrWxnMEZqXE0HHNRP7DhWi/NAJkVY6aQYz0iYnBwMNUch2wuN5aaR461ueha0ZWv6cchrSP1jOvH1/m8ceQrOZIkqUkuciRJUpNc5EiSpCaNeCZnONQOIuN+JPeox3Keph/Gwz44sw033nhjqpnb6gUPSmRuayzna66EmZw891xJjxAaS3mafhgPc1HLunrejBQzOZIkaVxxkSNJkprkIkeSJDWpiUyOLp/74OqH8Z7J0dVzLlI/mMmRJEnjioscSZLUJBc5kiSpSS5yJElSk1zkSJKkJrnIkSRJTXKRI0mSmuQiR5IkNWnIZoCSJEljla/kSJKkJrnIkSRJTXKRI0mSmuQiR5IkNclFjiRJapKLHEmS1CQXOZIkqUkuciRJUpNc5EiSpCa5yJEkSU1ykSNJkprkIkeSJDXJRY4kSWqSixxJktQkFzmSJKlJLnIkSVKTXORIkqQmuciRJElNcpEjSZKa5CJHkiQ1yUWOJElqkoscSZLUJBc5kiSpSdcP9c6BgYFLI/WNaGRdunRpYKS+luOoXSM1jhxD7XIuUj983jjylRxJktQkFzmSJKlJLnIkSVKTXORIkqQmuciRJElNcpEjSZKaNOS/kI9lAwMDl1V/4Qvd671Lly4NWV/J5/RyDUmSdPl8JUeSJDXJRY4kSWqSixxJktQkFzmSJKlJYyJ43BUSvvHGG4vPueWWW1I9bdq0Ia9x0003FddgKPjjjz8e8v0ffvhhcY2LFy8O+TGs33vvveIan3zySfE2SW3o5Z8gPv3001T7DwtSb3wlR5IkNclFjiRJapKLHEmS1KRRz+RwP7r2tuuuuy7VN998c6rnzJlTXGPZsmWpXrx4caqvvz7/6LVMDt/GXM8777yT6hMnThTXOH78eKqPHTuW6tOnT6d6cHCwuAZzOu7Hj221MX+1HBOjo5d7yXlk9uzZqV6xYkWqmSesXePtt99O9ZEjR1Jdm4uYD/zoo49SzeyfY6o3V/L3PBKf08uztZcmuMRx0ZUX4/trHzOcfCVHkiQ1yUWOJElqkoscSZLUpBHP5HAPsLY3x31D7kfPnDkz1dzTjoi4++67Uz1jxoxUs7fOxIkTi2vw6yxatCjVzPUwbxMRsW/fvlTv2rUr1du2bUs1cz4RZS8d9utRb7r2o3vZ8+7HXnrXNXr5m+g66LW2D072Xrl8XXnBWraPc8+9996b6m9/+9upZn4wohyr/Lq7d+9O9c9//vPiGpxrzp49m2rmfGrzzHgcI1091SZMmDBkHRExadKkVDPfyfezjiifURyLH3zwQaprzxLmsPi9Mpd17ty54hoXLlwY8usw+/Xuu+8W1+DY6mW+ulK+kiNJkprkIkeSJDXJRY4kSWrSsGdyuvIHtf/Tv+GGG1LNfUPuV27YsKG4Bnvn8HO4r1rbS2cmhxkc7pEuX768uMb06dNTPXfu3FRzj5T75BER58+fT7WZnN50nXHGe85xV/sY9miaMmVKqmfNmlVcg1+X+97cs+b9jijzEhwn77//fqpr56hx35sf4xlpl49zQi1LsWrVqlQ/9NBDqb7rrrtSPW/evOIazDkwf8G557777iuuwXwF7z+zFbXxMB4zOcw/TZ06NdX8m1+4cGFxDeZGmbti3pPPiYjyOchnK58LtUzOmTNnUs0xwfrkyZPFNQ4cOJDqo0ePppo9m3qZV/gc7Oc485UcSZLUJBc5kiSpSS5yJElSk1zkSJKkJl2TB3QyMMpwH5trMbQUUQY1GWRiAKvWDJChK36v/D7mz59fXGPy5MmpXrJkSaoZmmbzwIgy6MUDO1UfR12BdQYIa4HPtWvXppqHK7KpV+1wRX5vDHwyeMxmWxHlQa8cJxwjDKpGOG5GQm0e4Ri64447Us37/9RTTxXXYNCc44xjigH5iPKfMQ4dOpTqrgaT40FtHuE/H3DeWLlyZao3btxYXGP9+vWp5lxTu1+9fG+fxeckA9MR3Y0Mec9r/8DA5x6fpfy6w3Eg8eXwlRxJktQkFzmSJKlJLnIkSVKThj2T07WvW9uv6zqgk3t+tfwBG7vxY9go6dSpU8U12HCLPwvzNdxrj4i48847U839XDYc5PtrX1el2jhiPoJNu9atW5fqBx98sLgGG3tx7HEcnT59urgGxx6b8jG3VWsmxgwGvw/mOmoNI83kDL9aM0DmL3hvduzYkerNmzd3fh3m/5gV66UBG/MWzjN1bObJBq+cs5l9iigzN3y28HBn5lwiyoafxCxr7dBLPhc5b/Dr1prTcj7jWOM4qo3FkWw86is5kiSpSS5yJElSk1zkSJKkJo2JPjnMH7AHDnuIRJQ9brj/zF4kPJgsosxb8HvlfiUPa4wocx/EfVP+bBEeyNmL2uGazCnwIMTHHnss1UuXLi2uwRwL+9Ps3Lkz1YcPHy6uwT5OzG2sXr061czoRJT7/BzPBw8eTDX3+CO69851+Tgn1H6n7EXCv/FXX3011bUxxLmFGS0e6FjLPPDrdh3QOh7HR+1n5u+J2ZheesvwWXLixIlU7927N9W1ZxqfWV29kTjuIsp5ktfgs6aWA+LbOEdyvqs9v0ayJ5Ov5EiSpCa5yJEkSU1ykSNJkpo04pmcXvawmVPhviH3AI8cOVJcg9flnij7A9R6iPAa7NfTS98RnifS1Vel1h+Bv49efoetY26rlmNZtWpVqr/+9a+nmj2Maj0hXnnllVRv2bIl1czC1O4fx8DixYtTzZ4Z/Nkiyt4c/Drc966d58ZxNB7HTb/1Mg6Zg+i6d+zLElFmcpYtW5Zqjo89e/YU1+D5ZpwDHQ91vD/Mb3JOr+VYOLfwGrwXtYwo38ZxxQxOrU9O1xzQS76IPx/75nA+q30fIznWfCVHkiQ1yUWOJElqkoscSZLUJBc5kiSpSSMePGbgqJdQEuvz5893XoOBKYahejmcruugUIYOa4dr3nrrranuCh4zgBZR//nGO4bs2AwtIuL+++9P9aZNm1LNJpNsyBUR8eyzz6aazf+6QqQRZWiU44hNvHj4YkR5kOuhQ4dSzUZhte/DZm9Xj3MCQ+W8lxFlCJz3csGCBamuhZcXLVqUao53Nn/ctWtXcQ2O1V4CoirvH5sqMhBcC/1zrpkxY0aq+bdZuwafLxxrnFdqDVKJX4ch4lqzXs4tXQdyjvY84ys5kiSpSS5yJElSk1zkSJKkJo16JqeGWRfuFfdyAFjXx3CvsbZ/yf127omuXLky1WzQVbsGsxNsElU7oNO98u6mazzkMqI8kJP3jxmczZs3F9dgtuGtt95KNccV98UjyoNC582bl2rmLWqZHP78PICWe+m1hmSjvTc+HtR+74ODg6leuHBhqjdu3JjqWg6C2T5msti08vXXXy+uwTHivNIbZkyY5+QczsxoRHnALuciHrjK5o4R3c1pWTMHFFHOVxwT/FlqDVL5dTjmuxodRtQPkB0uvpIjSZKa5CJHkiQ1yUWOJElq0ohncpgtqGVhuP88ceLEVLNHSk3X//Lz6zI7U/s+li5dmuqvfOUrqWbWIqLc92am480330x1L4d+jscDOnnPmclZvnx58Tk8CJO/N/Yoqh2Kx8MSb7vttlRz3MyaNau4xtq1a1O9YcOGVK9ZsybVtZ4/PISW+/7cF+9lz3s8jqPhVsvUMffArASzfLX5jRksHgz78ssvp7o2lrsO+nU81PH3wGcL73ntkN6uQ1inTZuWaub4avgs5biqZbuYVeXzhv17avki5oWYL+Lz+tVXXy2uwfE8nBkdX8mRJElNcpEjSZKa5CJHkiQ1adgzOdw35F5krR8A8xXMQUyaNCnVtX4AzEowg8OafQpqb1uyZEmqmdHhXmREuV/LDM7hw4dTXev5Mx5xP5k5BZ7bUtt/5hjoyiCsWLGiuAbPGeJ45hip7aVzn5vZLWZw+DcSUe5hs671ZyEzF/3H85+Yv4ko8zG8V71kDtlr5MSJE0O+v3b2ETM53v/e8PfEPjmc4zmnR5S9r/g3zlxLLatae859FuemGo5PPm84nmtzEfvDcT7j87qGZwLyd9jPsekrOZIkqUkuciRJUpNc5EiSpCa5yJEkSU3qe/CY4UaG6hhKuvPOO4trbNq0KdU80I7XrIVOGdJikItBr1pYiuFWBgIZ9HrnnXeKa/AgPQaPGfSq/SxdB5aOB2wWxfAmG/tFRBw9ejTVbLrGxn333HNPcQ3eUwY6OSZqQb2uRmAMJnNMRETs2bMn1WwGx6ZeteAe32bwtNvlNsxjKDWiPOTw1KlTqWajxylTphTX4N885y8GWzn2I8pxOB7nkX7gXMTQLO9nRPnPMpxX+A8MteBx7dkw1PtrDfY4b3L88rnJuSmi/Mcg/gMOGxvWDvnk74iHJdeC81fKV3IkSVKTXORIkqQmuciRJElN6nsmh3t6zLps3Lgx1Q899FBxDWZy2JCN+YNaFob7k2w6OGfOnFRzjzui3BflNdlYqXYoHvcj+Tm9NDbk24bzMLNrRdeheDw4bufOncU1fvOb36SaB2My21XLqPCeMy/DJmwXLlworsH71ZUxe/3114trbNmyJdXc0+b3VRsjZnAuX9fvjH+btZwLr8Gxy3xVLdfFMbJ69epUc+7hgZ0R5dh0PFwZ/t74t8dGnRFlrpKHeN50002dX5dzUVfj2Fo+jN8rn3H8PmqHTvPnv//++1O9aNGiIeuIiAULFqSa2VUzOZIkSR1c5EiSpCa5yJEkSU3qeyaHfUO498b9u1omh3kZ7iVz/7m2B8r/76/1Hbhc3CdkNoh1RLnHyXwRs0DsMRBR7ueyL8N4yOjwd89cw759+4rP4bjZvn17qpkXqx2MyH1tZqpY1zIZ/Btgfx7evxdffLG4xiuvvJJq/g3w++wlG6LL13VwbO2gX95v5iK2bt3aeY3Fixenes2aNalm751ats/7Pzz4e63lSThfEZ9PtTmdf9P8m2dGp3ZoL8ceny081JhZmdr3cccdd6Sa8x2zkBERL730Uqprc2+/+EqOJElqkoscSZLUJBc5kiSpSX3P5LDvy4oVK1LNHjg8TyiizLYwg8K+ErV+AMxbdJ05U8vTcE+T/Xh6yWPw98Gs0OzZs1N94sSJ4hpde7G1s45aw98Bf+ba2VXM5Bw/fjzVzEvVcgzco+7Kvtx6663FNdgbipkzjrMdO3YU12Dmgp/juUSjg2OIf+8R5ZjguDxw4ECqa3mMJUuWpJq9lji2a7121B9d55nVzpji84Z/n73kaTguOAdybqpdo6v/TFdfsIgy/8i5l+dRcuxG1PvSDRdfyZEkSU1ykSNJkprkIkeSJDXJRY4kSWpS34PHDNmxMRCbWtWaADFgxTAfQ8U1DDYxUMUGggwRR5ShKx62ySByLXTIMCubL/H3VTsQjaG0t956q/iY8abrAM+I7sA2x0QvDdR4TQaTawftsQEmx8mxY8dS/dprrxXXYFiVAUIbvY0OjiH+Y0FExNKlS1O9a9euVDM0fPPNNxfXmDx58pBfh++vBUa7/vlCveHvrTZvUFfTPb6/Fhznc5FzHt/fyyG9XT/LlYwRPtP4s0WU8+Rw/vOMr+RIkqQmuciRJElNcpEjSZKa1PdMDg8amzJlymVfg3t6PLCOh1jysMKIiDNnzqT6jTfeSDUPHjt8+HBxDTZTYsNA5iJqWSHuPTLH08thm8wL1ZofqsT8TFeDvK5GWRHdjb+mT59efA4PqOPnMJOxd+/e4hocN+YpRgczC5zf1q5dW3wO8zIcZ8ws1OYRNlVl5pANUmvNTTU8eP9qzQC7MjnMYdVyPhw3fNbyc2rfB58vXVmgWsbsnnvuSfXy5cuHvGZtLPJ5O5zPNF/JkSRJTXKRI0mSmuQiR5IkNanvmRxmB5gvYA+Q2r4h97mZP2BGhXmbiIidO3emmnvW3BM8f/58cY133323eNtn9dIjhW9j3ohfgxmdiDJfVPudaXRwX5yH00WUGTLuP/OAxtohrbWslkYe7/fMmTNTzcNYI8reYOwJwsM2a4cWM5PDnl3btm1LteNl+PB5xKxMrc8RcyrMz0ycODHVtWwfr8uMIcdErZ8aP4cZHObBVq9eXVzjscceSzXHK59p+/fvL66xZ8+eIb+vfvKVHEmS1CQXOZIkqUkuciRJUpP6nsnhPuDTTz895Ptr5/TwvCvueTI/U9vz27dvX6qZc2Cup7aH3XVOEffna7jnyc/h+2tnePDnr53TpJHBPBTzY+yJE1HuxzN39dxzz6XaHifXDt471sxS1PqKrFu3LtWrVq0a8mvW/r45P7344oupfuWVV4b8ePVP17l57K8WUc77/Bhes5brYXZrzpw5qebzqdb3i+OX45X5wdpZirNnz041f34+jx9//PHiGszEmsmRJEm6TC5yJElSk1zkSJKkJrnIkSRJTep78JiBKh6EefLkyVS//PLLxTVqTfWG+hq1w734NgZ6GczrJfjEcBhrhroiyjAYg8YMstZCp/x5DRWOHjbx4uGLteApxyIPlGVQbzhDeLo6vDenTp1K9W9+85vic9hgbcmSJanmvMHmkBERR48eTfWvf/3rVLMhai+Hzao/OCZqc3jXs4NqB3RybpkwYUKqGRLmsyaiDDQzEM2fpRaCZ2NdPuOffPLJVP/2t78trsE5cDgPHPaVHEmS1CQXOZIkqUkuciRJUpP6nsnh3hr3J1nz8MmI0TmAcrj2BPmz8ADTkfo+dGV4/7hXPnXq1FTXshBsRMn62LFjqTaTc+3i/X3zzTdTzcaOEWXjUR7qyQMNa5mcI0eOpPr48eNDfo3aGHJuGR5dzQEjylwlP4aZ0QsXLhTX4LOTebBFixal+pZbbimuwbwr5zdmZU6fPl1cg/MX82CsOTYjylwpv49+jlVfyZEkSU1ykSNJkprkIkeSJDWp75mcfmhp77iln2U84v1j1oGHbTJfE1Ee4sjeUIODg0N+TY0e3m/2PGLfL9YR5eGZXfmDXvI0jpFrV+3eMMvFmpmds2fPFtdgLov5L/Zbqn0fXXlXjr1axpAfcyVjcSTHr6/kSJKkJrnIkSRJTXKRI0mSmjQw1N7YwMCAG7+NunTp0og1I2ppHHFPm30neE5RRLn/zF4U7H9RO5vsWs1gjNQ4amkMKXMuUj983jjylRxJktQkFzmSJKlJLnIkSVKTXORIkqQmGTwepwz7qR8MHutqORepHwweS5KkccVFjiRJapKLHEmS1KQhMzmSJEljla/kSJKkJrnIkSRJTXKRI0mSmuQiR5IkNclFjiRJapKLHEmS1KT/ByJwMVwUY5eNAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_images(model, n_images):\n",
    "    # Sample from the latent space\n",
    "    random_latent_vectors = tf.random.normal(shape=(n_images, latent_dim))\n",
    "    # Decode them to fake images\n",
    "    generated_images = model.decoder(random_latent_vectors)\n",
    "    generated_images = generated_images.numpy()\n",
    "\n",
    "    # Calculate the number of rows needed in the subplot grid\n",
    "    n_rows = int(np.ceil(n_images / 4))\n",
    "\n",
    "    # Plot the generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n_images):\n",
    "        plt.subplot(n_rows, 4, i + 1)\n",
    "        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate and display images\n",
    "generate_images(vae, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. <a id='toc12_'></a>[__Conclusion__](#toc0_)\n",
    "\n",
    "In this demonstration, we have successfully implemented and trained a Variational Autoencoder (VAE) using TensorFlow to generate images based on the MNIST dataset. The process encompassed several critical steps, from importing necessary libraries to training the model, and finally, generating a manifold of digits."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
